{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ca3ea6b3-40f6-478b-a04e-7df824ce0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3fe0cf69-c072-45b6-b1eb-9b54805d8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgDataLoader():\n",
    "    DATABASE_URL = \"postgresql://overcat:overmind@localhost:5432/stocks\"\n",
    "    query = \"\"\"\n",
    "    SELECT * from data.reversals;\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(DATABASE_URL)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            results = cur.fetchall()  # Fetch all rows from the query result\n",
    "            \n",
    "            for row in results:\n",
    "                matrix1 = np.array([\n",
    "                    row[1][\"Open\"],\n",
    "                    row[1][\"High\"],\n",
    "                    row[1][\"Low\"],\n",
    "                    row[1][\"Close\"],\n",
    "                    row[1][\"Volume\"]\n",
    "                    # row[1][\"Time\"]\n",
    "                ])\n",
    "    \n",
    "                matrix2 = np.array([\n",
    "                    row[2][\"Open\"],\n",
    "                    row[2][\"High\"],\n",
    "                    row[2][\"Low\"],\n",
    "                    row[2][\"Close\"],\n",
    "                    row[2][\"Volume\"]\n",
    "                    # row[2][\"Time\"]\n",
    "                ])\n",
    "    \n",
    "                # print(row[3], row[4])\n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.concatenate((matrix1, matrix2), axis=1), ax=axes, t0=row[4])\n",
    "    \n",
    "                matrix1 = np.moveaxis(matrix1, 1, 0)\n",
    "                matrix2 = np.moveaxis(matrix2, 1, 0)\n",
    "    \n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.moveaxis(np.concatenate((matrix1, matrix2)), 1, 0), ax=axes, t0=temp[0])\n",
    "                dataset.append(matrix1)\n",
    "                labels.append(row[5])\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "73c7ee45-024f-496a-9d54-2dcdc49f5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gasf = GramianAngularField(method=\"summation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ac331467-8b7a-46e6-8edf-1177db13659f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2093, 2093)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = pgDataLoader()\n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0915eebd-a272-41b4-8b96-34f5d4262a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_data = []\n",
    "for i, sample in enumerate(data):\n",
    "    open_prices = sample[:, 0]\n",
    "    high_prices = sample[:, 1]\n",
    "    low_prices = sample[:, 2]\n",
    "    close_prices = sample[:, 3]\n",
    "    volume = sample[:, 4]\n",
    "\n",
    "    body_length = np.abs(close_prices - open_prices)\n",
    "    upper_shadow_length = high_prices - np.maximum(open_prices, close_prices)\n",
    "    lower_shadow_length = np.minimum(open_prices, close_prices) - low_prices\n",
    "\n",
    "    alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices, volume)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length)), 1, 0)\n",
    "\n",
    "    alt_data.append(alt_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b4ddaedc-41b5-40a3-909e-6d4c64d6ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, gasf_transform=None):\n",
    "        self.sequences = data\n",
    "        self.labels = labels\n",
    "        self.gasf_transform = gasf_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.gasf_transform:\n",
    "            gasf_images = np.array(self.gasf_transform.transform(np.moveaxis(self.sequences[idx], 1, 0)))\n",
    "            # return torch.unsqueeze(torch.tensor(gasf_images, dtype=torch.float32), dim=0), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return torch.tensor(gasf_images, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.unsqueeze(torch.tensor(self.sequences[idx], dtype=torch.float32), dim=0), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        # return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ff95c1a5-fec5-46b0-bb13-d9d1b5f7f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.8*len(data))\n",
    "# test_size = len(data) - train_size\n",
    "# dataset = CustomDataset(data, labels)\n",
    "\n",
    "train_size = int(0.999*len(alt_data))\n",
    "test_size = len(alt_data) - train_size\n",
    "dataset = CustomDataset(alt_data, labels, gasf)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "48c20ecb-3696-4143-b5cb-2f28dab4ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "# for idx, img in enumerate(next(iter(train_dataloader))[0][0][0]):\n",
    "for idx, img in enumerate(next(iter(train_dataloader))[0][0]):\n",
    "    axes[idx].matshow(img)\n",
    "    axes[idx].set_xticks([])  \n",
    "    axes[idx].set_yticks([])  \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "76f0db7e-203f-4de1-b20f-cbbfe7419261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 16, 16]), torch.Size([32, 5, 16, 16]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = next(iter(train_dataloader))[0]\n",
    "# c1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=3, stride=1)\n",
    "# c1(temp.permute(0, 2, 1))\n",
    "c1 = nn.Conv2d(in_channels=5, out_channels=8, kernel_size=3, padding=1)\n",
    "c1(temp).shape, temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e9551c2-c8fc-46ba-9034-aad143552bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes, filters):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.c1 = nn.Conv1d(in_channels=input_size, out_channels=filters, kernel_size=2, stride=1)\n",
    "#         self.lstm = nn.LSTM(filters, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "#         self.hidden_size = hidden_size\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.c1(x.permute(0, 2, 1))\n",
    "#         x = torch.tanh(x)\n",
    "\n",
    "#         h0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "#         out, _ = self.lstm(x.permute(0, 2, 1), (h0, c0))\n",
    "#         # out = self.flatten(out)\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         # out = self.fc(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=5, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.c2 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout1 = nn.Dropout(p=0.4)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.bn2 = nn.BatchNorm2d(10)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=40, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=0.4, )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.reshape(-1, 256, 40)\n",
    "\n",
    "        h0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # out, _ = self.lstm(x.permute(0, 2, 1), (h0, c0))\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # out = self.flatten(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ed90c19e-1ac4-4fab-ad5f-dcd6b27a1c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (c1): Conv2d(5, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (c2): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(40, 32, num_layers=4, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "num_layers = 4\n",
    "num_classes = 3\n",
    "\n",
    "model = LSTMClassifier(hidden_size, num_layers, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "03a6a13f-2e71-4c5f-937d-6ac151053595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(next(iter(train_dataloader))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ff599b89-3109-4c5a-bef5-ab82e6ca3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryAccuracy:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply sigmoid to logits to get probabilities\n",
    "        probabilities = torch.sigmoid(logits).squeeze(dim=1)\n",
    "        # Convert probabilities to binary predictions\n",
    "        predictions = (probabilities >= self.threshold).float()\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()\n",
    "        return accuracy.item()\n",
    "\n",
    "class MultiClassAccuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply softmax to logits to get class probabilities (optional, for insight)\n",
    "        # probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the predicted class indices by applying argmax to logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()  # Total number of samples\n",
    "        return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5f5202a9-7bbe-415e-8f7b-d0649e6b5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "accuracy_fn = MultiClassAccuracy()\n",
    "# accuracy_fn = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a54e50c2-5f8e-4264-aeb8-dd5119db3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 1. Forward pass\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # print(X.shape)\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # print(y_pred)\n",
    "        # print()\n",
    "        # print(y)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred.squeeze(1), y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        train_acc += accuracy_fn(y_pred, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4692b420-0c1c-4a6d-9790-d7c63aada1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits.squeeze(1), y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_acc += accuracy_fn(test_pred_logits, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa45f7-eab5-4e7c-8706-676c196b0983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "11798971-37ff-4b95-9203-9bea26197884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int = 5):\n",
    "\n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "\n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f2dae77a-3ca0-48f9-a86f-3f28d060386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710a2061dd9e4cd69f393982d17f2582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.0985 | train_acc: 0.3484 | test_loss: 1.0926 | test_acc: 0.3333\n",
      "Epoch: 2 | train_loss: 1.0796 | train_acc: 0.3857 | test_loss: 0.9957 | test_acc: 0.3333\n",
      "Epoch: 3 | train_loss: 1.0272 | train_acc: 0.4499 | test_loss: 0.8975 | test_acc: 0.3333\n",
      "Epoch: 4 | train_loss: 0.9872 | train_acc: 0.4852 | test_loss: 0.7628 | test_acc: 0.6667\n",
      "Epoch: 5 | train_loss: 0.9686 | train_acc: 0.5131 | test_loss: 0.7372 | test_acc: 0.6667\n",
      "Epoch: 6 | train_loss: 0.9317 | train_acc: 0.5078 | test_loss: 0.7656 | test_acc: 0.6667\n",
      "Epoch: 7 | train_loss: 0.9361 | train_acc: 0.5223 | test_loss: 0.7151 | test_acc: 0.6667\n",
      "Epoch: 8 | train_loss: 0.9258 | train_acc: 0.5188 | test_loss: 0.6963 | test_acc: 0.6667\n",
      "Epoch: 9 | train_loss: 0.9122 | train_acc: 0.5406 | test_loss: 0.7019 | test_acc: 0.6667\n",
      "Epoch: 10 | train_loss: 0.9066 | train_acc: 0.5455 | test_loss: 0.7353 | test_acc: 0.6667\n",
      "Epoch: 11 | train_loss: 0.8838 | train_acc: 0.5832 | test_loss: 0.8650 | test_acc: 0.3333\n",
      "Epoch: 12 | train_loss: 0.8614 | train_acc: 0.6194 | test_loss: 1.0673 | test_acc: 0.3333\n",
      "Epoch: 13 | train_loss: 0.8266 | train_acc: 0.6400 | test_loss: 1.0699 | test_acc: 0.3333\n",
      "Epoch: 14 | train_loss: 0.8034 | train_acc: 0.6523 | test_loss: 1.1104 | test_acc: 0.3333\n",
      "Epoch: 15 | train_loss: 0.8128 | train_acc: 0.6614 | test_loss: 1.1656 | test_acc: 0.3333\n",
      "Epoch: 16 | train_loss: 0.7900 | train_acc: 0.6725 | test_loss: 1.0230 | test_acc: 0.3333\n",
      "Epoch: 17 | train_loss: 0.7771 | train_acc: 0.6775 | test_loss: 1.2913 | test_acc: 0.3333\n",
      "Epoch: 18 | train_loss: 0.7574 | train_acc: 0.6817 | test_loss: 0.9289 | test_acc: 0.6667\n",
      "Epoch: 19 | train_loss: 0.7549 | train_acc: 0.6928 | test_loss: 0.9824 | test_acc: 0.6667\n",
      "Epoch: 20 | train_loss: 0.7492 | train_acc: 0.6935 | test_loss: 1.0861 | test_acc: 0.3333\n",
      "Epoch: 21 | train_loss: 0.7324 | train_acc: 0.6911 | test_loss: 0.8947 | test_acc: 0.6667\n",
      "Epoch: 22 | train_loss: 0.7417 | train_acc: 0.6954 | test_loss: 0.8446 | test_acc: 0.6667\n",
      "Epoch: 23 | train_loss: 0.7242 | train_acc: 0.6978 | test_loss: 0.9322 | test_acc: 0.3333\n",
      "Epoch: 24 | train_loss: 0.7199 | train_acc: 0.7063 | test_loss: 0.8722 | test_acc: 0.6667\n",
      "Epoch: 25 | train_loss: 0.7488 | train_acc: 0.6742 | test_loss: 0.8006 | test_acc: 0.6667\n",
      "Epoch: 26 | train_loss: 0.7198 | train_acc: 0.7094 | test_loss: 0.8400 | test_acc: 0.6667\n",
      "Epoch: 27 | train_loss: 0.7000 | train_acc: 0.7273 | test_loss: 0.8550 | test_acc: 0.6667\n",
      "Epoch: 28 | train_loss: 0.6975 | train_acc: 0.7152 | test_loss: 0.6823 | test_acc: 0.6667\n",
      "Epoch: 29 | train_loss: 0.6991 | train_acc: 0.7199 | test_loss: 0.7550 | test_acc: 0.6667\n",
      "Epoch: 30 | train_loss: 0.6830 | train_acc: 0.7193 | test_loss: 1.0396 | test_acc: 0.3333\n",
      "Epoch: 31 | train_loss: 0.6978 | train_acc: 0.7095 | test_loss: 0.7578 | test_acc: 0.6667\n",
      "Epoch: 32 | train_loss: 0.6741 | train_acc: 0.7278 | test_loss: 0.7106 | test_acc: 0.6667\n",
      "Epoch: 33 | train_loss: 0.6654 | train_acc: 0.7228 | test_loss: 0.6402 | test_acc: 0.6667\n",
      "Epoch: 34 | train_loss: 0.6572 | train_acc: 0.7337 | test_loss: 0.9031 | test_acc: 0.6667\n",
      "Epoch: 35 | train_loss: 0.6730 | train_acc: 0.7349 | test_loss: 0.7946 | test_acc: 0.6667\n",
      "Epoch: 36 | train_loss: 0.6618 | train_acc: 0.7322 | test_loss: 0.9800 | test_acc: 0.3333\n",
      "Epoch: 37 | train_loss: 0.6501 | train_acc: 0.7385 | test_loss: 1.1632 | test_acc: 0.3333\n",
      "Epoch: 38 | train_loss: 0.6360 | train_acc: 0.7431 | test_loss: 0.7976 | test_acc: 0.6667\n",
      "Epoch: 39 | train_loss: 0.6465 | train_acc: 0.7388 | test_loss: 1.0229 | test_acc: 0.3333\n",
      "Epoch: 40 | train_loss: 0.6298 | train_acc: 0.7549 | test_loss: 0.8955 | test_acc: 0.6667\n",
      "Epoch: 41 | train_loss: 0.6293 | train_acc: 0.7517 | test_loss: 0.8212 | test_acc: 0.3333\n",
      "Epoch: 42 | train_loss: 0.6111 | train_acc: 0.7605 | test_loss: 0.8564 | test_acc: 0.6667\n",
      "Epoch: 43 | train_loss: 0.5985 | train_acc: 0.7701 | test_loss: 0.7924 | test_acc: 0.6667\n",
      "Epoch: 44 | train_loss: 0.6065 | train_acc: 0.7589 | test_loss: 1.1712 | test_acc: 0.3333\n",
      "Epoch: 45 | train_loss: 0.6070 | train_acc: 0.7586 | test_loss: 1.2565 | test_acc: 0.3333\n",
      "Epoch: 46 | train_loss: 0.5998 | train_acc: 0.7614 | test_loss: 0.8850 | test_acc: 0.6667\n",
      "Epoch: 47 | train_loss: 0.5994 | train_acc: 0.7618 | test_loss: 0.6349 | test_acc: 0.6667\n",
      "Epoch: 48 | train_loss: 0.5889 | train_acc: 0.7712 | test_loss: 1.0671 | test_acc: 0.3333\n",
      "Epoch: 49 | train_loss: 0.5929 | train_acc: 0.7676 | test_loss: 0.8252 | test_acc: 0.6667\n",
      "Epoch: 50 | train_loss: 0.6058 | train_acc: 0.7606 | test_loss: 0.6804 | test_acc: 0.6667\n",
      "Epoch: 51 | train_loss: 0.5948 | train_acc: 0.7641 | test_loss: 0.7858 | test_acc: 0.3333\n",
      "Epoch: 52 | train_loss: 0.5702 | train_acc: 0.7804 | test_loss: 0.8084 | test_acc: 0.6667\n",
      "Epoch: 53 | train_loss: 0.5853 | train_acc: 0.7676 | test_loss: 0.9759 | test_acc: 0.3333\n",
      "Epoch: 54 | train_loss: 0.5869 | train_acc: 0.7845 | test_loss: 1.2484 | test_acc: 0.3333\n",
      "Epoch: 55 | train_loss: 0.5669 | train_acc: 0.7806 | test_loss: 0.8929 | test_acc: 0.6667\n",
      "Epoch: 56 | train_loss: 0.5606 | train_acc: 0.7902 | test_loss: 0.9209 | test_acc: 0.6667\n",
      "Epoch: 57 | train_loss: 0.5401 | train_acc: 0.7883 | test_loss: 0.6937 | test_acc: 0.6667\n",
      "Epoch: 58 | train_loss: 0.5668 | train_acc: 0.7887 | test_loss: 0.6425 | test_acc: 0.6667\n",
      "Epoch: 59 | train_loss: 0.5567 | train_acc: 0.7787 | test_loss: 1.1412 | test_acc: 0.3333\n",
      "Epoch: 60 | train_loss: 0.5363 | train_acc: 0.8016 | test_loss: 0.4442 | test_acc: 0.6667\n",
      "Epoch: 61 | train_loss: 0.5301 | train_acc: 0.8053 | test_loss: 0.9236 | test_acc: 0.6667\n",
      "Epoch: 62 | train_loss: 0.5386 | train_acc: 0.7995 | test_loss: 0.9549 | test_acc: 0.6667\n",
      "Epoch: 63 | train_loss: 0.5420 | train_acc: 0.7881 | test_loss: 0.8994 | test_acc: 0.6667\n",
      "Epoch: 64 | train_loss: 0.5382 | train_acc: 0.7950 | test_loss: 0.8186 | test_acc: 0.6667\n",
      "Epoch: 65 | train_loss: 0.5308 | train_acc: 0.7937 | test_loss: 0.5968 | test_acc: 0.6667\n",
      "Epoch: 66 | train_loss: 0.5213 | train_acc: 0.8037 | test_loss: 1.0942 | test_acc: 0.3333\n",
      "Epoch: 67 | train_loss: 0.5367 | train_acc: 0.7986 | test_loss: 0.7328 | test_acc: 0.6667\n",
      "Epoch: 68 | train_loss: 0.5274 | train_acc: 0.7994 | test_loss: 0.9226 | test_acc: 0.3333\n",
      "Epoch: 69 | train_loss: 0.5194 | train_acc: 0.8027 | test_loss: 0.9298 | test_acc: 0.6667\n",
      "Epoch: 70 | train_loss: 0.5102 | train_acc: 0.8086 | test_loss: 0.8435 | test_acc: 0.6667\n",
      "Epoch: 71 | train_loss: 0.5270 | train_acc: 0.8043 | test_loss: 0.9286 | test_acc: 0.6667\n",
      "Epoch: 72 | train_loss: 0.5015 | train_acc: 0.8085 | test_loss: 0.9423 | test_acc: 0.6667\n",
      "Epoch: 73 | train_loss: 0.4917 | train_acc: 0.8228 | test_loss: 0.7665 | test_acc: 0.6667\n",
      "Epoch: 74 | train_loss: 0.5101 | train_acc: 0.8054 | test_loss: 0.9208 | test_acc: 0.6667\n",
      "Epoch: 75 | train_loss: 0.4862 | train_acc: 0.8199 | test_loss: 1.7773 | test_acc: 0.3333\n",
      "Epoch: 76 | train_loss: 0.4793 | train_acc: 0.8205 | test_loss: 0.8707 | test_acc: 0.6667\n",
      "Epoch: 77 | train_loss: 0.5007 | train_acc: 0.8149 | test_loss: 0.8183 | test_acc: 0.6667\n",
      "Epoch: 78 | train_loss: 0.4761 | train_acc: 0.8190 | test_loss: 0.9234 | test_acc: 0.6667\n",
      "Epoch: 79 | train_loss: 0.4729 | train_acc: 0.8215 | test_loss: 0.9266 | test_acc: 0.6667\n",
      "Epoch: 80 | train_loss: 0.4844 | train_acc: 0.8188 | test_loss: 0.8396 | test_acc: 0.6667\n",
      "Epoch: 81 | train_loss: 0.4729 | train_acc: 0.8174 | test_loss: 0.7380 | test_acc: 0.6667\n",
      "Epoch: 82 | train_loss: 0.4844 | train_acc: 0.8162 | test_loss: 0.8939 | test_acc: 0.6667\n",
      "Epoch: 83 | train_loss: 0.4649 | train_acc: 0.8336 | test_loss: 0.7738 | test_acc: 0.6667\n",
      "Epoch: 84 | train_loss: 0.4861 | train_acc: 0.8204 | test_loss: 0.8680 | test_acc: 0.6667\n",
      "Epoch: 85 | train_loss: 0.4554 | train_acc: 0.8446 | test_loss: 0.9967 | test_acc: 0.6667\n",
      "Epoch: 86 | train_loss: 0.4575 | train_acc: 0.8329 | test_loss: 0.8986 | test_acc: 0.6667\n",
      "Epoch: 87 | train_loss: 0.4576 | train_acc: 0.8347 | test_loss: 0.9918 | test_acc: 0.6667\n",
      "Epoch: 88 | train_loss: 0.4420 | train_acc: 0.8328 | test_loss: 0.9174 | test_acc: 0.6667\n",
      "Epoch: 89 | train_loss: 0.4318 | train_acc: 0.8455 | test_loss: 1.0678 | test_acc: 0.6667\n",
      "Epoch: 90 | train_loss: 0.4375 | train_acc: 0.8390 | test_loss: 1.3544 | test_acc: 0.3333\n",
      "Epoch: 91 | train_loss: 0.4438 | train_acc: 0.8360 | test_loss: 1.1002 | test_acc: 0.6667\n",
      "Epoch: 92 | train_loss: 0.4416 | train_acc: 0.8421 | test_loss: 0.9741 | test_acc: 0.6667\n",
      "Epoch: 93 | train_loss: 0.4357 | train_acc: 0.8421 | test_loss: 0.8771 | test_acc: 0.6667\n",
      "Epoch: 94 | train_loss: 0.4254 | train_acc: 0.8421 | test_loss: 0.9397 | test_acc: 0.6667\n",
      "Epoch: 95 | train_loss: 0.4311 | train_acc: 0.8374 | test_loss: 0.7454 | test_acc: 0.6667\n",
      "Epoch: 96 | train_loss: 0.4177 | train_acc: 0.8436 | test_loss: 0.8777 | test_acc: 0.6667\n",
      "Epoch: 97 | train_loss: 0.4375 | train_acc: 0.8328 | test_loss: 1.5706 | test_acc: 0.3333\n",
      "Epoch: 98 | train_loss: 0.4177 | train_acc: 0.8523 | test_loss: 0.8520 | test_acc: 0.6667\n",
      "Epoch: 99 | train_loss: 0.4387 | train_acc: 0.8441 | test_loss: 1.0510 | test_acc: 0.6667\n",
      "Epoch: 100 | train_loss: 0.4164 | train_acc: 0.8460 | test_loss: 1.0105 | test_acc: 0.6667\n",
      "Total training time: 2888.009 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0\n",
    "model_0_results = train(model=model,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c430feaa-6213-410b-9757-064b4e9e9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"lstm_model.pth\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f34bb-bb7f-44a6-ac3b-e31bc58d9a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10266b73-86d1-42a5-aed8-7ceb994cfed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169d9ef-ceef-4efb-b121-8d1f8968f92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
