{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf23e0b0-bd43-474a-bfa5-aa3a9cbcfc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "from pyts.image import GramianAngularField\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "import psycopg2\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d81f674-3e03-45a2-901b-f505db6fa165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgDataLoader():\n",
    "    DATABASE_URL = \"postgresql://overcat:overmind@localhost:5432/stocks\"\n",
    "    conn = psycopg2.connect(DATABASE_URL)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT * from data.patterns where pattern_name in ('Engulfing Bearish', 'Engulfing Bullish', 'Hammer', 'Hanging Man');\n",
    "    \"\"\"\n",
    "\n",
    "    # query = \"\"\"\n",
    "    # SELECT * from synthetic.patterns where pattern_name in ('Engulfing Bearish', 'Engulfing Bullish', 'Hammer', 'Hanging Man');\n",
    "    # \"\"\"\n",
    "    \n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(DATABASE_URL)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            results = cur.fetchall()  # Fetch all rows from the query result\n",
    "            \n",
    "            for row in results:\n",
    "                matrix1 = np.array([\n",
    "                    row[1][\"Open\"],\n",
    "                    row[1][\"High\"],\n",
    "                    row[1][\"Low\"],\n",
    "                    row[1][\"Close\"],\n",
    "                    # row[1][\"Time\"],\n",
    "                ])\n",
    "    \n",
    "                # print(row[3], row[4])\n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.concatenate((matrix1, matrix2), axis=1), ax=axes, t0=row[4])\n",
    "    \n",
    "                matrix1 = np.moveaxis(matrix1, 1, 0)\n",
    "    \n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.moveaxis(np.concatenate((matrix1, matrix2)), 1, 0), ax=axes, t0=temp[0])\n",
    "                dataset.append((matrix1, row[4]))\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4011cc0e-af5f-4077-a417-0f1b68ffaf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = pgDataLoader()\n",
    "len(raw_dataset)\n",
    "\n",
    "f, l = zip(*raw_dataset)\n",
    "np.unique(l, return_counts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8679e5e2-2bd6-4d3b-93dd-e02e2329a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_data = []\n",
    "for i, s in enumerate(raw_dataset):\n",
    "    sample = s[0]\n",
    "    # open_prices = sample[:, 0]\n",
    "    # high_prices = sample[:, 1]\n",
    "    # low_prices = sample[:, 2]\n",
    "    # close_prices = sample[:, 3]\n",
    "    # volume = sample[:, 4]\n",
    "\n",
    "    open_prices = sample[8:10, 0]\n",
    "    high_prices = sample[8:10, 1]\n",
    "    low_prices = sample[8:10, 2]\n",
    "    close_prices = sample[8:10, 3]\n",
    "\n",
    "    body_length = np.abs(close_prices - open_prices)\n",
    "    upper_shadow_length = high_prices - np.maximum(open_prices, close_prices)\n",
    "    lower_shadow_length = np.minimum(open_prices, close_prices) - low_prices\n",
    "\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices, volume)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length)), 1, 0)\n",
    "    alt_sample = np.moveaxis(np.vstack((open_prices, high_prices, low_prices, close_prices)), 1, 0)\n",
    "\n",
    "    alt_data.append((alt_sample, s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "791a10a1-77ad-4483-b14d-bd529c9fe56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt_data = []\n",
    "# for i, s in enumerate(raw_dataset):\n",
    "#     sample = s[0]\n",
    "#     # open_prices = sample[:, 0]\n",
    "#     # high_prices = sample[:, 1]\n",
    "#     # low_prices = sample[:, 2]\n",
    "#     # close_prices = sample[:, 3]\n",
    "#     # volume = sample[:, 4]\n",
    "\n",
    "#     open_prices = sample[:8, 0]\n",
    "#     high_prices = sample[:8, 1]\n",
    "#     low_prices = sample[:8, 2]\n",
    "#     close_prices = sample[:8, 3]\n",
    "\n",
    "#     body_length = np.abs(close_prices - open_prices)\n",
    "#     upper_shadow_length = high_prices - np.maximum(open_prices, close_prices)\n",
    "#     lower_shadow_length = np.minimum(open_prices, close_prices) - low_prices\n",
    "\n",
    "#     # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices, volume)), 1, 0)\n",
    "#     # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices)), 1, 0)\n",
    "#     # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length)), 1, 0)\n",
    "#     alt_sample = np.moveaxis(np.vstack((open_prices, high_prices, low_prices, close_prices)), 1, 0)\n",
    "\n",
    "#     alt_data.append((alt_sample, s[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "5d87bb28-7f42-4712-b6fa-78a2901e11e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gasf = GramianAngularField(method=\"summation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "cca75851-a5d9-4fc4-add1-b287ab57716b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = random.randint(0, len(alt_data))\n",
    "temp = np.moveaxis(alt_data[r][0], 1, 0)\n",
    "res = gasf.transform(temp)\n",
    "\n",
    "k = [\"Real body\", \"Upper shadown\", \"Lower shadow\", \"Close\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "for idx, img in enumerate(res):\n",
    "    axes[idx].matshow(img)\n",
    "    axes[idx].set_xticks([])  \n",
    "    axes[idx].set_yticks([])  \n",
    "    axes[idx].set_title(f\"{k[idx]}\", fontsize=10)  # Add title below the image\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/mnt/c/Users/malis/4th year/research/thesis/images/gaf_alt.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "5d55aaa5-b172-401a-b343-f757bf98f95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp = np.moveaxis(raw_dataset[1][0][:, :4], 1, 0)\n",
    "\n",
    "res = gasf.transform(temp)\n",
    "\n",
    "k = [\"Open\", \"High\", \"Low\", \"Close\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "for idx, img in enumerate(res):\n",
    "    axes[idx].matshow(img)\n",
    "    axes[idx].set_xticks([])  \n",
    "    axes[idx].set_yticks([])  \n",
    "    axes[idx].set_title(f\"{k[idx]}\", fontsize=10)  # Add title below the image\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"/mnt/c/Users/malis/4th year/research/thesis/images/gaf.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "46e1a1c5-f3f1-406f-b352-be9b81c58df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pattern(temp1, save_path=None):\n",
    "    data1 = {\n",
    "        'Open': temp1[0],\n",
    "        'High': temp1[1],\n",
    "        'Low': temp1[2],\n",
    "        'Close': temp1[3]\n",
    "    }\n",
    "    \n",
    "    df1 = pd.DataFrame(data1)\n",
    "    df1.index = pd.to_datetime(temp1[4], unit='s')\n",
    "    pattern_date = df1.index[9]\n",
    "    \n",
    "    # Plot the figure\n",
    "    if save_path:\n",
    "        # Save the figure if save_path is provided\n",
    "        mpf.plot(df1, \n",
    "                 type=\"candle\", \n",
    "                 vlines=dict(vlines=pattern_date, linewidths=50, alpha=0.4), \n",
    "                 style=\"yahoo\", \n",
    "                 savefig=save_path)\n",
    "        print(f\"Figure saved to {save_path}\")\n",
    "    else:\n",
    "        # Show the plot without saving if no path is provided\n",
    "        mpf.plot(df1, \n",
    "                 type=\"candle\", \n",
    "                 vlines=dict(vlines=pattern_date, linewidths=50, alpha=0.4), \n",
    "                 style=\"yahoo\")\n",
    "        print(\"Figure displayed, no save path provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "56f0a7a5-5405-4472-b519-ca085ff5c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_pattern(np.moveaxis(raw_dataset[1][0], 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "2158189a-d96d-4472-837b-6a576e5154b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, data, labels, gasf_transform=None):\n",
    "#         self.sequences = data\n",
    "#         self.labels = labels\n",
    "#         self.gasf_transform = gasf_transform\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         if self.gasf_transform:\n",
    "#             gasf_images = np.array(self.gasf_transform.transform(np.moveaxis(self.sequences[idx], 1, 0)))\n",
    "#             # return torch.unsqueeze(torch.tensor(gasf_images, dtype=torch.float32), dim=0), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#             return torch.tensor(gasf_images, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "#         else:\n",
    "#             return torch.unsqueeze(torch.tensor(self.sequences[idx], dtype=torch.float32), dim=0), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "72415648-46bc-4dcb-92f9-a327c0c82a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, gasf_transform=None):\n",
    "        self.sequences = data\n",
    "        self.labels = labels\n",
    "        self.gasf_transform = gasf_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.gasf_transform:\n",
    "            gasf_images = np.array(self.gasf_transform.transform(np.moveaxis(self.sequences[idx], 1, 0)))\n",
    "            # return torch.unsqueeze(torch.tensor(gasf_images, dtype=torch.float32), dim=0), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return torch.tensor(gasf_images, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        else:\n",
    "            return torch.tensor(np.moveaxis(self.sequences[idx], 1, 0), dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "eb22e714-2a4f-4d0b-bfe0-c81e6b98a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, labels = zip(*dataset)\n",
    "# label_count = Counter(labels)\n",
    "\n",
    "# for label in label_count:\n",
    "#     temp = [d for d, lbl in zip(data, labels) if lbl == label]\n",
    "#     print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "1e8d6585-6dd2-4345-8222-9ea039b8958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A_negative' 'Engulfing Bullish']\n",
      "619 1\n"
     ]
    }
   ],
   "source": [
    "# data, labels_str = zip(*raw_dataset)\n",
    "data, labels_str = zip(*alt_data)\n",
    "gasf = GramianAngularField(method=\"summation\")\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform(labels_str)\n",
    "print(encoder.classes_)\n",
    "\n",
    "train_size = int(0.999*len(data))\n",
    "test_size = len(data) - train_size\n",
    "dataset = CustomDataset(data, labels)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "6a551b6c-8060-4b53-a25a-e816284ef52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 2, figsize=(5, 5))\n",
    "# axes = axes.flatten()\n",
    "# # for idx, img in enumerate(next(iter(train_dataloader))[0][0][0]):\n",
    "# for idx, img in enumerate(next(iter(train_dataloader))[0][0]):\n",
    "#     axes[idx].matshow(img)\n",
    "#     axes[idx].set_xticks([])  \n",
    "#     axes[idx].set_yticks([])  \n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "0faf1424-4e07-4555-a58c-fa780ace0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN3D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(CNN3D, self).__init__()\n",
    "\n",
    "        self.conv = nn.Conv3d(in_channels=in_channels, out_channels=4, kernel_size=(2, 3, 3), padding=1)\n",
    "        self.fc = nn.Linear(2420, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = torch.relu(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "7a73176d-bd94-41be-be19-17036fde87bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "f54799c0-7a13-4566-b1e5-a8e33e031da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN2D(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, dropout_prob, filters):\n",
    "        super(CNN2D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=filters, kernel_size=2, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=filters, out_channels=filters, kernel_size=2, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.fc = nn.Linear(13*13*filters, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "2d2335f7-8be8-4f67-94da-5cce9af42e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, dropout_prob, filters):\n",
    "        super(CNN1D, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(in_channels=in_channels, out_channels=filters, kernel_size=2, padding=1)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=filters, out_channels=filters, kernel_size=2, padding=1)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc = nn.Linear(12 * filters, num_classes)  # Adjust output size based on pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # x = self.conv2(x)\n",
    "        # x = torch.relu(x)\n",
    "        # x = self.dropout2(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=64, dropout_prob=0.3):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1) # Flatten input: (batch_size, 4, 11) → (batch_size, 44)\n",
    "        \n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # x = torch.relu(self.fc2(x))\n",
    "        # x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)  # Output layer\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "349c0e81-7a3f-4e22-b743-34f546f241e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 4\n",
    "num_classes = 1\n",
    "dropout_prob = 0.3\n",
    "filters =  64\n",
    "# model = CNN3D(in_channels, num_classes) \n",
    "# model = CNN2D(in_channels, num_classes, dropout_prob, filters)\n",
    "# model = CNN1D(in_channels=in_channels, num_classes=num_classes, dropout_prob=dropout_prob, filters=filters)\n",
    "model_trend = MLP(input_dim=32, num_classes=num_classes, hidden_dim=filters, dropout_prob=dropout_prob)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_trend.parameters(), lr=0.0001)\n",
    "# accuracy_fn = MultiClassAccuracy()\n",
    "accuracy_fn = BinaryAccuracy()\n",
    "\n",
    "in_channels = 4\n",
    "num_classes = 1\n",
    "dropout_prob = 0.3\n",
    "filters =  64\n",
    "model_pattern = MLP(input_dim=8, num_classes=num_classes, hidden_dim=filters, dropout_prob=dropout_prob)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model_pattern.parameters(), lr=0.0001)\n",
    "# accuracy_fn = MultiClassAccuracy()\n",
    "accuracy_fn = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "5f91ee1a-b49c-4f56-af9f-ae926a186d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "# # loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(params=model_trend.parameters(), lr=0.0001)\n",
    "# # accuracy_fn = MultiClassAccuracy()\n",
    "# accuracy_fn = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "99b49cf0-98ef-4210-88ef-2172bda2efba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c05d8a2f65814ed29b71f819dc0180d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 5.4602 | train_acc: 0.4803 | test_loss: 0.1739 | test_acc: 1.0000\n",
      "Epoch: 2 | train_loss: 4.3268 | train_acc: 0.4684 | test_loss: 0.3856 | test_acc: 1.0000\n",
      "Epoch: 3 | train_loss: 3.4724 | train_acc: 0.5074 | test_loss: 0.5869 | test_acc: 1.0000\n",
      "Epoch: 4 | train_loss: 4.0455 | train_acc: 0.4741 | test_loss: 0.5691 | test_acc: 1.0000\n",
      "Epoch: 5 | train_loss: 3.9669 | train_acc: 0.4956 | test_loss: 0.6071 | test_acc: 1.0000\n",
      "Epoch: 6 | train_loss: 3.8200 | train_acc: 0.4773 | test_loss: 0.6056 | test_acc: 1.0000\n",
      "Epoch: 7 | train_loss: 3.5062 | train_acc: 0.4867 | test_loss: 0.6555 | test_acc: 1.0000\n",
      "Epoch: 8 | train_loss: 3.4884 | train_acc: 0.5068 | test_loss: 0.6784 | test_acc: 1.0000\n",
      "Epoch: 9 | train_loss: 3.6326 | train_acc: 0.5118 | test_loss: 0.6013 | test_acc: 1.0000\n",
      "Epoch: 10 | train_loss: 3.5470 | train_acc: 0.5109 | test_loss: 0.5767 | test_acc: 1.0000\n",
      "Epoch: 11 | train_loss: 3.5123 | train_acc: 0.4916 | test_loss: 0.5664 | test_acc: 1.0000\n",
      "Epoch: 12 | train_loss: 3.4850 | train_acc: 0.4964 | test_loss: 0.6142 | test_acc: 1.0000\n",
      "Epoch: 13 | train_loss: 3.3771 | train_acc: 0.5284 | test_loss: 0.5462 | test_acc: 1.0000\n",
      "Epoch: 14 | train_loss: 3.2376 | train_acc: 0.5019 | test_loss: 0.5836 | test_acc: 1.0000\n",
      "Epoch: 15 | train_loss: 3.4204 | train_acc: 0.4956 | test_loss: 0.6688 | test_acc: 1.0000\n",
      "Epoch: 16 | train_loss: 2.7966 | train_acc: 0.5691 | test_loss: 0.5842 | test_acc: 1.0000\n",
      "Epoch: 17 | train_loss: 2.8277 | train_acc: 0.5724 | test_loss: 0.6288 | test_acc: 1.0000\n",
      "Epoch: 18 | train_loss: 3.3462 | train_acc: 0.5074 | test_loss: 0.6692 | test_acc: 1.0000\n",
      "Epoch: 19 | train_loss: 2.9995 | train_acc: 0.5492 | test_loss: 0.6930 | test_acc: 1.0000\n",
      "Epoch: 20 | train_loss: 3.3280 | train_acc: 0.4860 | test_loss: 0.5371 | test_acc: 1.0000\n",
      "Epoch: 21 | train_loss: 2.7391 | train_acc: 0.5181 | test_loss: 0.5460 | test_acc: 1.0000\n",
      "Epoch: 22 | train_loss: 2.6886 | train_acc: 0.4956 | test_loss: 0.6086 | test_acc: 1.0000\n",
      "Epoch: 23 | train_loss: 2.5392 | train_acc: 0.5364 | test_loss: 0.6775 | test_acc: 1.0000\n",
      "Epoch: 24 | train_loss: 2.3765 | train_acc: 0.5293 | test_loss: 0.6711 | test_acc: 1.0000\n",
      "Epoch: 25 | train_loss: 2.7181 | train_acc: 0.5090 | test_loss: 0.6490 | test_acc: 1.0000\n",
      "Epoch: 26 | train_loss: 2.6594 | train_acc: 0.4996 | test_loss: 0.7113 | test_acc: 0.0000\n",
      "Epoch: 27 | train_loss: 2.5125 | train_acc: 0.5396 | test_loss: 0.6092 | test_acc: 1.0000\n",
      "Epoch: 28 | train_loss: 2.8332 | train_acc: 0.5138 | test_loss: 0.6108 | test_acc: 1.0000\n",
      "Epoch: 29 | train_loss: 2.5698 | train_acc: 0.4924 | test_loss: 0.5772 | test_acc: 1.0000\n",
      "Epoch: 30 | train_loss: 2.1945 | train_acc: 0.4885 | test_loss: 0.6631 | test_acc: 1.0000\n",
      "Epoch: 31 | train_loss: 2.2226 | train_acc: 0.5066 | test_loss: 0.5865 | test_acc: 1.0000\n",
      "Epoch: 32 | train_loss: 2.3294 | train_acc: 0.4923 | test_loss: 0.6393 | test_acc: 1.0000\n",
      "Epoch: 33 | train_loss: 2.1572 | train_acc: 0.5443 | test_loss: 0.6863 | test_acc: 1.0000\n",
      "Epoch: 34 | train_loss: 2.1737 | train_acc: 0.5411 | test_loss: 0.6130 | test_acc: 1.0000\n",
      "Epoch: 35 | train_loss: 1.8593 | train_acc: 0.5338 | test_loss: 0.6627 | test_acc: 1.0000\n",
      "Epoch: 36 | train_loss: 2.0788 | train_acc: 0.5197 | test_loss: 0.6562 | test_acc: 1.0000\n",
      "Epoch: 37 | train_loss: 2.0976 | train_acc: 0.5348 | test_loss: 0.6821 | test_acc: 1.0000\n",
      "Epoch: 38 | train_loss: 2.2045 | train_acc: 0.5084 | test_loss: 0.6469 | test_acc: 1.0000\n",
      "Epoch: 39 | train_loss: 1.7128 | train_acc: 0.5366 | test_loss: 0.6486 | test_acc: 1.0000\n",
      "Epoch: 40 | train_loss: 1.8415 | train_acc: 0.5102 | test_loss: 0.5925 | test_acc: 1.0000\n",
      "Epoch: 41 | train_loss: 1.9047 | train_acc: 0.5446 | test_loss: 0.5883 | test_acc: 1.0000\n",
      "Epoch: 42 | train_loss: 1.3567 | train_acc: 0.5653 | test_loss: 0.6884 | test_acc: 1.0000\n",
      "Epoch: 43 | train_loss: 2.1891 | train_acc: 0.5068 | test_loss: 0.6658 | test_acc: 1.0000\n",
      "Epoch: 44 | train_loss: 1.6025 | train_acc: 0.5379 | test_loss: 0.6444 | test_acc: 1.0000\n",
      "Epoch: 45 | train_loss: 1.7111 | train_acc: 0.5084 | test_loss: 0.6564 | test_acc: 1.0000\n",
      "Epoch: 46 | train_loss: 1.6240 | train_acc: 0.5101 | test_loss: 0.6859 | test_acc: 1.0000\n",
      "Epoch: 47 | train_loss: 1.7605 | train_acc: 0.5172 | test_loss: 0.7024 | test_acc: 0.0000\n",
      "Epoch: 48 | train_loss: 1.5117 | train_acc: 0.5198 | test_loss: 0.6312 | test_acc: 1.0000\n",
      "Epoch: 49 | train_loss: 1.4546 | train_acc: 0.5172 | test_loss: 0.6464 | test_acc: 1.0000\n",
      "Epoch: 50 | train_loss: 1.4346 | train_acc: 0.5227 | test_loss: 0.6570 | test_acc: 1.0000\n",
      "Epoch: 51 | train_loss: 1.6264 | train_acc: 0.5003 | test_loss: 0.6297 | test_acc: 1.0000\n",
      "Epoch: 52 | train_loss: 1.5170 | train_acc: 0.5309 | test_loss: 0.6788 | test_acc: 1.0000\n",
      "Epoch: 53 | train_loss: 1.3341 | train_acc: 0.5452 | test_loss: 0.6887 | test_acc: 1.0000\n",
      "Epoch: 54 | train_loss: 1.3255 | train_acc: 0.5628 | test_loss: 0.6887 | test_acc: 1.0000\n",
      "Epoch: 55 | train_loss: 1.3751 | train_acc: 0.5629 | test_loss: 0.6571 | test_acc: 1.0000\n",
      "Epoch: 56 | train_loss: 1.4828 | train_acc: 0.5246 | test_loss: 0.6932 | test_acc: 0.0000\n",
      "Epoch: 57 | train_loss: 1.2442 | train_acc: 0.5533 | test_loss: 0.6944 | test_acc: 0.0000\n",
      "Epoch: 58 | train_loss: 1.2289 | train_acc: 0.5621 | test_loss: 0.6779 | test_acc: 1.0000\n",
      "Epoch: 59 | train_loss: 1.2558 | train_acc: 0.5412 | test_loss: 0.7380 | test_acc: 0.0000\n",
      "Epoch: 60 | train_loss: 1.2471 | train_acc: 0.5492 | test_loss: 0.7239 | test_acc: 0.0000\n",
      "Epoch: 61 | train_loss: 1.0984 | train_acc: 0.5695 | test_loss: 0.7156 | test_acc: 0.0000\n",
      "Epoch: 62 | train_loss: 1.2439 | train_acc: 0.5564 | test_loss: 0.6677 | test_acc: 1.0000\n",
      "Epoch: 63 | train_loss: 1.2353 | train_acc: 0.5350 | test_loss: 0.6755 | test_acc: 1.0000\n",
      "Epoch: 64 | train_loss: 1.1950 | train_acc: 0.5654 | test_loss: 0.7050 | test_acc: 0.0000\n",
      "Epoch: 65 | train_loss: 1.1450 | train_acc: 0.5303 | test_loss: 0.6880 | test_acc: 1.0000\n",
      "Epoch: 66 | train_loss: 1.1579 | train_acc: 0.5647 | test_loss: 0.6647 | test_acc: 1.0000\n",
      "Epoch: 67 | train_loss: 1.1099 | train_acc: 0.5508 | test_loss: 0.6933 | test_acc: 0.0000\n",
      "Epoch: 68 | train_loss: 0.9578 | train_acc: 0.5532 | test_loss: 0.6811 | test_acc: 1.0000\n",
      "Epoch: 69 | train_loss: 1.1090 | train_acc: 0.5159 | test_loss: 0.6990 | test_acc: 0.0000\n",
      "Epoch: 70 | train_loss: 0.9954 | train_acc: 0.5622 | test_loss: 0.6912 | test_acc: 1.0000\n",
      "Epoch: 71 | train_loss: 0.9682 | train_acc: 0.5701 | test_loss: 0.6820 | test_acc: 1.0000\n",
      "Epoch: 72 | train_loss: 0.9480 | train_acc: 0.5790 | test_loss: 0.6714 | test_acc: 1.0000\n",
      "Epoch: 73 | train_loss: 0.8911 | train_acc: 0.6052 | test_loss: 0.6683 | test_acc: 1.0000\n",
      "Epoch: 74 | train_loss: 0.8835 | train_acc: 0.5940 | test_loss: 0.7139 | test_acc: 0.0000\n",
      "Epoch: 75 | train_loss: 0.9175 | train_acc: 0.5653 | test_loss: 0.7036 | test_acc: 0.0000\n",
      "Epoch: 76 | train_loss: 0.8258 | train_acc: 0.5940 | test_loss: 0.6893 | test_acc: 1.0000\n",
      "Epoch: 77 | train_loss: 1.0831 | train_acc: 0.5270 | test_loss: 0.6601 | test_acc: 1.0000\n",
      "Epoch: 78 | train_loss: 0.9058 | train_acc: 0.5788 | test_loss: 0.6932 | test_acc: 0.0000\n",
      "Epoch: 79 | train_loss: 0.7784 | train_acc: 0.6043 | test_loss: 0.7290 | test_acc: 0.0000\n",
      "Epoch: 80 | train_loss: 0.8963 | train_acc: 0.5915 | test_loss: 0.7122 | test_acc: 0.0000\n",
      "Epoch: 81 | train_loss: 0.8877 | train_acc: 0.5862 | test_loss: 0.6631 | test_acc: 1.0000\n",
      "Epoch: 82 | train_loss: 0.8409 | train_acc: 0.5828 | test_loss: 0.7193 | test_acc: 0.0000\n",
      "Epoch: 83 | train_loss: 0.8214 | train_acc: 0.6005 | test_loss: 0.6761 | test_acc: 1.0000\n",
      "Epoch: 84 | train_loss: 0.8354 | train_acc: 0.5557 | test_loss: 0.7009 | test_acc: 0.0000\n",
      "Epoch: 85 | train_loss: 0.7732 | train_acc: 0.5975 | test_loss: 0.6942 | test_acc: 0.0000\n",
      "Epoch: 86 | train_loss: 0.7856 | train_acc: 0.6132 | test_loss: 0.7091 | test_acc: 0.0000\n",
      "Epoch: 87 | train_loss: 0.7830 | train_acc: 0.5819 | test_loss: 0.7170 | test_acc: 0.0000\n",
      "Epoch: 88 | train_loss: 0.7565 | train_acc: 0.6180 | test_loss: 0.7303 | test_acc: 0.0000\n",
      "Epoch: 89 | train_loss: 0.7473 | train_acc: 0.6269 | test_loss: 0.7186 | test_acc: 0.0000\n",
      "Epoch: 90 | train_loss: 0.7527 | train_acc: 0.6149 | test_loss: 0.7089 | test_acc: 0.0000\n",
      "Epoch: 91 | train_loss: 0.7920 | train_acc: 0.5934 | test_loss: 0.6691 | test_acc: 1.0000\n",
      "Epoch: 92 | train_loss: 0.7331 | train_acc: 0.6147 | test_loss: 0.7312 | test_acc: 0.0000\n",
      "Epoch: 93 | train_loss: 0.7250 | train_acc: 0.6142 | test_loss: 0.7069 | test_acc: 0.0000\n",
      "Epoch: 94 | train_loss: 0.7075 | train_acc: 0.6349 | test_loss: 0.6926 | test_acc: 1.0000\n",
      "Epoch: 95 | train_loss: 0.7261 | train_acc: 0.5997 | test_loss: 0.7103 | test_acc: 0.0000\n",
      "Epoch: 96 | train_loss: 0.7269 | train_acc: 0.6238 | test_loss: 0.7077 | test_acc: 0.0000\n",
      "Epoch: 97 | train_loss: 0.7276 | train_acc: 0.6142 | test_loss: 0.7232 | test_acc: 0.0000\n",
      "Epoch: 98 | train_loss: 0.6630 | train_acc: 0.6487 | test_loss: 0.7167 | test_acc: 0.0000\n",
      "Epoch: 99 | train_loss: 0.6753 | train_acc: 0.6541 | test_loss: 0.6982 | test_acc: 0.0000\n",
      "Epoch: 100 | train_loss: 0.7143 | train_acc: 0.6284 | test_loss: 0.7312 | test_acc: 0.0000\n",
      "Epoch: 101 | train_loss: 0.6986 | train_acc: 0.6495 | test_loss: 0.7343 | test_acc: 0.0000\n",
      "Epoch: 102 | train_loss: 0.6737 | train_acc: 0.6477 | test_loss: 0.6853 | test_acc: 1.0000\n",
      "Epoch: 103 | train_loss: 0.6828 | train_acc: 0.6235 | test_loss: 0.7217 | test_acc: 0.0000\n",
      "Epoch: 104 | train_loss: 0.6843 | train_acc: 0.6045 | test_loss: 0.7169 | test_acc: 0.0000\n",
      "Epoch: 105 | train_loss: 0.6749 | train_acc: 0.6301 | test_loss: 0.7059 | test_acc: 0.0000\n",
      "Epoch: 106 | train_loss: 0.6559 | train_acc: 0.6358 | test_loss: 0.7008 | test_acc: 0.0000\n",
      "Epoch: 107 | train_loss: 0.7043 | train_acc: 0.6292 | test_loss: 0.7133 | test_acc: 0.0000\n",
      "Epoch: 108 | train_loss: 0.7006 | train_acc: 0.6214 | test_loss: 0.7019 | test_acc: 0.0000\n",
      "Epoch: 109 | train_loss: 0.6902 | train_acc: 0.6171 | test_loss: 0.7172 | test_acc: 0.0000\n",
      "Epoch: 110 | train_loss: 0.6857 | train_acc: 0.6453 | test_loss: 0.7095 | test_acc: 0.0000\n",
      "Epoch: 111 | train_loss: 0.6549 | train_acc: 0.6396 | test_loss: 0.7040 | test_acc: 0.0000\n",
      "Epoch: 112 | train_loss: 0.6769 | train_acc: 0.6164 | test_loss: 0.7365 | test_acc: 0.0000\n",
      "Epoch: 113 | train_loss: 0.6518 | train_acc: 0.6397 | test_loss: 0.7161 | test_acc: 0.0000\n",
      "Epoch: 114 | train_loss: 0.6381 | train_acc: 0.6677 | test_loss: 0.7315 | test_acc: 0.0000\n",
      "Epoch: 115 | train_loss: 0.6762 | train_acc: 0.6222 | test_loss: 0.6955 | test_acc: 0.0000\n",
      "Epoch: 116 | train_loss: 0.6481 | train_acc: 0.6636 | test_loss: 0.7010 | test_acc: 0.0000\n",
      "Epoch: 117 | train_loss: 0.6646 | train_acc: 0.6276 | test_loss: 0.7342 | test_acc: 0.0000\n",
      "Epoch: 118 | train_loss: 0.6370 | train_acc: 0.6670 | test_loss: 0.7022 | test_acc: 0.0000\n",
      "Epoch: 119 | train_loss: 0.6514 | train_acc: 0.6757 | test_loss: 0.7149 | test_acc: 0.0000\n",
      "Epoch: 120 | train_loss: 0.6669 | train_acc: 0.6477 | test_loss: 0.7003 | test_acc: 0.0000\n",
      "Epoch: 121 | train_loss: 0.6481 | train_acc: 0.6493 | test_loss: 0.7158 | test_acc: 0.0000\n",
      "Epoch: 122 | train_loss: 0.6443 | train_acc: 0.6837 | test_loss: 0.7390 | test_acc: 0.0000\n",
      "Epoch: 123 | train_loss: 0.6432 | train_acc: 0.6572 | test_loss: 0.7344 | test_acc: 0.0000\n",
      "Epoch: 124 | train_loss: 0.6388 | train_acc: 0.6693 | test_loss: 0.7262 | test_acc: 0.0000\n",
      "Epoch: 125 | train_loss: 0.6515 | train_acc: 0.6629 | test_loss: 0.7032 | test_acc: 0.0000\n",
      "Epoch: 126 | train_loss: 0.6348 | train_acc: 0.6757 | test_loss: 0.7179 | test_acc: 0.0000\n",
      "Epoch: 127 | train_loss: 0.6462 | train_acc: 0.6554 | test_loss: 0.7271 | test_acc: 0.0000\n",
      "Epoch: 128 | train_loss: 0.6404 | train_acc: 0.6839 | test_loss: 0.7445 | test_acc: 0.0000\n",
      "Epoch: 129 | train_loss: 0.6426 | train_acc: 0.6748 | test_loss: 0.7077 | test_acc: 0.0000\n",
      "Epoch: 130 | train_loss: 0.6513 | train_acc: 0.6533 | test_loss: 0.7322 | test_acc: 0.0000\n",
      "Epoch: 131 | train_loss: 0.6370 | train_acc: 0.6557 | test_loss: 0.7416 | test_acc: 0.0000\n",
      "Epoch: 132 | train_loss: 0.6556 | train_acc: 0.6846 | test_loss: 0.6965 | test_acc: 0.0000\n",
      "Epoch: 133 | train_loss: 0.6615 | train_acc: 0.6454 | test_loss: 0.7153 | test_acc: 0.0000\n",
      "Epoch: 134 | train_loss: 0.6375 | train_acc: 0.6723 | test_loss: 0.7008 | test_acc: 0.0000\n",
      "Epoch: 135 | train_loss: 0.6319 | train_acc: 0.6645 | test_loss: 0.7316 | test_acc: 0.0000\n",
      "Epoch: 136 | train_loss: 0.6238 | train_acc: 0.6840 | test_loss: 0.7499 | test_acc: 0.0000\n",
      "Epoch: 137 | train_loss: 0.6455 | train_acc: 0.6589 | test_loss: 0.7141 | test_acc: 0.0000\n",
      "Epoch: 138 | train_loss: 0.6492 | train_acc: 0.6460 | test_loss: 0.7166 | test_acc: 0.0000\n",
      "Epoch: 139 | train_loss: 0.6420 | train_acc: 0.6581 | test_loss: 0.7246 | test_acc: 0.0000\n",
      "Epoch: 140 | train_loss: 0.6337 | train_acc: 0.6933 | test_loss: 0.7371 | test_acc: 0.0000\n",
      "Epoch: 141 | train_loss: 0.6456 | train_acc: 0.6573 | test_loss: 0.7226 | test_acc: 0.0000\n",
      "Epoch: 142 | train_loss: 0.6343 | train_acc: 0.6782 | test_loss: 0.7404 | test_acc: 0.0000\n",
      "Epoch: 143 | train_loss: 0.6294 | train_acc: 0.7101 | test_loss: 0.7349 | test_acc: 0.0000\n",
      "Epoch: 144 | train_loss: 0.6336 | train_acc: 0.6869 | test_loss: 0.7307 | test_acc: 0.0000\n",
      "Epoch: 145 | train_loss: 0.6423 | train_acc: 0.6686 | test_loss: 0.7374 | test_acc: 0.0000\n",
      "Epoch: 146 | train_loss: 0.6361 | train_acc: 0.6559 | test_loss: 0.7531 | test_acc: 0.0000\n",
      "Epoch: 147 | train_loss: 0.6170 | train_acc: 0.6805 | test_loss: 0.7326 | test_acc: 0.0000\n",
      "Epoch: 148 | train_loss: 0.6200 | train_acc: 0.6716 | test_loss: 0.7338 | test_acc: 0.0000\n",
      "Epoch: 149 | train_loss: 0.6353 | train_acc: 0.6773 | test_loss: 0.7586 | test_acc: 0.0000\n",
      "Epoch: 150 | train_loss: 0.6399 | train_acc: 0.6613 | test_loss: 0.7367 | test_acc: 0.0000\n",
      "Epoch: 151 | train_loss: 0.6125 | train_acc: 0.6700 | test_loss: 0.7509 | test_acc: 0.0000\n",
      "Epoch: 152 | train_loss: 0.6279 | train_acc: 0.6623 | test_loss: 0.7399 | test_acc: 0.0000\n",
      "Epoch: 153 | train_loss: 0.6241 | train_acc: 0.6687 | test_loss: 0.7624 | test_acc: 0.0000\n",
      "Epoch: 154 | train_loss: 0.6167 | train_acc: 0.6812 | test_loss: 0.7403 | test_acc: 0.0000\n",
      "Epoch: 155 | train_loss: 0.6376 | train_acc: 0.6741 | test_loss: 0.7369 | test_acc: 0.0000\n",
      "Epoch: 156 | train_loss: 0.6077 | train_acc: 0.6935 | test_loss: 0.7275 | test_acc: 0.0000\n",
      "Epoch: 157 | train_loss: 0.6228 | train_acc: 0.6837 | test_loss: 0.7584 | test_acc: 0.0000\n",
      "Epoch: 158 | train_loss: 0.6247 | train_acc: 0.6732 | test_loss: 0.7612 | test_acc: 0.0000\n",
      "Epoch: 159 | train_loss: 0.6291 | train_acc: 0.6748 | test_loss: 0.7584 | test_acc: 0.0000\n",
      "Epoch: 160 | train_loss: 0.6324 | train_acc: 0.6413 | test_loss: 0.7483 | test_acc: 0.0000\n",
      "Epoch: 161 | train_loss: 0.6212 | train_acc: 0.6686 | test_loss: 0.7678 | test_acc: 0.0000\n",
      "Epoch: 162 | train_loss: 0.6347 | train_acc: 0.6732 | test_loss: 0.7463 | test_acc: 0.0000\n",
      "Epoch: 163 | train_loss: 0.6304 | train_acc: 0.6855 | test_loss: 0.7712 | test_acc: 0.0000\n",
      "Epoch: 164 | train_loss: 0.6244 | train_acc: 0.6677 | test_loss: 0.7643 | test_acc: 0.0000\n",
      "Epoch: 165 | train_loss: 0.6341 | train_acc: 0.6638 | test_loss: 0.7497 | test_acc: 0.0000\n",
      "Epoch: 166 | train_loss: 0.6369 | train_acc: 0.6741 | test_loss: 0.7470 | test_acc: 0.0000\n",
      "Epoch: 167 | train_loss: 0.6249 | train_acc: 0.6942 | test_loss: 0.7437 | test_acc: 0.0000\n",
      "Epoch: 168 | train_loss: 0.6355 | train_acc: 0.6557 | test_loss: 0.7557 | test_acc: 0.0000\n",
      "Epoch: 169 | train_loss: 0.6218 | train_acc: 0.6796 | test_loss: 0.7761 | test_acc: 0.0000\n",
      "Epoch: 170 | train_loss: 0.6110 | train_acc: 0.6958 | test_loss: 0.7363 | test_acc: 0.0000\n",
      "Epoch: 171 | train_loss: 0.6189 | train_acc: 0.6693 | test_loss: 0.7724 | test_acc: 0.0000\n",
      "Epoch: 172 | train_loss: 0.6303 | train_acc: 0.6684 | test_loss: 0.7470 | test_acc: 0.0000\n",
      "Epoch: 173 | train_loss: 0.6115 | train_acc: 0.6876 | test_loss: 0.7354 | test_acc: 0.0000\n",
      "Epoch: 174 | train_loss: 0.6247 | train_acc: 0.6891 | test_loss: 0.7736 | test_acc: 0.0000\n",
      "Epoch: 175 | train_loss: 0.6286 | train_acc: 0.6876 | test_loss: 0.7487 | test_acc: 0.0000\n",
      "Epoch: 176 | train_loss: 0.6087 | train_acc: 0.6871 | test_loss: 0.7646 | test_acc: 0.0000\n",
      "Epoch: 177 | train_loss: 0.6101 | train_acc: 0.6767 | test_loss: 0.7696 | test_acc: 0.0000\n",
      "Epoch: 178 | train_loss: 0.6131 | train_acc: 0.6876 | test_loss: 0.7386 | test_acc: 0.0000\n",
      "Epoch: 179 | train_loss: 0.6258 | train_acc: 0.6878 | test_loss: 0.7770 | test_acc: 0.0000\n",
      "Epoch: 180 | train_loss: 0.6221 | train_acc: 0.6783 | test_loss: 0.7812 | test_acc: 0.0000\n",
      "Epoch: 181 | train_loss: 0.6029 | train_acc: 0.6981 | test_loss: 0.7661 | test_acc: 0.0000\n",
      "Epoch: 182 | train_loss: 0.6257 | train_acc: 0.6846 | test_loss: 0.7594 | test_acc: 0.0000\n",
      "Epoch: 183 | train_loss: 0.6208 | train_acc: 0.6788 | test_loss: 0.7873 | test_acc: 0.0000\n",
      "Epoch: 184 | train_loss: 0.6269 | train_acc: 0.6678 | test_loss: 0.7683 | test_acc: 0.0000\n",
      "Epoch: 185 | train_loss: 0.6093 | train_acc: 0.6859 | test_loss: 0.7747 | test_acc: 0.0000\n",
      "Epoch: 186 | train_loss: 0.6089 | train_acc: 0.6828 | test_loss: 0.7644 | test_acc: 0.0000\n",
      "Epoch: 187 | train_loss: 0.6302 | train_acc: 0.6630 | test_loss: 0.7688 | test_acc: 0.0000\n",
      "Epoch: 188 | train_loss: 0.6213 | train_acc: 0.6686 | test_loss: 0.7545 | test_acc: 0.0000\n",
      "Epoch: 189 | train_loss: 0.6167 | train_acc: 0.6782 | test_loss: 0.7551 | test_acc: 0.0000\n",
      "Epoch: 190 | train_loss: 0.6139 | train_acc: 0.6967 | test_loss: 0.7881 | test_acc: 0.0000\n",
      "Epoch: 191 | train_loss: 0.6227 | train_acc: 0.6725 | test_loss: 0.7708 | test_acc: 0.0000\n",
      "Epoch: 192 | train_loss: 0.6047 | train_acc: 0.6728 | test_loss: 0.7571 | test_acc: 0.0000\n",
      "Epoch: 193 | train_loss: 0.6119 | train_acc: 0.6916 | test_loss: 0.7759 | test_acc: 0.0000\n",
      "Epoch: 194 | train_loss: 0.6180 | train_acc: 0.6693 | test_loss: 0.7536 | test_acc: 0.0000\n",
      "Epoch: 195 | train_loss: 0.6170 | train_acc: 0.6981 | test_loss: 0.7770 | test_acc: 0.0000\n",
      "Epoch: 196 | train_loss: 0.6151 | train_acc: 0.6853 | test_loss: 0.7467 | test_acc: 0.0000\n",
      "Epoch: 197 | train_loss: 0.6190 | train_acc: 0.6646 | test_loss: 0.7453 | test_acc: 0.0000\n",
      "Epoch: 198 | train_loss: 0.6173 | train_acc: 0.6742 | test_loss: 0.7534 | test_acc: 0.0000\n",
      "Epoch: 199 | train_loss: 0.6124 | train_acc: 0.6901 | test_loss: 0.8027 | test_acc: 0.0000\n",
      "Epoch: 200 | train_loss: 0.5880 | train_acc: 0.6974 | test_loss: 0.7427 | test_acc: 0.0000\n",
      "Epoch: 201 | train_loss: 0.6065 | train_acc: 0.6846 | test_loss: 0.7613 | test_acc: 0.0000\n",
      "Epoch: 202 | train_loss: 0.6034 | train_acc: 0.7085 | test_loss: 0.8089 | test_acc: 0.0000\n",
      "Epoch: 203 | train_loss: 0.6008 | train_acc: 0.6843 | test_loss: 0.7772 | test_acc: 0.0000\n",
      "Epoch: 204 | train_loss: 0.5906 | train_acc: 0.7005 | test_loss: 0.7991 | test_acc: 0.0000\n",
      "Epoch: 205 | train_loss: 0.6065 | train_acc: 0.6747 | test_loss: 0.7860 | test_acc: 0.0000\n",
      "Epoch: 206 | train_loss: 0.6073 | train_acc: 0.6837 | test_loss: 0.7807 | test_acc: 0.0000\n",
      "Epoch: 207 | train_loss: 0.6050 | train_acc: 0.6951 | test_loss: 0.7793 | test_acc: 0.0000\n",
      "Epoch: 208 | train_loss: 0.6034 | train_acc: 0.6990 | test_loss: 0.7846 | test_acc: 0.0000\n",
      "Epoch: 209 | train_loss: 0.5999 | train_acc: 0.6830 | test_loss: 0.7888 | test_acc: 0.0000\n",
      "Epoch: 210 | train_loss: 0.5972 | train_acc: 0.6949 | test_loss: 0.7997 | test_acc: 0.0000\n",
      "Epoch: 211 | train_loss: 0.6132 | train_acc: 0.6767 | test_loss: 0.8012 | test_acc: 0.0000\n",
      "Epoch: 212 | train_loss: 0.6018 | train_acc: 0.6751 | test_loss: 0.8019 | test_acc: 0.0000\n",
      "Epoch: 213 | train_loss: 0.5995 | train_acc: 0.6884 | test_loss: 0.8042 | test_acc: 0.0000\n",
      "Epoch: 214 | train_loss: 0.5873 | train_acc: 0.6990 | test_loss: 0.8077 | test_acc: 0.0000\n",
      "Epoch: 215 | train_loss: 0.5929 | train_acc: 0.6796 | test_loss: 0.7768 | test_acc: 0.0000\n",
      "Epoch: 216 | train_loss: 0.6017 | train_acc: 0.6582 | test_loss: 0.7883 | test_acc: 0.0000\n",
      "Epoch: 217 | train_loss: 0.6138 | train_acc: 0.6782 | test_loss: 0.7861 | test_acc: 0.0000\n",
      "Epoch: 218 | train_loss: 0.5985 | train_acc: 0.7061 | test_loss: 0.7921 | test_acc: 0.0000\n",
      "Epoch: 219 | train_loss: 0.5980 | train_acc: 0.6871 | test_loss: 0.8157 | test_acc: 0.0000\n",
      "Epoch: 220 | train_loss: 0.6066 | train_acc: 0.6773 | test_loss: 0.8148 | test_acc: 0.0000\n",
      "Epoch: 221 | train_loss: 0.6152 | train_acc: 0.6782 | test_loss: 0.8173 | test_acc: 0.0000\n",
      "Epoch: 222 | train_loss: 0.5857 | train_acc: 0.7069 | test_loss: 0.8094 | test_acc: 0.0000\n",
      "Epoch: 223 | train_loss: 0.5979 | train_acc: 0.6773 | test_loss: 0.7716 | test_acc: 0.0000\n",
      "Epoch: 224 | train_loss: 0.5917 | train_acc: 0.7015 | test_loss: 0.7889 | test_acc: 0.0000\n",
      "Epoch: 225 | train_loss: 0.5961 | train_acc: 0.6830 | test_loss: 0.8119 | test_acc: 0.0000\n",
      "Epoch: 226 | train_loss: 0.5831 | train_acc: 0.6997 | test_loss: 0.8248 | test_acc: 0.0000\n",
      "Epoch: 227 | train_loss: 0.5905 | train_acc: 0.6909 | test_loss: 0.8234 | test_acc: 0.0000\n",
      "Epoch: 228 | train_loss: 0.6185 | train_acc: 0.6694 | test_loss: 0.7912 | test_acc: 0.0000\n",
      "Epoch: 229 | train_loss: 0.5970 | train_acc: 0.6780 | test_loss: 0.8073 | test_acc: 0.0000\n",
      "Epoch: 230 | train_loss: 0.5987 | train_acc: 0.6807 | test_loss: 0.7911 | test_acc: 0.0000\n",
      "Epoch: 231 | train_loss: 0.5958 | train_acc: 0.6965 | test_loss: 0.8168 | test_acc: 0.0000\n",
      "Epoch: 232 | train_loss: 0.5916 | train_acc: 0.6869 | test_loss: 0.8282 | test_acc: 0.0000\n",
      "Epoch: 233 | train_loss: 0.5893 | train_acc: 0.6766 | test_loss: 0.8386 | test_acc: 0.0000\n",
      "Epoch: 234 | train_loss: 0.6033 | train_acc: 0.6726 | test_loss: 0.8164 | test_acc: 0.0000\n",
      "Epoch: 235 | train_loss: 0.5965 | train_acc: 0.6702 | test_loss: 0.8151 | test_acc: 0.0000\n",
      "Epoch: 236 | train_loss: 0.5900 | train_acc: 0.6958 | test_loss: 0.8570 | test_acc: 0.0000\n",
      "Epoch: 237 | train_loss: 0.6084 | train_acc: 0.6823 | test_loss: 0.8284 | test_acc: 0.0000\n",
      "Epoch: 238 | train_loss: 0.6114 | train_acc: 0.6805 | test_loss: 0.7696 | test_acc: 0.0000\n",
      "Epoch: 239 | train_loss: 0.5971 | train_acc: 0.7038 | test_loss: 0.8199 | test_acc: 0.0000\n",
      "Epoch: 240 | train_loss: 0.5807 | train_acc: 0.6965 | test_loss: 0.8096 | test_acc: 0.0000\n",
      "Epoch: 241 | train_loss: 0.5972 | train_acc: 0.6949 | test_loss: 0.8209 | test_acc: 0.0000\n",
      "Epoch: 242 | train_loss: 0.5897 | train_acc: 0.6913 | test_loss: 0.8134 | test_acc: 0.0000\n",
      "Epoch: 243 | train_loss: 0.5914 | train_acc: 0.6925 | test_loss: 0.7877 | test_acc: 0.0000\n",
      "Epoch: 244 | train_loss: 0.5795 | train_acc: 0.6909 | test_loss: 0.8050 | test_acc: 0.0000\n",
      "Epoch: 245 | train_loss: 0.5973 | train_acc: 0.6894 | test_loss: 0.7886 | test_acc: 0.0000\n",
      "Epoch: 246 | train_loss: 0.5774 | train_acc: 0.6887 | test_loss: 0.8165 | test_acc: 0.0000\n",
      "Epoch: 247 | train_loss: 0.5917 | train_acc: 0.6935 | test_loss: 0.8198 | test_acc: 0.0000\n",
      "Epoch: 248 | train_loss: 0.5869 | train_acc: 0.6981 | test_loss: 0.8417 | test_acc: 0.0000\n",
      "Epoch: 249 | train_loss: 0.5816 | train_acc: 0.6837 | test_loss: 0.8294 | test_acc: 0.0000\n",
      "Epoch: 250 | train_loss: 0.5776 | train_acc: 0.6869 | test_loss: 0.8188 | test_acc: 0.0000\n",
      "Epoch: 251 | train_loss: 0.5908 | train_acc: 0.6885 | test_loss: 0.8059 | test_acc: 0.0000\n",
      "Epoch: 252 | train_loss: 0.6015 | train_acc: 0.7013 | test_loss: 0.7945 | test_acc: 0.0000\n",
      "Epoch: 253 | train_loss: 0.5949 | train_acc: 0.7005 | test_loss: 0.8211 | test_acc: 0.0000\n",
      "Epoch: 254 | train_loss: 0.5838 | train_acc: 0.7085 | test_loss: 0.7861 | test_acc: 0.0000\n",
      "Epoch: 255 | train_loss: 0.5885 | train_acc: 0.6974 | test_loss: 0.8096 | test_acc: 0.0000\n",
      "Epoch: 256 | train_loss: 0.5778 | train_acc: 0.7063 | test_loss: 0.8061 | test_acc: 0.0000\n",
      "Epoch: 257 | train_loss: 0.5933 | train_acc: 0.6879 | test_loss: 0.8355 | test_acc: 0.0000\n",
      "Epoch: 258 | train_loss: 0.5939 | train_acc: 0.6919 | test_loss: 0.8166 | test_acc: 0.0000\n",
      "Epoch: 259 | train_loss: 0.5882 | train_acc: 0.7063 | test_loss: 0.8335 | test_acc: 0.0000\n",
      "Epoch: 260 | train_loss: 0.5869 | train_acc: 0.6958 | test_loss: 0.8278 | test_acc: 0.0000\n",
      "Epoch: 261 | train_loss: 0.5888 | train_acc: 0.7127 | test_loss: 0.8093 | test_acc: 0.0000\n",
      "Epoch: 262 | train_loss: 0.5833 | train_acc: 0.6871 | test_loss: 0.8803 | test_acc: 0.0000\n",
      "Epoch: 263 | train_loss: 0.5926 | train_acc: 0.6707 | test_loss: 0.8394 | test_acc: 0.0000\n",
      "Epoch: 264 | train_loss: 0.5951 | train_acc: 0.6885 | test_loss: 0.7893 | test_acc: 0.0000\n",
      "Epoch: 265 | train_loss: 0.6034 | train_acc: 0.6887 | test_loss: 0.8362 | test_acc: 0.0000\n",
      "Epoch: 266 | train_loss: 0.5929 | train_acc: 0.6891 | test_loss: 0.7906 | test_acc: 0.0000\n",
      "Epoch: 267 | train_loss: 0.5874 | train_acc: 0.7158 | test_loss: 0.8161 | test_acc: 0.0000\n",
      "Epoch: 268 | train_loss: 0.5844 | train_acc: 0.6909 | test_loss: 0.8271 | test_acc: 0.0000\n",
      "Epoch: 269 | train_loss: 0.5850 | train_acc: 0.6901 | test_loss: 0.8378 | test_acc: 0.0000\n",
      "Epoch: 270 | train_loss: 0.5874 | train_acc: 0.6941 | test_loss: 0.8178 | test_acc: 0.0000\n",
      "Epoch: 271 | train_loss: 0.5800 | train_acc: 0.6935 | test_loss: 0.8372 | test_acc: 0.0000\n",
      "Epoch: 272 | train_loss: 0.5867 | train_acc: 0.6958 | test_loss: 0.8497 | test_acc: 0.0000\n",
      "Epoch: 273 | train_loss: 0.5722 | train_acc: 0.7181 | test_loss: 0.8409 | test_acc: 0.0000\n",
      "Epoch: 274 | train_loss: 0.5749 | train_acc: 0.7028 | test_loss: 0.8514 | test_acc: 0.0000\n",
      "Epoch: 275 | train_loss: 0.5784 | train_acc: 0.7022 | test_loss: 0.8212 | test_acc: 0.0000\n",
      "Epoch: 276 | train_loss: 0.5818 | train_acc: 0.7031 | test_loss: 0.8644 | test_acc: 0.0000\n",
      "Epoch: 277 | train_loss: 0.5840 | train_acc: 0.6828 | test_loss: 0.8395 | test_acc: 0.0000\n",
      "Epoch: 278 | train_loss: 0.5607 | train_acc: 0.7013 | test_loss: 0.8195 | test_acc: 0.0000\n",
      "Epoch: 279 | train_loss: 0.5751 | train_acc: 0.7045 | test_loss: 0.7942 | test_acc: 0.0000\n",
      "Epoch: 280 | train_loss: 0.5682 | train_acc: 0.7021 | test_loss: 0.8465 | test_acc: 0.0000\n",
      "Epoch: 281 | train_loss: 0.5790 | train_acc: 0.7029 | test_loss: 0.8066 | test_acc: 0.0000\n",
      "Epoch: 282 | train_loss: 0.5704 | train_acc: 0.7118 | test_loss: 0.8685 | test_acc: 0.0000\n",
      "Epoch: 283 | train_loss: 0.5889 | train_acc: 0.6933 | test_loss: 0.8235 | test_acc: 0.0000\n",
      "Epoch: 284 | train_loss: 0.5953 | train_acc: 0.6830 | test_loss: 0.8177 | test_acc: 0.0000\n",
      "Epoch: 285 | train_loss: 0.5709 | train_acc: 0.7054 | test_loss: 0.8584 | test_acc: 0.0000\n",
      "Epoch: 286 | train_loss: 0.5771 | train_acc: 0.7006 | test_loss: 0.7968 | test_acc: 0.0000\n",
      "Epoch: 287 | train_loss: 0.5675 | train_acc: 0.7079 | test_loss: 0.8274 | test_acc: 0.0000\n",
      "Epoch: 288 | train_loss: 0.5751 | train_acc: 0.6935 | test_loss: 0.8427 | test_acc: 0.0000\n",
      "Epoch: 289 | train_loss: 0.5880 | train_acc: 0.7175 | test_loss: 0.8669 | test_acc: 0.0000\n",
      "Epoch: 290 | train_loss: 0.5721 | train_acc: 0.6933 | test_loss: 0.8308 | test_acc: 0.0000\n",
      "Epoch: 291 | train_loss: 0.5598 | train_acc: 0.7247 | test_loss: 0.8316 | test_acc: 0.0000\n",
      "Epoch: 292 | train_loss: 0.5717 | train_acc: 0.7012 | test_loss: 0.8406 | test_acc: 0.0000\n",
      "Epoch: 293 | train_loss: 0.5772 | train_acc: 0.7078 | test_loss: 0.8191 | test_acc: 0.0000\n",
      "Epoch: 294 | train_loss: 0.5758 | train_acc: 0.7021 | test_loss: 0.8311 | test_acc: 0.0000\n",
      "Epoch: 295 | train_loss: 0.5728 | train_acc: 0.7013 | test_loss: 0.8071 | test_acc: 0.0000\n",
      "Epoch: 296 | train_loss: 0.5791 | train_acc: 0.7254 | test_loss: 0.8480 | test_acc: 0.0000\n",
      "Epoch: 297 | train_loss: 0.5782 | train_acc: 0.7070 | test_loss: 0.8646 | test_acc: 0.0000\n",
      "Epoch: 298 | train_loss: 0.5902 | train_acc: 0.6989 | test_loss: 0.8365 | test_acc: 0.0000\n",
      "Epoch: 299 | train_loss: 0.5849 | train_acc: 0.7254 | test_loss: 0.8300 | test_acc: 0.0000\n",
      "Epoch: 300 | train_loss: 0.5630 | train_acc: 0.7126 | test_loss: 0.8224 | test_acc: 0.0000\n",
      "Epoch: 301 | train_loss: 0.5566 | train_acc: 0.7213 | test_loss: 0.8228 | test_acc: 0.0000\n",
      "Epoch: 302 | train_loss: 0.5790 | train_acc: 0.7102 | test_loss: 0.8258 | test_acc: 0.0000\n",
      "Epoch: 303 | train_loss: 0.5707 | train_acc: 0.7303 | test_loss: 0.8731 | test_acc: 0.0000\n",
      "Epoch: 304 | train_loss: 0.5646 | train_acc: 0.7252 | test_loss: 0.8209 | test_acc: 0.0000\n",
      "Epoch: 305 | train_loss: 0.5691 | train_acc: 0.7270 | test_loss: 0.8305 | test_acc: 0.0000\n",
      "Epoch: 306 | train_loss: 0.5634 | train_acc: 0.7150 | test_loss: 0.8361 | test_acc: 0.0000\n",
      "Epoch: 307 | train_loss: 0.5778 | train_acc: 0.7134 | test_loss: 0.8364 | test_acc: 0.0000\n",
      "Epoch: 308 | train_loss: 0.5548 | train_acc: 0.7293 | test_loss: 0.8334 | test_acc: 0.0000\n",
      "Epoch: 309 | train_loss: 0.5703 | train_acc: 0.7095 | test_loss: 0.8285 | test_acc: 0.0000\n",
      "Epoch: 310 | train_loss: 0.5691 | train_acc: 0.6958 | test_loss: 0.8269 | test_acc: 0.0000\n",
      "Epoch: 311 | train_loss: 0.5670 | train_acc: 0.7174 | test_loss: 0.8608 | test_acc: 0.0000\n",
      "Epoch: 312 | train_loss: 0.5606 | train_acc: 0.7437 | test_loss: 0.8421 | test_acc: 0.0000\n",
      "Epoch: 313 | train_loss: 0.5684 | train_acc: 0.7239 | test_loss: 0.8936 | test_acc: 0.0000\n",
      "Epoch: 314 | train_loss: 0.5673 | train_acc: 0.6935 | test_loss: 0.8298 | test_acc: 0.0000\n",
      "Epoch: 315 | train_loss: 0.5818 | train_acc: 0.7117 | test_loss: 0.8103 | test_acc: 0.0000\n",
      "Epoch: 316 | train_loss: 0.5546 | train_acc: 0.7175 | test_loss: 0.8650 | test_acc: 0.0000\n",
      "Epoch: 317 | train_loss: 0.5447 | train_acc: 0.7261 | test_loss: 0.7726 | test_acc: 0.0000\n",
      "Epoch: 318 | train_loss: 0.5669 | train_acc: 0.7078 | test_loss: 0.8425 | test_acc: 0.0000\n",
      "Epoch: 319 | train_loss: 0.5693 | train_acc: 0.7133 | test_loss: 0.8470 | test_acc: 0.0000\n",
      "Epoch: 320 | train_loss: 0.5664 | train_acc: 0.7029 | test_loss: 0.8197 | test_acc: 0.0000\n",
      "Epoch: 321 | train_loss: 0.5604 | train_acc: 0.7238 | test_loss: 0.8602 | test_acc: 0.0000\n",
      "Epoch: 322 | train_loss: 0.5608 | train_acc: 0.7156 | test_loss: 0.8290 | test_acc: 0.0000\n",
      "Epoch: 323 | train_loss: 0.5722 | train_acc: 0.7053 | test_loss: 0.8466 | test_acc: 0.0000\n",
      "Epoch: 324 | train_loss: 0.5507 | train_acc: 0.7038 | test_loss: 0.8156 | test_acc: 0.0000\n",
      "Epoch: 325 | train_loss: 0.5635 | train_acc: 0.7302 | test_loss: 0.8240 | test_acc: 0.0000\n",
      "Epoch: 326 | train_loss: 0.5566 | train_acc: 0.7086 | test_loss: 0.8693 | test_acc: 0.0000\n",
      "Epoch: 327 | train_loss: 0.5597 | train_acc: 0.7092 | test_loss: 0.8174 | test_acc: 0.0000\n",
      "Epoch: 328 | train_loss: 0.5512 | train_acc: 0.7270 | test_loss: 0.8265 | test_acc: 0.0000\n",
      "Epoch: 329 | train_loss: 0.5449 | train_acc: 0.7198 | test_loss: 0.8164 | test_acc: 0.0000\n",
      "Epoch: 330 | train_loss: 0.5409 | train_acc: 0.7309 | test_loss: 0.8180 | test_acc: 0.0000\n",
      "Epoch: 331 | train_loss: 0.5740 | train_acc: 0.7152 | test_loss: 0.8325 | test_acc: 0.0000\n",
      "Epoch: 332 | train_loss: 0.5662 | train_acc: 0.7159 | test_loss: 0.8972 | test_acc: 0.0000\n",
      "Epoch: 333 | train_loss: 0.5532 | train_acc: 0.7029 | test_loss: 0.8263 | test_acc: 0.0000\n",
      "Epoch: 334 | train_loss: 0.5594 | train_acc: 0.7239 | test_loss: 0.8976 | test_acc: 0.0000\n",
      "Epoch: 335 | train_loss: 0.5609 | train_acc: 0.7158 | test_loss: 0.8564 | test_acc: 0.0000\n",
      "Epoch: 336 | train_loss: 0.5549 | train_acc: 0.7142 | test_loss: 0.8372 | test_acc: 0.0000\n",
      "Epoch: 337 | train_loss: 0.5594 | train_acc: 0.7270 | test_loss: 0.8211 | test_acc: 0.0000\n",
      "Epoch: 338 | train_loss: 0.5628 | train_acc: 0.7166 | test_loss: 0.8010 | test_acc: 0.0000\n",
      "Epoch: 339 | train_loss: 0.5539 | train_acc: 0.7245 | test_loss: 0.8287 | test_acc: 0.0000\n",
      "Epoch: 340 | train_loss: 0.5461 | train_acc: 0.7206 | test_loss: 0.8821 | test_acc: 0.0000\n",
      "Epoch: 341 | train_loss: 0.5509 | train_acc: 0.7127 | test_loss: 0.8481 | test_acc: 0.0000\n",
      "Epoch: 342 | train_loss: 0.5503 | train_acc: 0.7373 | test_loss: 0.8557 | test_acc: 0.0000\n",
      "Epoch: 343 | train_loss: 0.5510 | train_acc: 0.7198 | test_loss: 0.8410 | test_acc: 0.0000\n",
      "Epoch: 344 | train_loss: 0.5600 | train_acc: 0.7270 | test_loss: 0.8379 | test_acc: 0.0000\n",
      "Epoch: 345 | train_loss: 0.5458 | train_acc: 0.7318 | test_loss: 0.8577 | test_acc: 0.0000\n",
      "Epoch: 346 | train_loss: 0.5551 | train_acc: 0.7120 | test_loss: 0.8842 | test_acc: 0.0000\n",
      "Epoch: 347 | train_loss: 0.5401 | train_acc: 0.7188 | test_loss: 0.8619 | test_acc: 0.0000\n",
      "Epoch: 348 | train_loss: 0.5459 | train_acc: 0.7229 | test_loss: 0.8393 | test_acc: 0.0000\n",
      "Epoch: 349 | train_loss: 0.5454 | train_acc: 0.7126 | test_loss: 0.8584 | test_acc: 0.0000\n",
      "Epoch: 350 | train_loss: 0.5463 | train_acc: 0.7213 | test_loss: 0.8673 | test_acc: 0.0000\n",
      "Epoch: 351 | train_loss: 0.5546 | train_acc: 0.7318 | test_loss: 0.8704 | test_acc: 0.0000\n",
      "Epoch: 352 | train_loss: 0.5428 | train_acc: 0.7182 | test_loss: 0.8475 | test_acc: 0.0000\n",
      "Epoch: 353 | train_loss: 0.5508 | train_acc: 0.7222 | test_loss: 0.8515 | test_acc: 0.0000\n",
      "Epoch: 354 | train_loss: 0.5426 | train_acc: 0.7177 | test_loss: 0.8679 | test_acc: 0.0000\n",
      "Epoch: 355 | train_loss: 0.5451 | train_acc: 0.7134 | test_loss: 0.8223 | test_acc: 0.0000\n",
      "Epoch: 356 | train_loss: 0.5570 | train_acc: 0.7045 | test_loss: 0.8439 | test_acc: 0.0000\n",
      "Epoch: 357 | train_loss: 0.5584 | train_acc: 0.7054 | test_loss: 0.8785 | test_acc: 0.0000\n",
      "Epoch: 358 | train_loss: 0.5415 | train_acc: 0.7200 | test_loss: 0.8919 | test_acc: 0.0000\n",
      "Epoch: 359 | train_loss: 0.5467 | train_acc: 0.7095 | test_loss: 0.8437 | test_acc: 0.0000\n",
      "Epoch: 360 | train_loss: 0.5398 | train_acc: 0.7295 | test_loss: 0.8049 | test_acc: 0.0000\n",
      "Epoch: 361 | train_loss: 0.5410 | train_acc: 0.7437 | test_loss: 0.8441 | test_acc: 0.0000\n",
      "Epoch: 362 | train_loss: 0.5461 | train_acc: 0.7392 | test_loss: 0.8714 | test_acc: 0.0000\n",
      "Epoch: 363 | train_loss: 0.5436 | train_acc: 0.7286 | test_loss: 0.8250 | test_acc: 0.0000\n",
      "Epoch: 364 | train_loss: 0.5355 | train_acc: 0.7286 | test_loss: 0.8408 | test_acc: 0.0000\n",
      "Epoch: 365 | train_loss: 0.5691 | train_acc: 0.7140 | test_loss: 0.8090 | test_acc: 0.0000\n",
      "Epoch: 366 | train_loss: 0.5437 | train_acc: 0.7239 | test_loss: 0.8202 | test_acc: 0.0000\n",
      "Epoch: 367 | train_loss: 0.5232 | train_acc: 0.7413 | test_loss: 0.8342 | test_acc: 0.0000\n",
      "Epoch: 368 | train_loss: 0.5484 | train_acc: 0.7350 | test_loss: 0.8698 | test_acc: 0.0000\n",
      "Epoch: 369 | train_loss: 0.5442 | train_acc: 0.7373 | test_loss: 0.8453 | test_acc: 0.0000\n",
      "Epoch: 370 | train_loss: 0.5316 | train_acc: 0.7525 | test_loss: 0.8602 | test_acc: 0.0000\n",
      "Epoch: 371 | train_loss: 0.5505 | train_acc: 0.7158 | test_loss: 0.8622 | test_acc: 0.0000\n",
      "Epoch: 372 | train_loss: 0.5454 | train_acc: 0.7254 | test_loss: 0.9083 | test_acc: 0.0000\n",
      "Epoch: 373 | train_loss: 0.5411 | train_acc: 0.7228 | test_loss: 0.8840 | test_acc: 0.0000\n",
      "Epoch: 374 | train_loss: 0.5383 | train_acc: 0.7357 | test_loss: 0.8585 | test_acc: 0.0000\n",
      "Epoch: 375 | train_loss: 0.5347 | train_acc: 0.7423 | test_loss: 0.8145 | test_acc: 0.0000\n",
      "Epoch: 376 | train_loss: 0.5313 | train_acc: 0.7453 | test_loss: 0.8272 | test_acc: 0.0000\n",
      "Epoch: 377 | train_loss: 0.5365 | train_acc: 0.7207 | test_loss: 0.8377 | test_acc: 0.0000\n",
      "Epoch: 378 | train_loss: 0.5410 | train_acc: 0.7311 | test_loss: 0.8554 | test_acc: 0.0000\n",
      "Epoch: 379 | train_loss: 0.5266 | train_acc: 0.7501 | test_loss: 0.8409 | test_acc: 0.0000\n",
      "Epoch: 380 | train_loss: 0.5335 | train_acc: 0.7366 | test_loss: 0.8291 | test_acc: 0.0000\n",
      "Epoch: 381 | train_loss: 0.5394 | train_acc: 0.7478 | test_loss: 0.9000 | test_acc: 0.0000\n",
      "Epoch: 382 | train_loss: 0.5469 | train_acc: 0.7245 | test_loss: 0.8495 | test_acc: 0.0000\n",
      "Epoch: 383 | train_loss: 0.5313 | train_acc: 0.7398 | test_loss: 0.8436 | test_acc: 0.0000\n",
      "Epoch: 384 | train_loss: 0.5290 | train_acc: 0.7429 | test_loss: 0.8617 | test_acc: 0.0000\n",
      "Epoch: 385 | train_loss: 0.5191 | train_acc: 0.7606 | test_loss: 0.8705 | test_acc: 0.0000\n",
      "Epoch: 386 | train_loss: 0.5221 | train_acc: 0.7453 | test_loss: 0.8455 | test_acc: 0.0000\n",
      "Epoch: 387 | train_loss: 0.5260 | train_acc: 0.7429 | test_loss: 0.8344 | test_acc: 0.0000\n",
      "Epoch: 388 | train_loss: 0.5229 | train_acc: 0.7391 | test_loss: 0.8716 | test_acc: 0.0000\n",
      "Epoch: 389 | train_loss: 0.5105 | train_acc: 0.7573 | test_loss: 0.8433 | test_acc: 0.0000\n",
      "Epoch: 390 | train_loss: 0.5339 | train_acc: 0.7287 | test_loss: 0.8814 | test_acc: 0.0000\n",
      "Epoch: 391 | train_loss: 0.5203 | train_acc: 0.7446 | test_loss: 0.8939 | test_acc: 0.0000\n",
      "Epoch: 392 | train_loss: 0.5325 | train_acc: 0.7397 | test_loss: 0.9064 | test_acc: 0.0000\n",
      "Epoch: 393 | train_loss: 0.5126 | train_acc: 0.7478 | test_loss: 0.8253 | test_acc: 0.0000\n",
      "Epoch: 394 | train_loss: 0.5418 | train_acc: 0.7550 | test_loss: 0.8948 | test_acc: 0.0000\n",
      "Epoch: 395 | train_loss: 0.5192 | train_acc: 0.7414 | test_loss: 0.8361 | test_acc: 0.0000\n",
      "Epoch: 396 | train_loss: 0.5386 | train_acc: 0.7383 | test_loss: 0.8219 | test_acc: 0.0000\n",
      "Epoch: 397 | train_loss: 0.5342 | train_acc: 0.7381 | test_loss: 0.8242 | test_acc: 0.0000\n",
      "Epoch: 398 | train_loss: 0.5323 | train_acc: 0.7481 | test_loss: 0.8527 | test_acc: 0.0000\n",
      "Epoch: 399 | train_loss: 0.5131 | train_acc: 0.7678 | test_loss: 0.8459 | test_acc: 0.0000\n",
      "Epoch: 400 | train_loss: 0.5470 | train_acc: 0.7359 | test_loss: 0.8467 | test_acc: 0.0000\n",
      "Epoch: 401 | train_loss: 0.5295 | train_acc: 0.7302 | test_loss: 0.8465 | test_acc: 0.0000\n",
      "Epoch: 402 | train_loss: 0.5384 | train_acc: 0.7517 | test_loss: 0.8710 | test_acc: 0.0000\n",
      "Epoch: 403 | train_loss: 0.5169 | train_acc: 0.7430 | test_loss: 0.8539 | test_acc: 0.0000\n",
      "Epoch: 404 | train_loss: 0.5096 | train_acc: 0.7663 | test_loss: 0.8327 | test_acc: 0.0000\n",
      "Epoch: 405 | train_loss: 0.5092 | train_acc: 0.7296 | test_loss: 0.8851 | test_acc: 0.0000\n",
      "Epoch: 406 | train_loss: 0.5254 | train_acc: 0.7494 | test_loss: 0.8475 | test_acc: 0.0000\n",
      "Epoch: 407 | train_loss: 0.5208 | train_acc: 0.7558 | test_loss: 0.8541 | test_acc: 0.0000\n",
      "Epoch: 408 | train_loss: 0.5089 | train_acc: 0.7685 | test_loss: 0.8042 | test_acc: 0.0000\n",
      "Epoch: 409 | train_loss: 0.5272 | train_acc: 0.7439 | test_loss: 0.8473 | test_acc: 0.0000\n",
      "Epoch: 410 | train_loss: 0.5247 | train_acc: 0.7545 | test_loss: 0.8190 | test_acc: 0.0000\n",
      "Epoch: 411 | train_loss: 0.5285 | train_acc: 0.7551 | test_loss: 0.8662 | test_acc: 0.0000\n",
      "Epoch: 412 | train_loss: 0.5134 | train_acc: 0.7391 | test_loss: 0.8413 | test_acc: 0.0000\n",
      "Epoch: 413 | train_loss: 0.5219 | train_acc: 0.7389 | test_loss: 0.8522 | test_acc: 0.0000\n",
      "Epoch: 414 | train_loss: 0.5134 | train_acc: 0.7550 | test_loss: 0.8869 | test_acc: 0.0000\n",
      "Epoch: 415 | train_loss: 0.5320 | train_acc: 0.7509 | test_loss: 0.8465 | test_acc: 0.0000\n",
      "Epoch: 416 | train_loss: 0.5121 | train_acc: 0.7471 | test_loss: 0.8839 | test_acc: 0.0000\n",
      "Epoch: 417 | train_loss: 0.5185 | train_acc: 0.7420 | test_loss: 0.8814 | test_acc: 0.0000\n",
      "Epoch: 418 | train_loss: 0.5059 | train_acc: 0.7598 | test_loss: 0.8897 | test_acc: 0.0000\n",
      "Epoch: 419 | train_loss: 0.5031 | train_acc: 0.7525 | test_loss: 0.8811 | test_acc: 0.0000\n",
      "Epoch: 420 | train_loss: 0.5207 | train_acc: 0.7375 | test_loss: 0.8172 | test_acc: 0.0000\n",
      "Epoch: 421 | train_loss: 0.5159 | train_acc: 0.7464 | test_loss: 0.8275 | test_acc: 0.0000\n",
      "Epoch: 422 | train_loss: 0.5152 | train_acc: 0.7566 | test_loss: 0.8432 | test_acc: 0.0000\n",
      "Epoch: 423 | train_loss: 0.5160 | train_acc: 0.7327 | test_loss: 0.8353 | test_acc: 0.0000\n",
      "Epoch: 424 | train_loss: 0.5191 | train_acc: 0.7332 | test_loss: 0.8341 | test_acc: 0.0000\n",
      "Epoch: 425 | train_loss: 0.5279 | train_acc: 0.7558 | test_loss: 0.8415 | test_acc: 0.0000\n",
      "Epoch: 426 | train_loss: 0.5243 | train_acc: 0.7484 | test_loss: 0.8662 | test_acc: 0.0000\n",
      "Epoch: 427 | train_loss: 0.5071 | train_acc: 0.7512 | test_loss: 0.9003 | test_acc: 0.0000\n",
      "Epoch: 428 | train_loss: 0.5130 | train_acc: 0.7413 | test_loss: 0.8576 | test_acc: 0.0000\n",
      "Epoch: 429 | train_loss: 0.5167 | train_acc: 0.7516 | test_loss: 0.8233 | test_acc: 0.0000\n",
      "Epoch: 430 | train_loss: 0.5211 | train_acc: 0.7430 | test_loss: 0.8423 | test_acc: 0.0000\n",
      "Epoch: 431 | train_loss: 0.5195 | train_acc: 0.7407 | test_loss: 0.8858 | test_acc: 0.0000\n",
      "Epoch: 432 | train_loss: 0.5166 | train_acc: 0.7437 | test_loss: 0.8636 | test_acc: 0.0000\n",
      "Epoch: 433 | train_loss: 0.5116 | train_acc: 0.7614 | test_loss: 0.8528 | test_acc: 0.0000\n",
      "Epoch: 434 | train_loss: 0.5163 | train_acc: 0.7640 | test_loss: 0.8902 | test_acc: 0.0000\n",
      "Epoch: 435 | train_loss: 0.4953 | train_acc: 0.7589 | test_loss: 0.8569 | test_acc: 0.0000\n",
      "Epoch: 436 | train_loss: 0.4921 | train_acc: 0.7615 | test_loss: 0.8811 | test_acc: 0.0000\n",
      "Epoch: 437 | train_loss: 0.5061 | train_acc: 0.7461 | test_loss: 0.8466 | test_acc: 0.0000\n",
      "Epoch: 438 | train_loss: 0.5155 | train_acc: 0.7548 | test_loss: 0.8281 | test_acc: 0.0000\n",
      "Epoch: 439 | train_loss: 0.5030 | train_acc: 0.7638 | test_loss: 0.8546 | test_acc: 0.0000\n",
      "Epoch: 440 | train_loss: 0.5045 | train_acc: 0.7558 | test_loss: 0.8295 | test_acc: 0.0000\n",
      "Epoch: 441 | train_loss: 0.4992 | train_acc: 0.7599 | test_loss: 0.8760 | test_acc: 0.0000\n",
      "Epoch: 442 | train_loss: 0.4891 | train_acc: 0.7654 | test_loss: 0.8376 | test_acc: 0.0000\n",
      "Epoch: 443 | train_loss: 0.5154 | train_acc: 0.7633 | test_loss: 0.8545 | test_acc: 0.0000\n",
      "Epoch: 444 | train_loss: 0.4897 | train_acc: 0.7807 | test_loss: 0.8577 | test_acc: 0.0000\n",
      "Epoch: 445 | train_loss: 0.4924 | train_acc: 0.7694 | test_loss: 0.8820 | test_acc: 0.0000\n",
      "Epoch: 446 | train_loss: 0.5071 | train_acc: 0.7576 | test_loss: 0.9030 | test_acc: 0.0000\n",
      "Epoch: 447 | train_loss: 0.5148 | train_acc: 0.7567 | test_loss: 0.8933 | test_acc: 0.0000\n",
      "Epoch: 448 | train_loss: 0.4951 | train_acc: 0.7686 | test_loss: 0.8013 | test_acc: 0.0000\n",
      "Epoch: 449 | train_loss: 0.5096 | train_acc: 0.7701 | test_loss: 0.8396 | test_acc: 0.0000\n",
      "Epoch: 450 | train_loss: 0.5040 | train_acc: 0.7590 | test_loss: 0.8503 | test_acc: 0.0000\n",
      "Epoch: 451 | train_loss: 0.5063 | train_acc: 0.7644 | test_loss: 0.8907 | test_acc: 0.0000\n",
      "Epoch: 452 | train_loss: 0.4816 | train_acc: 0.7685 | test_loss: 0.8459 | test_acc: 0.0000\n",
      "Epoch: 453 | train_loss: 0.5108 | train_acc: 0.7640 | test_loss: 0.8213 | test_acc: 0.0000\n",
      "Epoch: 454 | train_loss: 0.4949 | train_acc: 0.7733 | test_loss: 0.8252 | test_acc: 0.0000\n",
      "Epoch: 455 | train_loss: 0.5157 | train_acc: 0.7596 | test_loss: 0.8199 | test_acc: 0.0000\n",
      "Epoch: 456 | train_loss: 0.5033 | train_acc: 0.7678 | test_loss: 0.8108 | test_acc: 0.0000\n",
      "Epoch: 457 | train_loss: 0.5250 | train_acc: 0.7499 | test_loss: 0.7999 | test_acc: 0.0000\n",
      "Epoch: 458 | train_loss: 0.5000 | train_acc: 0.7596 | test_loss: 0.8391 | test_acc: 0.0000\n",
      "Epoch: 459 | train_loss: 0.4971 | train_acc: 0.7695 | test_loss: 0.8515 | test_acc: 0.0000\n",
      "Epoch: 460 | train_loss: 0.5036 | train_acc: 0.7605 | test_loss: 0.8713 | test_acc: 0.0000\n",
      "Epoch: 461 | train_loss: 0.4954 | train_acc: 0.7710 | test_loss: 0.8172 | test_acc: 0.0000\n",
      "Epoch: 462 | train_loss: 0.5019 | train_acc: 0.7455 | test_loss: 0.8861 | test_acc: 0.0000\n",
      "Epoch: 463 | train_loss: 0.4929 | train_acc: 0.7790 | test_loss: 0.8567 | test_acc: 0.0000\n",
      "Epoch: 464 | train_loss: 0.4962 | train_acc: 0.7710 | test_loss: 0.8395 | test_acc: 0.0000\n",
      "Epoch: 465 | train_loss: 0.5089 | train_acc: 0.7679 | test_loss: 0.8858 | test_acc: 0.0000\n",
      "Epoch: 466 | train_loss: 0.4954 | train_acc: 0.7557 | test_loss: 0.8154 | test_acc: 0.0000\n",
      "Epoch: 467 | train_loss: 0.4811 | train_acc: 0.7845 | test_loss: 0.8732 | test_acc: 0.0000\n",
      "Epoch: 468 | train_loss: 0.4847 | train_acc: 0.7647 | test_loss: 0.8579 | test_acc: 0.0000\n",
      "Epoch: 469 | train_loss: 0.5167 | train_acc: 0.7653 | test_loss: 0.8523 | test_acc: 0.0000\n",
      "Epoch: 470 | train_loss: 0.4887 | train_acc: 0.7726 | test_loss: 0.8606 | test_acc: 0.0000\n",
      "Epoch: 471 | train_loss: 0.4973 | train_acc: 0.7646 | test_loss: 0.8464 | test_acc: 0.0000\n",
      "Epoch: 472 | train_loss: 0.4843 | train_acc: 0.7854 | test_loss: 0.9264 | test_acc: 0.0000\n",
      "Epoch: 473 | train_loss: 0.5173 | train_acc: 0.7576 | test_loss: 0.8838 | test_acc: 0.0000\n",
      "Epoch: 474 | train_loss: 0.5006 | train_acc: 0.7557 | test_loss: 0.8562 | test_acc: 0.0000\n",
      "Epoch: 475 | train_loss: 0.4871 | train_acc: 0.7774 | test_loss: 0.8912 | test_acc: 0.0000\n",
      "Epoch: 476 | train_loss: 0.4982 | train_acc: 0.7605 | test_loss: 0.8024 | test_acc: 0.0000\n",
      "Epoch: 477 | train_loss: 0.4853 | train_acc: 0.7678 | test_loss: 0.8648 | test_acc: 0.0000\n",
      "Epoch: 478 | train_loss: 0.4847 | train_acc: 0.7605 | test_loss: 0.8601 | test_acc: 0.0000\n",
      "Epoch: 479 | train_loss: 0.4828 | train_acc: 0.7742 | test_loss: 0.8502 | test_acc: 0.0000\n",
      "Epoch: 480 | train_loss: 0.4808 | train_acc: 0.7790 | test_loss: 0.8806 | test_acc: 0.0000\n",
      "Epoch: 481 | train_loss: 0.4860 | train_acc: 0.7590 | test_loss: 0.8462 | test_acc: 0.0000\n",
      "Epoch: 482 | train_loss: 0.4753 | train_acc: 0.7774 | test_loss: 0.7743 | test_acc: 0.0000\n",
      "Epoch: 483 | train_loss: 0.4842 | train_acc: 0.7758 | test_loss: 0.8165 | test_acc: 0.0000\n",
      "Epoch: 484 | train_loss: 0.4942 | train_acc: 0.7870 | test_loss: 0.8408 | test_acc: 0.0000\n",
      "Epoch: 485 | train_loss: 0.4899 | train_acc: 0.7879 | test_loss: 0.7820 | test_acc: 0.0000\n",
      "Epoch: 486 | train_loss: 0.4771 | train_acc: 0.7822 | test_loss: 0.8016 | test_acc: 0.0000\n",
      "Epoch: 487 | train_loss: 0.4782 | train_acc: 0.7847 | test_loss: 0.8350 | test_acc: 0.0000\n",
      "Epoch: 488 | train_loss: 0.5001 | train_acc: 0.7998 | test_loss: 0.8226 | test_acc: 0.0000\n",
      "Epoch: 489 | train_loss: 0.4937 | train_acc: 0.7847 | test_loss: 0.8397 | test_acc: 0.0000\n",
      "Epoch: 490 | train_loss: 0.4907 | train_acc: 0.7742 | test_loss: 0.8014 | test_acc: 0.0000\n",
      "Epoch: 491 | train_loss: 0.4692 | train_acc: 0.7901 | test_loss: 0.8434 | test_acc: 0.0000\n",
      "Epoch: 492 | train_loss: 0.4918 | train_acc: 0.7813 | test_loss: 0.8806 | test_acc: 0.0000\n",
      "Epoch: 493 | train_loss: 0.4974 | train_acc: 0.7799 | test_loss: 0.8500 | test_acc: 0.0000\n",
      "Epoch: 494 | train_loss: 0.4656 | train_acc: 0.7909 | test_loss: 0.8431 | test_acc: 0.0000\n",
      "Epoch: 495 | train_loss: 0.4910 | train_acc: 0.7807 | test_loss: 0.7951 | test_acc: 0.0000\n",
      "Epoch: 496 | train_loss: 0.4665 | train_acc: 0.8071 | test_loss: 0.8244 | test_acc: 0.0000\n",
      "Epoch: 497 | train_loss: 0.4764 | train_acc: 0.8014 | test_loss: 0.8668 | test_acc: 0.0000\n",
      "Epoch: 498 | train_loss: 0.4863 | train_acc: 0.7719 | test_loss: 0.8056 | test_acc: 0.0000\n",
      "Epoch: 499 | train_loss: 0.4904 | train_acc: 0.7703 | test_loss: 0.8364 | test_acc: 0.0000\n",
      "Epoch: 500 | train_loss: 0.4608 | train_acc: 0.7911 | test_loss: 0.8055 | test_acc: 0.0000\n",
      "Epoch: 501 | train_loss: 0.4741 | train_acc: 0.7829 | test_loss: 0.8401 | test_acc: 0.0000\n",
      "Epoch: 502 | train_loss: 0.4863 | train_acc: 0.7686 | test_loss: 0.8314 | test_acc: 0.0000\n",
      "Epoch: 503 | train_loss: 0.4807 | train_acc: 0.7941 | test_loss: 0.8407 | test_acc: 0.0000\n",
      "Epoch: 504 | train_loss: 0.4698 | train_acc: 0.7941 | test_loss: 0.8605 | test_acc: 0.0000\n",
      "Epoch: 505 | train_loss: 0.4959 | train_acc: 0.7838 | test_loss: 0.8130 | test_acc: 0.0000\n",
      "Epoch: 506 | train_loss: 0.4849 | train_acc: 0.7872 | test_loss: 0.8328 | test_acc: 0.0000\n",
      "Epoch: 507 | train_loss: 0.4667 | train_acc: 0.7911 | test_loss: 0.8614 | test_acc: 0.0000\n",
      "Epoch: 508 | train_loss: 0.4750 | train_acc: 0.7838 | test_loss: 0.8386 | test_acc: 0.0000\n",
      "Epoch: 509 | train_loss: 0.4778 | train_acc: 0.7742 | test_loss: 0.8086 | test_acc: 0.0000\n",
      "Epoch: 510 | train_loss: 0.4799 | train_acc: 0.7902 | test_loss: 0.8102 | test_acc: 0.0000\n",
      "Epoch: 511 | train_loss: 0.4788 | train_acc: 0.7886 | test_loss: 0.8409 | test_acc: 0.0000\n",
      "Epoch: 512 | train_loss: 0.4671 | train_acc: 0.7975 | test_loss: 0.9118 | test_acc: 0.0000\n",
      "Epoch: 513 | train_loss: 0.4706 | train_acc: 0.7831 | test_loss: 0.8512 | test_acc: 0.0000\n",
      "Epoch: 514 | train_loss: 0.4840 | train_acc: 0.7678 | test_loss: 0.9086 | test_acc: 0.0000\n",
      "Epoch: 515 | train_loss: 0.4763 | train_acc: 0.8016 | test_loss: 0.8894 | test_acc: 0.0000\n",
      "Epoch: 516 | train_loss: 0.4653 | train_acc: 0.7703 | test_loss: 0.9154 | test_acc: 0.0000\n",
      "Epoch: 517 | train_loss: 0.4715 | train_acc: 0.7767 | test_loss: 0.8095 | test_acc: 0.0000\n",
      "Epoch: 518 | train_loss: 0.4729 | train_acc: 0.8017 | test_loss: 0.8761 | test_acc: 0.0000\n",
      "Epoch: 519 | train_loss: 0.4750 | train_acc: 0.7837 | test_loss: 0.8192 | test_acc: 0.0000\n",
      "Epoch: 520 | train_loss: 0.4803 | train_acc: 0.7822 | test_loss: 0.8339 | test_acc: 0.0000\n",
      "Epoch: 521 | train_loss: 0.4602 | train_acc: 0.7854 | test_loss: 0.8637 | test_acc: 0.0000\n",
      "Epoch: 522 | train_loss: 0.4584 | train_acc: 0.7998 | test_loss: 0.7989 | test_acc: 0.0000\n",
      "Epoch: 523 | train_loss: 0.4590 | train_acc: 0.7784 | test_loss: 0.8930 | test_acc: 0.0000\n",
      "Epoch: 524 | train_loss: 0.4797 | train_acc: 0.7679 | test_loss: 0.8969 | test_acc: 0.0000\n",
      "Epoch: 525 | train_loss: 0.4801 | train_acc: 0.7861 | test_loss: 0.7973 | test_acc: 0.0000\n",
      "Epoch: 526 | train_loss: 0.4774 | train_acc: 0.7943 | test_loss: 0.8057 | test_acc: 0.0000\n",
      "Epoch: 527 | train_loss: 0.4824 | train_acc: 0.7825 | test_loss: 0.7950 | test_acc: 0.0000\n",
      "Epoch: 528 | train_loss: 0.4636 | train_acc: 0.7823 | test_loss: 0.7818 | test_acc: 0.0000\n",
      "Epoch: 529 | train_loss: 0.4580 | train_acc: 0.7918 | test_loss: 0.7773 | test_acc: 0.0000\n",
      "Epoch: 530 | train_loss: 0.4571 | train_acc: 0.7998 | test_loss: 0.8316 | test_acc: 0.0000\n",
      "Epoch: 531 | train_loss: 0.4793 | train_acc: 0.7719 | test_loss: 0.7306 | test_acc: 0.0000\n",
      "Epoch: 532 | train_loss: 0.4717 | train_acc: 0.7957 | test_loss: 0.7745 | test_acc: 0.0000\n",
      "Epoch: 533 | train_loss: 0.4553 | train_acc: 0.8070 | test_loss: 0.8050 | test_acc: 0.0000\n",
      "Epoch: 534 | train_loss: 0.4538 | train_acc: 0.7973 | test_loss: 0.7819 | test_acc: 0.0000\n",
      "Epoch: 535 | train_loss: 0.4601 | train_acc: 0.8014 | test_loss: 0.7847 | test_acc: 0.0000\n",
      "Epoch: 536 | train_loss: 0.4526 | train_acc: 0.7934 | test_loss: 0.8242 | test_acc: 0.0000\n",
      "Epoch: 537 | train_loss: 0.4645 | train_acc: 0.7870 | test_loss: 0.8479 | test_acc: 0.0000\n",
      "Epoch: 538 | train_loss: 0.4498 | train_acc: 0.7838 | test_loss: 0.8171 | test_acc: 0.0000\n",
      "Epoch: 539 | train_loss: 0.4581 | train_acc: 0.7854 | test_loss: 0.8029 | test_acc: 0.0000\n",
      "Epoch: 540 | train_loss: 0.4702 | train_acc: 0.7984 | test_loss: 0.8024 | test_acc: 0.0000\n",
      "Epoch: 541 | train_loss: 0.4536 | train_acc: 0.7790 | test_loss: 0.7789 | test_acc: 0.0000\n",
      "Epoch: 542 | train_loss: 0.4730 | train_acc: 0.7959 | test_loss: 0.8300 | test_acc: 0.0000\n",
      "Epoch: 543 | train_loss: 0.4557 | train_acc: 0.7944 | test_loss: 0.8429 | test_acc: 0.0000\n",
      "Epoch: 544 | train_loss: 0.4608 | train_acc: 0.8062 | test_loss: 0.7894 | test_acc: 0.0000\n",
      "Epoch: 545 | train_loss: 0.4755 | train_acc: 0.7855 | test_loss: 0.8610 | test_acc: 0.0000\n",
      "Epoch: 546 | train_loss: 0.4756 | train_acc: 0.7813 | test_loss: 0.8138 | test_acc: 0.0000\n",
      "Epoch: 547 | train_loss: 0.4696 | train_acc: 0.7957 | test_loss: 0.7898 | test_acc: 0.0000\n",
      "Epoch: 548 | train_loss: 0.4687 | train_acc: 0.8006 | test_loss: 0.7971 | test_acc: 0.0000\n",
      "Epoch: 549 | train_loss: 0.4488 | train_acc: 0.8008 | test_loss: 0.8396 | test_acc: 0.0000\n",
      "Epoch: 550 | train_loss: 0.4727 | train_acc: 0.7911 | test_loss: 0.7886 | test_acc: 0.0000\n",
      "Epoch: 551 | train_loss: 0.4567 | train_acc: 0.7982 | test_loss: 0.7919 | test_acc: 0.0000\n",
      "Epoch: 552 | train_loss: 0.4462 | train_acc: 0.8110 | test_loss: 0.7826 | test_acc: 0.0000\n",
      "Epoch: 553 | train_loss: 0.4542 | train_acc: 0.7911 | test_loss: 0.7832 | test_acc: 0.0000\n",
      "Epoch: 554 | train_loss: 0.4583 | train_acc: 0.7864 | test_loss: 0.8621 | test_acc: 0.0000\n",
      "Epoch: 555 | train_loss: 0.4607 | train_acc: 0.7950 | test_loss: 0.8042 | test_acc: 0.0000\n",
      "Epoch: 556 | train_loss: 0.4745 | train_acc: 0.7902 | test_loss: 0.7954 | test_acc: 0.0000\n",
      "Epoch: 557 | train_loss: 0.4557 | train_acc: 0.7870 | test_loss: 0.7891 | test_acc: 0.0000\n",
      "Epoch: 558 | train_loss: 0.4460 | train_acc: 0.8032 | test_loss: 0.8749 | test_acc: 0.0000\n",
      "Epoch: 559 | train_loss: 0.4568 | train_acc: 0.7927 | test_loss: 0.8235 | test_acc: 0.0000\n",
      "Epoch: 560 | train_loss: 0.4499 | train_acc: 0.7998 | test_loss: 0.9238 | test_acc: 0.0000\n",
      "Epoch: 561 | train_loss: 0.4496 | train_acc: 0.7959 | test_loss: 0.8011 | test_acc: 0.0000\n",
      "Epoch: 562 | train_loss: 0.4567 | train_acc: 0.7934 | test_loss: 0.8037 | test_acc: 0.0000\n",
      "Epoch: 563 | train_loss: 0.4679 | train_acc: 0.8022 | test_loss: 0.7957 | test_acc: 0.0000\n",
      "Epoch: 564 | train_loss: 0.4453 | train_acc: 0.8062 | test_loss: 0.7846 | test_acc: 0.0000\n",
      "Epoch: 565 | train_loss: 0.4533 | train_acc: 0.7925 | test_loss: 0.7534 | test_acc: 0.0000\n",
      "Epoch: 566 | train_loss: 0.4575 | train_acc: 0.8055 | test_loss: 0.8475 | test_acc: 0.0000\n",
      "Epoch: 567 | train_loss: 0.4560 | train_acc: 0.7984 | test_loss: 0.8076 | test_acc: 0.0000\n",
      "Epoch: 568 | train_loss: 0.4446 | train_acc: 0.8039 | test_loss: 0.7916 | test_acc: 0.0000\n",
      "Epoch: 569 | train_loss: 0.4530 | train_acc: 0.8198 | test_loss: 0.8363 | test_acc: 0.0000\n",
      "Epoch: 570 | train_loss: 0.4705 | train_acc: 0.7975 | test_loss: 0.8850 | test_acc: 0.0000\n",
      "Epoch: 571 | train_loss: 0.4657 | train_acc: 0.7934 | test_loss: 0.7734 | test_acc: 0.0000\n",
      "Epoch: 572 | train_loss: 0.4795 | train_acc: 0.7879 | test_loss: 0.7109 | test_acc: 0.0000\n",
      "Epoch: 573 | train_loss: 0.4499 | train_acc: 0.8086 | test_loss: 0.7855 | test_acc: 0.0000\n",
      "Epoch: 574 | train_loss: 0.4673 | train_acc: 0.7807 | test_loss: 0.8311 | test_acc: 0.0000\n",
      "Epoch: 575 | train_loss: 0.4573 | train_acc: 0.7904 | test_loss: 0.7402 | test_acc: 0.0000\n",
      "Epoch: 576 | train_loss: 0.4545 | train_acc: 0.8150 | test_loss: 0.8003 | test_acc: 0.0000\n",
      "Epoch: 577 | train_loss: 0.4554 | train_acc: 0.8038 | test_loss: 0.8332 | test_acc: 0.0000\n",
      "Epoch: 578 | train_loss: 0.4343 | train_acc: 0.8041 | test_loss: 0.7971 | test_acc: 0.0000\n",
      "Epoch: 579 | train_loss: 0.4418 | train_acc: 0.8167 | test_loss: 0.7909 | test_acc: 0.0000\n",
      "Epoch: 580 | train_loss: 0.4427 | train_acc: 0.8119 | test_loss: 0.7884 | test_acc: 0.0000\n",
      "Epoch: 581 | train_loss: 0.4495 | train_acc: 0.7870 | test_loss: 0.8220 | test_acc: 0.0000\n",
      "Epoch: 582 | train_loss: 0.4312 | train_acc: 0.8199 | test_loss: 0.8344 | test_acc: 0.0000\n",
      "Epoch: 583 | train_loss: 0.4489 | train_acc: 0.7952 | test_loss: 0.7922 | test_acc: 0.0000\n",
      "Epoch: 584 | train_loss: 0.4698 | train_acc: 0.7959 | test_loss: 0.7630 | test_acc: 0.0000\n",
      "Epoch: 585 | train_loss: 0.4766 | train_acc: 0.7886 | test_loss: 0.7650 | test_acc: 0.0000\n",
      "Epoch: 586 | train_loss: 0.4353 | train_acc: 0.8006 | test_loss: 0.7939 | test_acc: 0.0000\n",
      "Epoch: 587 | train_loss: 0.4374 | train_acc: 0.8151 | test_loss: 0.7728 | test_acc: 0.0000\n",
      "Epoch: 588 | train_loss: 0.4393 | train_acc: 0.8301 | test_loss: 0.7983 | test_acc: 0.0000\n",
      "Epoch: 589 | train_loss: 0.4627 | train_acc: 0.8016 | test_loss: 0.7891 | test_acc: 0.0000\n",
      "Epoch: 590 | train_loss: 0.4425 | train_acc: 0.8175 | test_loss: 0.7323 | test_acc: 0.0000\n",
      "Epoch: 591 | train_loss: 0.4319 | train_acc: 0.8191 | test_loss: 0.7734 | test_acc: 0.0000\n",
      "Epoch: 592 | train_loss: 0.4429 | train_acc: 0.8013 | test_loss: 0.7679 | test_acc: 0.0000\n",
      "Epoch: 593 | train_loss: 0.4734 | train_acc: 0.7724 | test_loss: 0.7690 | test_acc: 0.0000\n",
      "Epoch: 594 | train_loss: 0.4264 | train_acc: 0.8077 | test_loss: 0.8201 | test_acc: 0.0000\n",
      "Epoch: 595 | train_loss: 0.4459 | train_acc: 0.8166 | test_loss: 0.8130 | test_acc: 0.0000\n",
      "Epoch: 596 | train_loss: 0.4478 | train_acc: 0.8191 | test_loss: 0.8377 | test_acc: 0.0000\n",
      "Epoch: 597 | train_loss: 0.4438 | train_acc: 0.8103 | test_loss: 0.8333 | test_acc: 0.0000\n",
      "Epoch: 598 | train_loss: 0.4441 | train_acc: 0.8038 | test_loss: 0.7676 | test_acc: 0.0000\n",
      "Epoch: 599 | train_loss: 0.4300 | train_acc: 0.8150 | test_loss: 0.8165 | test_acc: 0.0000\n",
      "Epoch: 600 | train_loss: 0.4432 | train_acc: 0.8198 | test_loss: 0.8493 | test_acc: 0.0000\n",
      "Epoch: 601 | train_loss: 0.4514 | train_acc: 0.8160 | test_loss: 0.8682 | test_acc: 0.0000\n",
      "Epoch: 602 | train_loss: 0.4246 | train_acc: 0.8294 | test_loss: 0.7890 | test_acc: 0.0000\n",
      "Epoch: 603 | train_loss: 0.4454 | train_acc: 0.7943 | test_loss: 0.8156 | test_acc: 0.0000\n",
      "Epoch: 604 | train_loss: 0.4414 | train_acc: 0.8128 | test_loss: 0.8364 | test_acc: 0.0000\n",
      "Epoch: 605 | train_loss: 0.4681 | train_acc: 0.7799 | test_loss: 0.8312 | test_acc: 0.0000\n",
      "Epoch: 606 | train_loss: 0.4390 | train_acc: 0.8125 | test_loss: 0.8288 | test_acc: 0.0000\n",
      "Epoch: 607 | train_loss: 0.4506 | train_acc: 0.8030 | test_loss: 0.8621 | test_acc: 0.0000\n",
      "Epoch: 608 | train_loss: 0.4273 | train_acc: 0.8151 | test_loss: 0.8466 | test_acc: 0.0000\n",
      "Epoch: 609 | train_loss: 0.4623 | train_acc: 0.7829 | test_loss: 0.7248 | test_acc: 0.0000\n",
      "Epoch: 610 | train_loss: 0.4382 | train_acc: 0.8239 | test_loss: 0.7495 | test_acc: 0.0000\n",
      "Epoch: 611 | train_loss: 0.4278 | train_acc: 0.8151 | test_loss: 0.7504 | test_acc: 0.0000\n",
      "Epoch: 612 | train_loss: 0.4396 | train_acc: 0.8038 | test_loss: 0.7840 | test_acc: 0.0000\n",
      "Epoch: 613 | train_loss: 0.4364 | train_acc: 0.8094 | test_loss: 0.8221 | test_acc: 0.0000\n",
      "Epoch: 614 | train_loss: 0.4255 | train_acc: 0.8014 | test_loss: 0.8082 | test_acc: 0.0000\n",
      "Epoch: 615 | train_loss: 0.4421 | train_acc: 0.8022 | test_loss: 0.7916 | test_acc: 0.0000\n",
      "Epoch: 616 | train_loss: 0.4612 | train_acc: 0.7861 | test_loss: 0.7680 | test_acc: 0.0000\n",
      "Epoch: 617 | train_loss: 0.4298 | train_acc: 0.8182 | test_loss: 0.8453 | test_acc: 0.0000\n",
      "Epoch: 618 | train_loss: 0.4239 | train_acc: 0.8086 | test_loss: 0.7913 | test_acc: 0.0000\n",
      "Epoch: 619 | train_loss: 0.4315 | train_acc: 0.8175 | test_loss: 0.7466 | test_acc: 0.0000\n",
      "Epoch: 620 | train_loss: 0.4324 | train_acc: 0.8182 | test_loss: 0.7626 | test_acc: 0.0000\n",
      "Epoch: 621 | train_loss: 0.4270 | train_acc: 0.8087 | test_loss: 0.7976 | test_acc: 0.0000\n",
      "Epoch: 622 | train_loss: 0.4274 | train_acc: 0.8294 | test_loss: 0.7540 | test_acc: 0.0000\n",
      "Epoch: 623 | train_loss: 0.4468 | train_acc: 0.7982 | test_loss: 0.7588 | test_acc: 0.0000\n",
      "Epoch: 624 | train_loss: 0.4309 | train_acc: 0.8128 | test_loss: 0.8946 | test_acc: 0.0000\n",
      "Epoch: 625 | train_loss: 0.4426 | train_acc: 0.8032 | test_loss: 0.7970 | test_acc: 0.0000\n",
      "Epoch: 626 | train_loss: 0.4119 | train_acc: 0.8215 | test_loss: 0.7749 | test_acc: 0.0000\n",
      "Epoch: 627 | train_loss: 0.4523 | train_acc: 0.8141 | test_loss: 0.8079 | test_acc: 0.0000\n",
      "Epoch: 628 | train_loss: 0.4197 | train_acc: 0.8150 | test_loss: 0.7546 | test_acc: 0.0000\n",
      "Epoch: 629 | train_loss: 0.4617 | train_acc: 0.7799 | test_loss: 0.7714 | test_acc: 0.0000\n",
      "Epoch: 630 | train_loss: 0.4294 | train_acc: 0.8224 | test_loss: 0.7666 | test_acc: 0.0000\n",
      "Epoch: 631 | train_loss: 0.4260 | train_acc: 0.8118 | test_loss: 0.7567 | test_acc: 0.0000\n",
      "Epoch: 632 | train_loss: 0.4269 | train_acc: 0.8118 | test_loss: 0.7867 | test_acc: 0.0000\n",
      "Epoch: 633 | train_loss: 0.4223 | train_acc: 0.8199 | test_loss: 0.8049 | test_acc: 0.0000\n",
      "Epoch: 634 | train_loss: 0.4308 | train_acc: 0.8191 | test_loss: 0.7653 | test_acc: 0.0000\n",
      "Epoch: 635 | train_loss: 0.4305 | train_acc: 0.8240 | test_loss: 0.7540 | test_acc: 0.0000\n",
      "Epoch: 636 | train_loss: 0.4385 | train_acc: 0.8207 | test_loss: 0.8415 | test_acc: 0.0000\n",
      "Epoch: 637 | train_loss: 0.4159 | train_acc: 0.8262 | test_loss: 0.8058 | test_acc: 0.0000\n",
      "Epoch: 638 | train_loss: 0.4392 | train_acc: 0.8078 | test_loss: 0.8055 | test_acc: 0.0000\n",
      "Epoch: 639 | train_loss: 0.4373 | train_acc: 0.8054 | test_loss: 0.7573 | test_acc: 0.0000\n",
      "Epoch: 640 | train_loss: 0.4228 | train_acc: 0.8022 | test_loss: 0.7645 | test_acc: 0.0000\n",
      "Epoch: 641 | train_loss: 0.4202 | train_acc: 0.8287 | test_loss: 0.7344 | test_acc: 0.0000\n",
      "Epoch: 642 | train_loss: 0.4556 | train_acc: 0.7981 | test_loss: 0.7603 | test_acc: 0.0000\n",
      "Epoch: 643 | train_loss: 0.4369 | train_acc: 0.7854 | test_loss: 0.7956 | test_acc: 0.0000\n",
      "Epoch: 644 | train_loss: 0.4281 | train_acc: 0.8110 | test_loss: 0.7950 | test_acc: 0.0000\n",
      "Epoch: 645 | train_loss: 0.4138 | train_acc: 0.8215 | test_loss: 0.7804 | test_acc: 0.0000\n",
      "Epoch: 646 | train_loss: 0.4134 | train_acc: 0.8142 | test_loss: 0.7576 | test_acc: 0.0000\n",
      "Epoch: 647 | train_loss: 0.4303 | train_acc: 0.8230 | test_loss: 0.8172 | test_acc: 0.0000\n",
      "Epoch: 648 | train_loss: 0.4384 | train_acc: 0.8125 | test_loss: 0.7420 | test_acc: 0.0000\n",
      "Epoch: 649 | train_loss: 0.4191 | train_acc: 0.8253 | test_loss: 0.7554 | test_acc: 0.0000\n",
      "Epoch: 650 | train_loss: 0.4178 | train_acc: 0.8295 | test_loss: 0.7903 | test_acc: 0.0000\n",
      "Epoch: 651 | train_loss: 0.4223 | train_acc: 0.8215 | test_loss: 0.7545 | test_acc: 0.0000\n",
      "Epoch: 652 | train_loss: 0.4335 | train_acc: 0.8126 | test_loss: 0.7652 | test_acc: 0.0000\n",
      "Epoch: 653 | train_loss: 0.4206 | train_acc: 0.8135 | test_loss: 0.7454 | test_acc: 0.0000\n",
      "Epoch: 654 | train_loss: 0.4228 | train_acc: 0.8207 | test_loss: 0.7219 | test_acc: 0.0000\n",
      "Epoch: 655 | train_loss: 0.4318 | train_acc: 0.8102 | test_loss: 0.7242 | test_acc: 0.0000\n",
      "Epoch: 656 | train_loss: 0.4119 | train_acc: 0.8390 | test_loss: 0.7094 | test_acc: 0.0000\n",
      "Epoch: 657 | train_loss: 0.4301 | train_acc: 0.8223 | test_loss: 0.7215 | test_acc: 0.0000\n",
      "Epoch: 658 | train_loss: 0.4218 | train_acc: 0.8207 | test_loss: 0.6831 | test_acc: 1.0000\n",
      "Epoch: 659 | train_loss: 0.4141 | train_acc: 0.8215 | test_loss: 0.7146 | test_acc: 0.0000\n",
      "Epoch: 660 | train_loss: 0.4091 | train_acc: 0.8205 | test_loss: 0.7149 | test_acc: 0.0000\n",
      "Epoch: 661 | train_loss: 0.4351 | train_acc: 0.8118 | test_loss: 0.7656 | test_acc: 0.0000\n",
      "Epoch: 662 | train_loss: 0.4092 | train_acc: 0.8189 | test_loss: 0.7528 | test_acc: 0.0000\n",
      "Epoch: 663 | train_loss: 0.3927 | train_acc: 0.8342 | test_loss: 0.8203 | test_acc: 0.0000\n",
      "Epoch: 664 | train_loss: 0.4386 | train_acc: 0.8256 | test_loss: 0.7980 | test_acc: 0.0000\n",
      "Epoch: 665 | train_loss: 0.4274 | train_acc: 0.8119 | test_loss: 0.7571 | test_acc: 0.0000\n",
      "Epoch: 666 | train_loss: 0.4222 | train_acc: 0.8128 | test_loss: 0.7123 | test_acc: 0.0000\n",
      "Epoch: 667 | train_loss: 0.4222 | train_acc: 0.8183 | test_loss: 0.8296 | test_acc: 0.0000\n",
      "Epoch: 668 | train_loss: 0.4179 | train_acc: 0.8223 | test_loss: 0.8270 | test_acc: 0.0000\n",
      "Epoch: 669 | train_loss: 0.4185 | train_acc: 0.8285 | test_loss: 0.7723 | test_acc: 0.0000\n",
      "Epoch: 670 | train_loss: 0.4222 | train_acc: 0.8030 | test_loss: 0.7141 | test_acc: 0.0000\n",
      "Epoch: 671 | train_loss: 0.4277 | train_acc: 0.8191 | test_loss: 0.7687 | test_acc: 0.0000\n",
      "Epoch: 672 | train_loss: 0.4234 | train_acc: 0.8303 | test_loss: 0.7571 | test_acc: 0.0000\n",
      "Epoch: 673 | train_loss: 0.3986 | train_acc: 0.8431 | test_loss: 0.7552 | test_acc: 0.0000\n",
      "Epoch: 674 | train_loss: 0.4029 | train_acc: 0.8311 | test_loss: 0.8035 | test_acc: 0.0000\n",
      "Epoch: 675 | train_loss: 0.4134 | train_acc: 0.8320 | test_loss: 0.8361 | test_acc: 0.0000\n",
      "Epoch: 676 | train_loss: 0.4300 | train_acc: 0.8223 | test_loss: 0.7250 | test_acc: 0.0000\n",
      "Epoch: 677 | train_loss: 0.4337 | train_acc: 0.8022 | test_loss: 0.6658 | test_acc: 1.0000\n",
      "Epoch: 678 | train_loss: 0.4255 | train_acc: 0.8294 | test_loss: 0.7498 | test_acc: 0.0000\n",
      "Epoch: 679 | train_loss: 0.4074 | train_acc: 0.8093 | test_loss: 0.7357 | test_acc: 0.0000\n",
      "Epoch: 680 | train_loss: 0.4059 | train_acc: 0.8263 | test_loss: 0.7592 | test_acc: 0.0000\n",
      "Epoch: 681 | train_loss: 0.4164 | train_acc: 0.8207 | test_loss: 0.7250 | test_acc: 0.0000\n",
      "Epoch: 682 | train_loss: 0.4191 | train_acc: 0.8301 | test_loss: 0.7902 | test_acc: 0.0000\n",
      "Epoch: 683 | train_loss: 0.4211 | train_acc: 0.8064 | test_loss: 0.7766 | test_acc: 0.0000\n",
      "Epoch: 684 | train_loss: 0.4259 | train_acc: 0.8023 | test_loss: 0.7441 | test_acc: 0.0000\n",
      "Epoch: 685 | train_loss: 0.4265 | train_acc: 0.8205 | test_loss: 0.7667 | test_acc: 0.0000\n",
      "Epoch: 686 | train_loss: 0.4045 | train_acc: 0.8167 | test_loss: 0.7362 | test_acc: 0.0000\n",
      "Epoch: 687 | train_loss: 0.4283 | train_acc: 0.7968 | test_loss: 0.7448 | test_acc: 0.0000\n",
      "Epoch: 688 | train_loss: 0.4260 | train_acc: 0.8062 | test_loss: 0.7812 | test_acc: 0.0000\n",
      "Epoch: 689 | train_loss: 0.4011 | train_acc: 0.8319 | test_loss: 0.7777 | test_acc: 0.0000\n",
      "Epoch: 690 | train_loss: 0.4196 | train_acc: 0.8255 | test_loss: 0.8159 | test_acc: 0.0000\n",
      "Epoch: 691 | train_loss: 0.4191 | train_acc: 0.8093 | test_loss: 0.7456 | test_acc: 0.0000\n",
      "Epoch: 692 | train_loss: 0.4016 | train_acc: 0.8384 | test_loss: 0.7858 | test_acc: 0.0000\n",
      "Epoch: 693 | train_loss: 0.4003 | train_acc: 0.8262 | test_loss: 0.7803 | test_acc: 0.0000\n",
      "Epoch: 694 | train_loss: 0.4042 | train_acc: 0.8223 | test_loss: 0.7840 | test_acc: 0.0000\n",
      "Epoch: 695 | train_loss: 0.4087 | train_acc: 0.8071 | test_loss: 0.7172 | test_acc: 0.0000\n",
      "Epoch: 696 | train_loss: 0.4313 | train_acc: 0.8253 | test_loss: 0.8243 | test_acc: 0.0000\n",
      "Epoch: 697 | train_loss: 0.4211 | train_acc: 0.8160 | test_loss: 0.7944 | test_acc: 0.0000\n",
      "Epoch: 698 | train_loss: 0.4156 | train_acc: 0.8173 | test_loss: 0.7330 | test_acc: 0.0000\n",
      "Epoch: 699 | train_loss: 0.4075 | train_acc: 0.8191 | test_loss: 0.7530 | test_acc: 0.0000\n",
      "Epoch: 700 | train_loss: 0.4027 | train_acc: 0.8344 | test_loss: 0.7273 | test_acc: 0.0000\n",
      "Epoch: 701 | train_loss: 0.4337 | train_acc: 0.7839 | test_loss: 0.7354 | test_acc: 0.0000\n",
      "Epoch: 702 | train_loss: 0.4357 | train_acc: 0.8110 | test_loss: 0.7407 | test_acc: 0.0000\n",
      "Epoch: 703 | train_loss: 0.4131 | train_acc: 0.8319 | test_loss: 0.6970 | test_acc: 0.0000\n",
      "Epoch: 704 | train_loss: 0.4028 | train_acc: 0.8408 | test_loss: 0.7337 | test_acc: 0.0000\n",
      "Epoch: 705 | train_loss: 0.4195 | train_acc: 0.8303 | test_loss: 0.7414 | test_acc: 0.0000\n",
      "Epoch: 706 | train_loss: 0.3981 | train_acc: 0.8303 | test_loss: 0.6651 | test_acc: 1.0000\n",
      "Epoch: 707 | train_loss: 0.4065 | train_acc: 0.8263 | test_loss: 0.7576 | test_acc: 0.0000\n",
      "Epoch: 708 | train_loss: 0.4011 | train_acc: 0.8438 | test_loss: 0.6981 | test_acc: 0.0000\n",
      "Epoch: 709 | train_loss: 0.4065 | train_acc: 0.8287 | test_loss: 0.7224 | test_acc: 0.0000\n",
      "Epoch: 710 | train_loss: 0.4091 | train_acc: 0.8329 | test_loss: 0.7701 | test_acc: 0.0000\n",
      "Epoch: 711 | train_loss: 0.4332 | train_acc: 0.8119 | test_loss: 0.7115 | test_acc: 0.0000\n",
      "Epoch: 712 | train_loss: 0.4509 | train_acc: 0.8223 | test_loss: 0.6615 | test_acc: 1.0000\n",
      "Epoch: 713 | train_loss: 0.4023 | train_acc: 0.8237 | test_loss: 0.7223 | test_acc: 0.0000\n",
      "Epoch: 714 | train_loss: 0.4323 | train_acc: 0.8239 | test_loss: 0.7075 | test_acc: 0.0000\n",
      "Epoch: 715 | train_loss: 0.4153 | train_acc: 0.8255 | test_loss: 0.7136 | test_acc: 0.0000\n",
      "Epoch: 716 | train_loss: 0.4169 | train_acc: 0.8134 | test_loss: 0.8011 | test_acc: 0.0000\n",
      "Epoch: 717 | train_loss: 0.4304 | train_acc: 0.8191 | test_loss: 0.8022 | test_acc: 0.0000\n",
      "Epoch: 718 | train_loss: 0.4143 | train_acc: 0.8182 | test_loss: 0.7083 | test_acc: 0.0000\n",
      "Epoch: 719 | train_loss: 0.4136 | train_acc: 0.8239 | test_loss: 0.8005 | test_acc: 0.0000\n",
      "Epoch: 720 | train_loss: 0.3949 | train_acc: 0.8335 | test_loss: 0.7676 | test_acc: 0.0000\n",
      "Epoch: 721 | train_loss: 0.4266 | train_acc: 0.8032 | test_loss: 0.7489 | test_acc: 0.0000\n",
      "Epoch: 722 | train_loss: 0.4022 | train_acc: 0.8408 | test_loss: 0.6989 | test_acc: 0.0000\n",
      "Epoch: 723 | train_loss: 0.4083 | train_acc: 0.8336 | test_loss: 0.7970 | test_acc: 0.0000\n",
      "Epoch: 724 | train_loss: 0.4006 | train_acc: 0.8199 | test_loss: 0.7015 | test_acc: 0.0000\n",
      "Epoch: 725 | train_loss: 0.4127 | train_acc: 0.8166 | test_loss: 0.7271 | test_acc: 0.0000\n",
      "Epoch: 726 | train_loss: 0.4240 | train_acc: 0.8285 | test_loss: 0.8234 | test_acc: 0.0000\n",
      "Epoch: 727 | train_loss: 0.4013 | train_acc: 0.8126 | test_loss: 0.7378 | test_acc: 0.0000\n",
      "Epoch: 728 | train_loss: 0.4134 | train_acc: 0.8198 | test_loss: 0.7283 | test_acc: 0.0000\n",
      "Epoch: 729 | train_loss: 0.3928 | train_acc: 0.8472 | test_loss: 0.7636 | test_acc: 0.0000\n",
      "Epoch: 730 | train_loss: 0.4102 | train_acc: 0.8185 | test_loss: 0.7804 | test_acc: 0.0000\n",
      "Epoch: 731 | train_loss: 0.4031 | train_acc: 0.8265 | test_loss: 0.7791 | test_acc: 0.0000\n",
      "Epoch: 732 | train_loss: 0.4528 | train_acc: 0.8030 | test_loss: 0.7726 | test_acc: 0.0000\n",
      "Epoch: 733 | train_loss: 0.3931 | train_acc: 0.8422 | test_loss: 0.7453 | test_acc: 0.0000\n",
      "Epoch: 734 | train_loss: 0.4242 | train_acc: 0.8039 | test_loss: 0.7319 | test_acc: 0.0000\n",
      "Epoch: 735 | train_loss: 0.3965 | train_acc: 0.8335 | test_loss: 0.8083 | test_acc: 0.0000\n",
      "Epoch: 736 | train_loss: 0.3980 | train_acc: 0.8150 | test_loss: 0.7500 | test_acc: 0.0000\n",
      "Epoch: 737 | train_loss: 0.4011 | train_acc: 0.8326 | test_loss: 0.7396 | test_acc: 0.0000\n",
      "Epoch: 738 | train_loss: 0.3944 | train_acc: 0.8246 | test_loss: 0.7518 | test_acc: 0.0000\n",
      "Epoch: 739 | train_loss: 0.4052 | train_acc: 0.8239 | test_loss: 0.7510 | test_acc: 0.0000\n",
      "Epoch: 740 | train_loss: 0.4250 | train_acc: 0.8198 | test_loss: 0.7173 | test_acc: 0.0000\n",
      "Epoch: 741 | train_loss: 0.4199 | train_acc: 0.8301 | test_loss: 0.7317 | test_acc: 0.0000\n",
      "Epoch: 742 | train_loss: 0.4006 | train_acc: 0.8365 | test_loss: 0.7981 | test_acc: 0.0000\n",
      "Epoch: 743 | train_loss: 0.3997 | train_acc: 0.8381 | test_loss: 0.7080 | test_acc: 0.0000\n",
      "Epoch: 744 | train_loss: 0.3818 | train_acc: 0.8311 | test_loss: 0.7595 | test_acc: 0.0000\n",
      "Epoch: 745 | train_loss: 0.4017 | train_acc: 0.8376 | test_loss: 0.6837 | test_acc: 1.0000\n",
      "Epoch: 746 | train_loss: 0.4037 | train_acc: 0.8223 | test_loss: 0.7126 | test_acc: 0.0000\n",
      "Epoch: 747 | train_loss: 0.4024 | train_acc: 0.8367 | test_loss: 0.7097 | test_acc: 0.0000\n",
      "Epoch: 748 | train_loss: 0.3847 | train_acc: 0.8415 | test_loss: 0.6877 | test_acc: 1.0000\n",
      "Epoch: 749 | train_loss: 0.4039 | train_acc: 0.8367 | test_loss: 0.6902 | test_acc: 1.0000\n",
      "Epoch: 750 | train_loss: 0.4046 | train_acc: 0.8368 | test_loss: 0.6526 | test_acc: 1.0000\n",
      "Epoch: 751 | train_loss: 0.4005 | train_acc: 0.8342 | test_loss: 0.6840 | test_acc: 1.0000\n",
      "Epoch: 752 | train_loss: 0.4009 | train_acc: 0.8310 | test_loss: 0.8081 | test_acc: 0.0000\n",
      "Epoch: 753 | train_loss: 0.4074 | train_acc: 0.8358 | test_loss: 0.7725 | test_acc: 0.0000\n",
      "Epoch: 754 | train_loss: 0.4146 | train_acc: 0.8175 | test_loss: 0.6934 | test_acc: 0.0000\n",
      "Epoch: 755 | train_loss: 0.3857 | train_acc: 0.8342 | test_loss: 0.7417 | test_acc: 0.0000\n",
      "Epoch: 756 | train_loss: 0.4245 | train_acc: 0.8344 | test_loss: 0.7818 | test_acc: 0.0000\n",
      "Epoch: 757 | train_loss: 0.3992 | train_acc: 0.8271 | test_loss: 0.6894 | test_acc: 1.0000\n",
      "Epoch: 758 | train_loss: 0.4009 | train_acc: 0.8150 | test_loss: 0.7446 | test_acc: 0.0000\n",
      "Epoch: 759 | train_loss: 0.3949 | train_acc: 0.8183 | test_loss: 0.7643 | test_acc: 0.0000\n",
      "Epoch: 760 | train_loss: 0.3851 | train_acc: 0.8502 | test_loss: 0.7472 | test_acc: 0.0000\n",
      "Epoch: 761 | train_loss: 0.4072 | train_acc: 0.8247 | test_loss: 0.7757 | test_acc: 0.0000\n",
      "Epoch: 762 | train_loss: 0.4005 | train_acc: 0.8271 | test_loss: 0.7880 | test_acc: 0.0000\n",
      "Epoch: 763 | train_loss: 0.3991 | train_acc: 0.8319 | test_loss: 0.8380 | test_acc: 0.0000\n",
      "Epoch: 764 | train_loss: 0.4157 | train_acc: 0.8118 | test_loss: 0.6946 | test_acc: 0.0000\n",
      "Epoch: 765 | train_loss: 0.3928 | train_acc: 0.8440 | test_loss: 0.7146 | test_acc: 0.0000\n",
      "Epoch: 766 | train_loss: 0.4166 | train_acc: 0.8328 | test_loss: 0.6610 | test_acc: 1.0000\n",
      "Epoch: 767 | train_loss: 0.4116 | train_acc: 0.8198 | test_loss: 0.6971 | test_acc: 0.0000\n",
      "Epoch: 768 | train_loss: 0.3960 | train_acc: 0.8374 | test_loss: 0.6451 | test_acc: 1.0000\n",
      "Epoch: 769 | train_loss: 0.3930 | train_acc: 0.8470 | test_loss: 0.6636 | test_acc: 1.0000\n",
      "Epoch: 770 | train_loss: 0.4136 | train_acc: 0.8311 | test_loss: 0.6539 | test_acc: 1.0000\n",
      "Epoch: 771 | train_loss: 0.3780 | train_acc: 0.8534 | test_loss: 0.7164 | test_acc: 0.0000\n",
      "Epoch: 772 | train_loss: 0.3818 | train_acc: 0.8431 | test_loss: 0.7083 | test_acc: 0.0000\n",
      "Epoch: 773 | train_loss: 0.4045 | train_acc: 0.8271 | test_loss: 0.7467 | test_acc: 0.0000\n",
      "Epoch: 774 | train_loss: 0.4132 | train_acc: 0.8351 | test_loss: 0.6068 | test_acc: 1.0000\n",
      "Epoch: 775 | train_loss: 0.4007 | train_acc: 0.8278 | test_loss: 0.6976 | test_acc: 0.0000\n",
      "Epoch: 776 | train_loss: 0.3989 | train_acc: 0.8399 | test_loss: 0.7091 | test_acc: 0.0000\n",
      "Epoch: 777 | train_loss: 0.4095 | train_acc: 0.8118 | test_loss: 0.7126 | test_acc: 0.0000\n",
      "Epoch: 778 | train_loss: 0.3890 | train_acc: 0.8397 | test_loss: 0.7215 | test_acc: 0.0000\n",
      "Epoch: 779 | train_loss: 0.4192 | train_acc: 0.8271 | test_loss: 0.7423 | test_acc: 0.0000\n",
      "Epoch: 780 | train_loss: 0.4049 | train_acc: 0.8374 | test_loss: 0.6206 | test_acc: 1.0000\n",
      "Epoch: 781 | train_loss: 0.3827 | train_acc: 0.8351 | test_loss: 0.7414 | test_acc: 0.0000\n",
      "Epoch: 782 | train_loss: 0.3916 | train_acc: 0.8303 | test_loss: 0.7028 | test_acc: 0.0000\n",
      "Epoch: 783 | train_loss: 0.3981 | train_acc: 0.8488 | test_loss: 0.6490 | test_acc: 1.0000\n",
      "Epoch: 784 | train_loss: 0.4095 | train_acc: 0.8415 | test_loss: 0.6937 | test_acc: 0.0000\n",
      "Epoch: 785 | train_loss: 0.3812 | train_acc: 0.8527 | test_loss: 0.7096 | test_acc: 0.0000\n",
      "Epoch: 786 | train_loss: 0.4124 | train_acc: 0.8231 | test_loss: 0.6977 | test_acc: 0.0000\n",
      "Epoch: 787 | train_loss: 0.3879 | train_acc: 0.8518 | test_loss: 0.6578 | test_acc: 1.0000\n",
      "Epoch: 788 | train_loss: 0.3707 | train_acc: 0.8470 | test_loss: 0.6836 | test_acc: 1.0000\n",
      "Epoch: 789 | train_loss: 0.4048 | train_acc: 0.8360 | test_loss: 0.7580 | test_acc: 0.0000\n",
      "Epoch: 790 | train_loss: 0.3877 | train_acc: 0.8344 | test_loss: 0.6710 | test_acc: 1.0000\n",
      "Epoch: 791 | train_loss: 0.3824 | train_acc: 0.8463 | test_loss: 0.7428 | test_acc: 0.0000\n",
      "Epoch: 792 | train_loss: 0.4102 | train_acc: 0.8240 | test_loss: 0.6771 | test_acc: 1.0000\n",
      "Epoch: 793 | train_loss: 0.3877 | train_acc: 0.8502 | test_loss: 0.6796 | test_acc: 1.0000\n",
      "Epoch: 794 | train_loss: 0.3985 | train_acc: 0.8479 | test_loss: 0.8030 | test_acc: 0.0000\n",
      "Epoch: 795 | train_loss: 0.3774 | train_acc: 0.8462 | test_loss: 0.7074 | test_acc: 0.0000\n",
      "Epoch: 796 | train_loss: 0.3890 | train_acc: 0.8399 | test_loss: 0.6481 | test_acc: 1.0000\n",
      "Epoch: 797 | train_loss: 0.3783 | train_acc: 0.8247 | test_loss: 0.6871 | test_acc: 1.0000\n",
      "Epoch: 798 | train_loss: 0.3876 | train_acc: 0.8376 | test_loss: 0.7003 | test_acc: 0.0000\n",
      "Epoch: 799 | train_loss: 0.3794 | train_acc: 0.8472 | test_loss: 0.6082 | test_acc: 1.0000\n",
      "Epoch: 800 | train_loss: 0.3759 | train_acc: 0.8374 | test_loss: 0.7014 | test_acc: 0.0000\n",
      "Epoch: 801 | train_loss: 0.3821 | train_acc: 0.8502 | test_loss: 0.6649 | test_acc: 1.0000\n",
      "Epoch: 802 | train_loss: 0.4257 | train_acc: 0.8272 | test_loss: 0.6134 | test_acc: 1.0000\n",
      "Epoch: 803 | train_loss: 0.3978 | train_acc: 0.8392 | test_loss: 0.6344 | test_acc: 1.0000\n",
      "Epoch: 804 | train_loss: 0.4009 | train_acc: 0.8429 | test_loss: 0.6628 | test_acc: 1.0000\n",
      "Epoch: 805 | train_loss: 0.3876 | train_acc: 0.8511 | test_loss: 0.7523 | test_acc: 0.0000\n",
      "Epoch: 806 | train_loss: 0.3755 | train_acc: 0.8510 | test_loss: 0.7259 | test_acc: 0.0000\n",
      "Epoch: 807 | train_loss: 0.3973 | train_acc: 0.8199 | test_loss: 0.7110 | test_acc: 0.0000\n",
      "Epoch: 808 | train_loss: 0.3899 | train_acc: 0.8295 | test_loss: 0.6400 | test_acc: 1.0000\n",
      "Epoch: 809 | train_loss: 0.3943 | train_acc: 0.8311 | test_loss: 0.7777 | test_acc: 0.0000\n",
      "Epoch: 810 | train_loss: 0.3954 | train_acc: 0.8351 | test_loss: 0.6438 | test_acc: 1.0000\n",
      "Epoch: 811 | train_loss: 0.3901 | train_acc: 0.8326 | test_loss: 0.7301 | test_acc: 0.0000\n",
      "Epoch: 812 | train_loss: 0.3790 | train_acc: 0.8454 | test_loss: 0.7651 | test_acc: 0.0000\n",
      "Epoch: 813 | train_loss: 0.3890 | train_acc: 0.8367 | test_loss: 0.6948 | test_acc: 0.0000\n",
      "Epoch: 814 | train_loss: 0.3770 | train_acc: 0.8383 | test_loss: 0.6778 | test_acc: 1.0000\n",
      "Epoch: 815 | train_loss: 0.3876 | train_acc: 0.8301 | test_loss: 0.7371 | test_acc: 0.0000\n",
      "Epoch: 816 | train_loss: 0.3738 | train_acc: 0.8488 | test_loss: 0.6755 | test_acc: 1.0000\n",
      "Epoch: 817 | train_loss: 0.4006 | train_acc: 0.8454 | test_loss: 0.6382 | test_acc: 1.0000\n",
      "Epoch: 818 | train_loss: 0.3792 | train_acc: 0.8422 | test_loss: 0.6322 | test_acc: 1.0000\n",
      "Epoch: 819 | train_loss: 0.3988 | train_acc: 0.8253 | test_loss: 0.7021 | test_acc: 0.0000\n",
      "Epoch: 820 | train_loss: 0.3890 | train_acc: 0.8558 | test_loss: 0.6269 | test_acc: 1.0000\n",
      "Epoch: 821 | train_loss: 0.3886 | train_acc: 0.8454 | test_loss: 0.6483 | test_acc: 1.0000\n",
      "Epoch: 822 | train_loss: 0.3785 | train_acc: 0.8527 | test_loss: 0.6148 | test_acc: 1.0000\n",
      "Epoch: 823 | train_loss: 0.3878 | train_acc: 0.8344 | test_loss: 0.7142 | test_acc: 0.0000\n",
      "Epoch: 824 | train_loss: 0.3798 | train_acc: 0.8473 | test_loss: 0.7715 | test_acc: 0.0000\n",
      "Epoch: 825 | train_loss: 0.3890 | train_acc: 0.8381 | test_loss: 0.7142 | test_acc: 0.0000\n",
      "Epoch: 826 | train_loss: 0.4084 | train_acc: 0.8285 | test_loss: 0.7286 | test_acc: 0.0000\n",
      "Epoch: 827 | train_loss: 0.3898 | train_acc: 0.8278 | test_loss: 0.6151 | test_acc: 1.0000\n",
      "Epoch: 828 | train_loss: 0.3878 | train_acc: 0.8278 | test_loss: 0.6805 | test_acc: 1.0000\n",
      "Epoch: 829 | train_loss: 0.3590 | train_acc: 0.8631 | test_loss: 0.6270 | test_acc: 1.0000\n",
      "Epoch: 830 | train_loss: 0.3958 | train_acc: 0.8374 | test_loss: 0.7268 | test_acc: 0.0000\n",
      "Epoch: 831 | train_loss: 0.4017 | train_acc: 0.8494 | test_loss: 0.8180 | test_acc: 0.0000\n",
      "Epoch: 832 | train_loss: 0.4169 | train_acc: 0.8269 | test_loss: 0.7434 | test_acc: 0.0000\n",
      "Epoch: 833 | train_loss: 0.3746 | train_acc: 0.8479 | test_loss: 0.6808 | test_acc: 1.0000\n",
      "Epoch: 834 | train_loss: 0.3861 | train_acc: 0.8409 | test_loss: 0.6749 | test_acc: 1.0000\n",
      "Epoch: 835 | train_loss: 0.3943 | train_acc: 0.8431 | test_loss: 0.6485 | test_acc: 1.0000\n",
      "Epoch: 836 | train_loss: 0.3885 | train_acc: 0.8534 | test_loss: 0.6540 | test_acc: 1.0000\n",
      "Epoch: 837 | train_loss: 0.3636 | train_acc: 0.8542 | test_loss: 0.6725 | test_acc: 1.0000\n",
      "Epoch: 838 | train_loss: 0.3751 | train_acc: 0.8447 | test_loss: 0.6744 | test_acc: 1.0000\n",
      "Epoch: 839 | train_loss: 0.4103 | train_acc: 0.8381 | test_loss: 0.6815 | test_acc: 1.0000\n",
      "Epoch: 840 | train_loss: 0.3806 | train_acc: 0.8288 | test_loss: 0.7078 | test_acc: 0.0000\n",
      "Epoch: 841 | train_loss: 0.4047 | train_acc: 0.8358 | test_loss: 0.5670 | test_acc: 1.0000\n",
      "Epoch: 842 | train_loss: 0.3687 | train_acc: 0.8424 | test_loss: 0.5886 | test_acc: 1.0000\n",
      "Epoch: 843 | train_loss: 0.3736 | train_acc: 0.8520 | test_loss: 0.6421 | test_acc: 1.0000\n",
      "Epoch: 844 | train_loss: 0.3887 | train_acc: 0.8431 | test_loss: 0.6714 | test_acc: 1.0000\n",
      "Epoch: 845 | train_loss: 0.3797 | train_acc: 0.8494 | test_loss: 0.7352 | test_acc: 0.0000\n",
      "Epoch: 846 | train_loss: 0.3712 | train_acc: 0.8511 | test_loss: 0.6821 | test_acc: 1.0000\n",
      "Epoch: 847 | train_loss: 0.3705 | train_acc: 0.8408 | test_loss: 0.6690 | test_acc: 1.0000\n",
      "Epoch: 848 | train_loss: 0.3819 | train_acc: 0.8447 | test_loss: 0.6499 | test_acc: 1.0000\n",
      "Epoch: 849 | train_loss: 0.4013 | train_acc: 0.8431 | test_loss: 0.7228 | test_acc: 0.0000\n",
      "Epoch: 850 | train_loss: 0.3683 | train_acc: 0.8486 | test_loss: 0.6575 | test_acc: 1.0000\n",
      "Epoch: 851 | train_loss: 0.3815 | train_acc: 0.8472 | test_loss: 0.6588 | test_acc: 1.0000\n",
      "Epoch: 852 | train_loss: 0.3861 | train_acc: 0.8454 | test_loss: 0.6887 | test_acc: 1.0000\n",
      "Epoch: 853 | train_loss: 0.3887 | train_acc: 0.8470 | test_loss: 0.7420 | test_acc: 0.0000\n",
      "Epoch: 854 | train_loss: 0.3807 | train_acc: 0.8408 | test_loss: 0.6999 | test_acc: 0.0000\n",
      "Epoch: 855 | train_loss: 0.3911 | train_acc: 0.8335 | test_loss: 0.6950 | test_acc: 0.0000\n",
      "Epoch: 856 | train_loss: 0.3641 | train_acc: 0.8671 | test_loss: 0.6648 | test_acc: 1.0000\n",
      "Epoch: 857 | train_loss: 0.3891 | train_acc: 0.8390 | test_loss: 0.6406 | test_acc: 1.0000\n",
      "Epoch: 858 | train_loss: 0.3715 | train_acc: 0.8511 | test_loss: 0.6708 | test_acc: 1.0000\n",
      "Epoch: 859 | train_loss: 0.3905 | train_acc: 0.8462 | test_loss: 0.6587 | test_acc: 1.0000\n",
      "Epoch: 860 | train_loss: 0.3583 | train_acc: 0.8582 | test_loss: 0.6082 | test_acc: 1.0000\n",
      "Epoch: 861 | train_loss: 0.3557 | train_acc: 0.8575 | test_loss: 0.6344 | test_acc: 1.0000\n",
      "Epoch: 862 | train_loss: 0.3768 | train_acc: 0.8495 | test_loss: 0.6269 | test_acc: 1.0000\n",
      "Epoch: 863 | train_loss: 0.3823 | train_acc: 0.8464 | test_loss: 0.6638 | test_acc: 1.0000\n",
      "Epoch: 864 | train_loss: 0.3736 | train_acc: 0.8559 | test_loss: 0.6186 | test_acc: 1.0000\n",
      "Epoch: 865 | train_loss: 0.3736 | train_acc: 0.8536 | test_loss: 0.6656 | test_acc: 1.0000\n",
      "Epoch: 866 | train_loss: 0.3738 | train_acc: 0.8582 | test_loss: 0.6814 | test_acc: 1.0000\n",
      "Epoch: 867 | train_loss: 0.3740 | train_acc: 0.8534 | test_loss: 0.6784 | test_acc: 1.0000\n",
      "Epoch: 868 | train_loss: 0.3665 | train_acc: 0.8429 | test_loss: 0.5365 | test_acc: 1.0000\n",
      "Epoch: 869 | train_loss: 0.3991 | train_acc: 0.8360 | test_loss: 0.5825 | test_acc: 1.0000\n",
      "Epoch: 870 | train_loss: 0.3737 | train_acc: 0.8590 | test_loss: 0.6081 | test_acc: 1.0000\n",
      "Epoch: 871 | train_loss: 0.3657 | train_acc: 0.8631 | test_loss: 0.6357 | test_acc: 1.0000\n",
      "Epoch: 872 | train_loss: 0.3902 | train_acc: 0.8504 | test_loss: 0.6592 | test_acc: 1.0000\n",
      "Epoch: 873 | train_loss: 0.3808 | train_acc: 0.8360 | test_loss: 0.6691 | test_acc: 1.0000\n",
      "Epoch: 874 | train_loss: 0.3750 | train_acc: 0.8543 | test_loss: 0.6736 | test_acc: 1.0000\n",
      "Epoch: 875 | train_loss: 0.3571 | train_acc: 0.8478 | test_loss: 0.6387 | test_acc: 1.0000\n",
      "Epoch: 876 | train_loss: 0.3766 | train_acc: 0.8429 | test_loss: 0.6561 | test_acc: 1.0000\n",
      "Epoch: 877 | train_loss: 0.3889 | train_acc: 0.8367 | test_loss: 0.6346 | test_acc: 1.0000\n",
      "Epoch: 878 | train_loss: 0.3746 | train_acc: 0.8328 | test_loss: 0.6403 | test_acc: 1.0000\n",
      "Epoch: 879 | train_loss: 0.3876 | train_acc: 0.8454 | test_loss: 0.7359 | test_acc: 0.0000\n",
      "Epoch: 880 | train_loss: 0.3764 | train_acc: 0.8429 | test_loss: 0.7601 | test_acc: 0.0000\n",
      "Epoch: 881 | train_loss: 0.3745 | train_acc: 0.8440 | test_loss: 0.7158 | test_acc: 0.0000\n",
      "Epoch: 882 | train_loss: 0.3743 | train_acc: 0.8511 | test_loss: 0.6686 | test_acc: 1.0000\n",
      "Epoch: 883 | train_loss: 0.3622 | train_acc: 0.8534 | test_loss: 0.6950 | test_acc: 0.0000\n",
      "Epoch: 884 | train_loss: 0.3634 | train_acc: 0.8543 | test_loss: 0.7473 | test_acc: 0.0000\n",
      "Epoch: 885 | train_loss: 0.3814 | train_acc: 0.8415 | test_loss: 0.6840 | test_acc: 1.0000\n",
      "Epoch: 886 | train_loss: 0.3906 | train_acc: 0.8317 | test_loss: 0.7163 | test_acc: 0.0000\n",
      "Epoch: 887 | train_loss: 0.3746 | train_acc: 0.8310 | test_loss: 0.7009 | test_acc: 0.0000\n",
      "Epoch: 888 | train_loss: 0.3733 | train_acc: 0.8623 | test_loss: 0.6559 | test_acc: 1.0000\n",
      "Epoch: 889 | train_loss: 0.3489 | train_acc: 0.8559 | test_loss: 0.6275 | test_acc: 1.0000\n",
      "Epoch: 890 | train_loss: 0.3920 | train_acc: 0.8344 | test_loss: 0.6375 | test_acc: 1.0000\n",
      "Epoch: 891 | train_loss: 0.3735 | train_acc: 0.8502 | test_loss: 0.6410 | test_acc: 1.0000\n",
      "Epoch: 892 | train_loss: 0.3882 | train_acc: 0.8470 | test_loss: 0.6471 | test_acc: 1.0000\n",
      "Epoch: 893 | train_loss: 0.3842 | train_acc: 0.8352 | test_loss: 0.7060 | test_acc: 0.0000\n",
      "Epoch: 894 | train_loss: 0.3462 | train_acc: 0.8727 | test_loss: 0.5902 | test_acc: 1.0000\n",
      "Epoch: 895 | train_loss: 0.3970 | train_acc: 0.8495 | test_loss: 0.6377 | test_acc: 1.0000\n",
      "Epoch: 896 | train_loss: 0.3898 | train_acc: 0.8510 | test_loss: 0.6166 | test_acc: 1.0000\n",
      "Epoch: 897 | train_loss: 0.4071 | train_acc: 0.8342 | test_loss: 0.6871 | test_acc: 1.0000\n",
      "Epoch: 898 | train_loss: 0.3659 | train_acc: 0.8383 | test_loss: 0.6167 | test_acc: 1.0000\n",
      "Epoch: 899 | train_loss: 0.3636 | train_acc: 0.8472 | test_loss: 0.6559 | test_acc: 1.0000\n",
      "Epoch: 900 | train_loss: 0.3761 | train_acc: 0.8504 | test_loss: 0.6931 | test_acc: 1.0000\n",
      "Epoch: 901 | train_loss: 0.3795 | train_acc: 0.8575 | test_loss: 0.6778 | test_acc: 1.0000\n",
      "Epoch: 902 | train_loss: 0.3826 | train_acc: 0.8582 | test_loss: 0.6257 | test_acc: 1.0000\n",
      "Epoch: 903 | train_loss: 0.3559 | train_acc: 0.8510 | test_loss: 0.7343 | test_acc: 0.0000\n",
      "Epoch: 904 | train_loss: 0.3678 | train_acc: 0.8542 | test_loss: 0.6672 | test_acc: 1.0000\n",
      "Epoch: 905 | train_loss: 0.3676 | train_acc: 0.8390 | test_loss: 0.6731 | test_acc: 1.0000\n",
      "Epoch: 906 | train_loss: 0.3687 | train_acc: 0.8253 | test_loss: 0.6768 | test_acc: 1.0000\n",
      "Epoch: 907 | train_loss: 0.3847 | train_acc: 0.8510 | test_loss: 0.6308 | test_acc: 1.0000\n",
      "Epoch: 908 | train_loss: 0.3799 | train_acc: 0.8703 | test_loss: 0.6522 | test_acc: 1.0000\n",
      "Epoch: 909 | train_loss: 0.3865 | train_acc: 0.8431 | test_loss: 0.6181 | test_acc: 1.0000\n",
      "Epoch: 910 | train_loss: 0.3841 | train_acc: 0.8463 | test_loss: 0.6301 | test_acc: 1.0000\n",
      "Epoch: 911 | train_loss: 0.3440 | train_acc: 0.8575 | test_loss: 0.6508 | test_acc: 1.0000\n",
      "Epoch: 912 | train_loss: 0.3881 | train_acc: 0.8527 | test_loss: 0.6170 | test_acc: 1.0000\n",
      "Epoch: 913 | train_loss: 0.3895 | train_acc: 0.8486 | test_loss: 0.5834 | test_acc: 1.0000\n",
      "Epoch: 914 | train_loss: 0.3554 | train_acc: 0.8536 | test_loss: 0.6043 | test_acc: 1.0000\n",
      "Epoch: 915 | train_loss: 0.3857 | train_acc: 0.8518 | test_loss: 0.5674 | test_acc: 1.0000\n",
      "Epoch: 916 | train_loss: 0.3631 | train_acc: 0.8295 | test_loss: 0.5871 | test_acc: 1.0000\n",
      "Epoch: 917 | train_loss: 0.3629 | train_acc: 0.8495 | test_loss: 0.5885 | test_acc: 1.0000\n",
      "Epoch: 918 | train_loss: 0.3704 | train_acc: 0.8575 | test_loss: 0.5875 | test_acc: 1.0000\n",
      "Epoch: 919 | train_loss: 0.3734 | train_acc: 0.8486 | test_loss: 0.6962 | test_acc: 0.0000\n",
      "Epoch: 920 | train_loss: 0.4099 | train_acc: 0.8374 | test_loss: 0.5590 | test_acc: 1.0000\n",
      "Epoch: 921 | train_loss: 0.3901 | train_acc: 0.8513 | test_loss: 0.7221 | test_acc: 0.0000\n",
      "Epoch: 922 | train_loss: 0.3800 | train_acc: 0.8448 | test_loss: 0.7025 | test_acc: 0.0000\n",
      "Epoch: 923 | train_loss: 0.3653 | train_acc: 0.8454 | test_loss: 0.6497 | test_acc: 1.0000\n",
      "Epoch: 924 | train_loss: 0.3644 | train_acc: 0.8511 | test_loss: 0.6659 | test_acc: 1.0000\n",
      "Epoch: 925 | train_loss: 0.3839 | train_acc: 0.8623 | test_loss: 0.6380 | test_acc: 1.0000\n",
      "Epoch: 926 | train_loss: 0.3608 | train_acc: 0.8559 | test_loss: 0.6600 | test_acc: 1.0000\n",
      "Epoch: 927 | train_loss: 0.3588 | train_acc: 0.8456 | test_loss: 0.6487 | test_acc: 1.0000\n",
      "Epoch: 928 | train_loss: 0.3812 | train_acc: 0.8438 | test_loss: 0.6043 | test_acc: 1.0000\n",
      "Epoch: 929 | train_loss: 0.3602 | train_acc: 0.8527 | test_loss: 0.6029 | test_acc: 1.0000\n",
      "Epoch: 930 | train_loss: 0.3447 | train_acc: 0.8582 | test_loss: 0.5912 | test_acc: 1.0000\n",
      "Epoch: 931 | train_loss: 0.3572 | train_acc: 0.8480 | test_loss: 0.6419 | test_acc: 1.0000\n",
      "Epoch: 932 | train_loss: 0.3596 | train_acc: 0.8479 | test_loss: 0.5987 | test_acc: 1.0000\n",
      "Epoch: 933 | train_loss: 0.3977 | train_acc: 0.8374 | test_loss: 0.6480 | test_acc: 1.0000\n",
      "Epoch: 934 | train_loss: 0.3579 | train_acc: 0.8454 | test_loss: 0.6685 | test_acc: 1.0000\n",
      "Epoch: 935 | train_loss: 0.3710 | train_acc: 0.8462 | test_loss: 0.6945 | test_acc: 0.0000\n",
      "Epoch: 936 | train_loss: 0.3719 | train_acc: 0.8278 | test_loss: 0.6503 | test_acc: 1.0000\n",
      "Epoch: 937 | train_loss: 0.3573 | train_acc: 0.8648 | test_loss: 0.5779 | test_acc: 1.0000\n",
      "Epoch: 938 | train_loss: 0.3745 | train_acc: 0.8559 | test_loss: 0.6333 | test_acc: 1.0000\n",
      "Epoch: 939 | train_loss: 0.3796 | train_acc: 0.8399 | test_loss: 0.5803 | test_acc: 1.0000\n",
      "Epoch: 940 | train_loss: 0.3679 | train_acc: 0.8574 | test_loss: 0.5947 | test_acc: 1.0000\n",
      "Epoch: 941 | train_loss: 0.3446 | train_acc: 0.8798 | test_loss: 0.6046 | test_acc: 1.0000\n",
      "Epoch: 942 | train_loss: 0.3443 | train_acc: 0.8558 | test_loss: 0.6586 | test_acc: 1.0000\n",
      "Epoch: 943 | train_loss: 0.3621 | train_acc: 0.8671 | test_loss: 0.5743 | test_acc: 1.0000\n",
      "Epoch: 944 | train_loss: 0.3594 | train_acc: 0.8550 | test_loss: 0.5825 | test_acc: 1.0000\n",
      "Epoch: 945 | train_loss: 0.3665 | train_acc: 0.8488 | test_loss: 0.5869 | test_acc: 1.0000\n",
      "Epoch: 946 | train_loss: 0.3818 | train_acc: 0.8534 | test_loss: 0.6044 | test_acc: 1.0000\n",
      "Epoch: 947 | train_loss: 0.3494 | train_acc: 0.8711 | test_loss: 0.6965 | test_acc: 0.0000\n",
      "Epoch: 948 | train_loss: 0.3609 | train_acc: 0.8575 | test_loss: 0.6131 | test_acc: 1.0000\n",
      "Epoch: 949 | train_loss: 0.3638 | train_acc: 0.8521 | test_loss: 0.6405 | test_acc: 1.0000\n",
      "Epoch: 950 | train_loss: 0.3638 | train_acc: 0.8537 | test_loss: 0.6844 | test_acc: 1.0000\n",
      "Epoch: 951 | train_loss: 0.3436 | train_acc: 0.8593 | test_loss: 0.6329 | test_acc: 1.0000\n",
      "Epoch: 952 | train_loss: 0.3590 | train_acc: 0.8463 | test_loss: 0.5978 | test_acc: 1.0000\n",
      "Epoch: 953 | train_loss: 0.3587 | train_acc: 0.8568 | test_loss: 0.5571 | test_acc: 1.0000\n",
      "Epoch: 954 | train_loss: 0.3623 | train_acc: 0.8472 | test_loss: 0.5327 | test_acc: 1.0000\n",
      "Epoch: 955 | train_loss: 0.3821 | train_acc: 0.8687 | test_loss: 0.5873 | test_acc: 1.0000\n",
      "Epoch: 956 | train_loss: 0.3646 | train_acc: 0.8527 | test_loss: 0.5696 | test_acc: 1.0000\n",
      "Epoch: 957 | train_loss: 0.3404 | train_acc: 0.8735 | test_loss: 0.5813 | test_acc: 1.0000\n",
      "Epoch: 958 | train_loss: 0.3596 | train_acc: 0.8559 | test_loss: 0.5846 | test_acc: 1.0000\n",
      "Epoch: 959 | train_loss: 0.3487 | train_acc: 0.8655 | test_loss: 0.5904 | test_acc: 1.0000\n",
      "Epoch: 960 | train_loss: 0.3701 | train_acc: 0.8504 | test_loss: 0.6623 | test_acc: 1.0000\n",
      "Epoch: 961 | train_loss: 0.3625 | train_acc: 0.8711 | test_loss: 0.5723 | test_acc: 1.0000\n",
      "Epoch: 962 | train_loss: 0.3735 | train_acc: 0.8591 | test_loss: 0.6054 | test_acc: 1.0000\n",
      "Epoch: 963 | train_loss: 0.3845 | train_acc: 0.8495 | test_loss: 0.6366 | test_acc: 1.0000\n",
      "Epoch: 964 | train_loss: 0.3528 | train_acc: 0.8591 | test_loss: 0.5730 | test_acc: 1.0000\n",
      "Epoch: 965 | train_loss: 0.3534 | train_acc: 0.8486 | test_loss: 0.6612 | test_acc: 1.0000\n",
      "Epoch: 966 | train_loss: 0.3559 | train_acc: 0.8518 | test_loss: 0.5588 | test_acc: 1.0000\n",
      "Epoch: 967 | train_loss: 0.3564 | train_acc: 0.8601 | test_loss: 0.6085 | test_acc: 1.0000\n",
      "Epoch: 968 | train_loss: 0.3610 | train_acc: 0.8696 | test_loss: 0.6089 | test_acc: 1.0000\n",
      "Epoch: 969 | train_loss: 0.3503 | train_acc: 0.8575 | test_loss: 0.6065 | test_acc: 1.0000\n",
      "Epoch: 970 | train_loss: 0.3496 | train_acc: 0.8598 | test_loss: 0.5512 | test_acc: 1.0000\n",
      "Epoch: 971 | train_loss: 0.3593 | train_acc: 0.8518 | test_loss: 0.5981 | test_acc: 1.0000\n",
      "Epoch: 972 | train_loss: 0.3539 | train_acc: 0.8559 | test_loss: 0.6583 | test_acc: 1.0000\n",
      "Epoch: 973 | train_loss: 0.3534 | train_acc: 0.8679 | test_loss: 0.6190 | test_acc: 1.0000\n",
      "Epoch: 974 | train_loss: 0.3528 | train_acc: 0.8448 | test_loss: 0.6343 | test_acc: 1.0000\n",
      "Epoch: 975 | train_loss: 0.3476 | train_acc: 0.8494 | test_loss: 0.6089 | test_acc: 1.0000\n",
      "Epoch: 976 | train_loss: 0.3516 | train_acc: 0.8632 | test_loss: 0.7207 | test_acc: 0.0000\n",
      "Epoch: 977 | train_loss: 0.3586 | train_acc: 0.8552 | test_loss: 0.5929 | test_acc: 1.0000\n",
      "Epoch: 978 | train_loss: 0.3627 | train_acc: 0.8606 | test_loss: 0.6067 | test_acc: 1.0000\n",
      "Epoch: 979 | train_loss: 0.3496 | train_acc: 0.8559 | test_loss: 0.7018 | test_acc: 0.0000\n",
      "Epoch: 980 | train_loss: 0.3728 | train_acc: 0.8351 | test_loss: 0.5939 | test_acc: 1.0000\n",
      "Epoch: 981 | train_loss: 0.3748 | train_acc: 0.8438 | test_loss: 0.6073 | test_acc: 1.0000\n",
      "Epoch: 982 | train_loss: 0.3401 | train_acc: 0.8647 | test_loss: 0.6005 | test_acc: 1.0000\n",
      "Epoch: 983 | train_loss: 0.3493 | train_acc: 0.8615 | test_loss: 0.5835 | test_acc: 1.0000\n",
      "Epoch: 984 | train_loss: 0.3537 | train_acc: 0.8632 | test_loss: 0.6698 | test_acc: 1.0000\n",
      "Epoch: 985 | train_loss: 0.3883 | train_acc: 0.8358 | test_loss: 0.6241 | test_acc: 1.0000\n",
      "Epoch: 986 | train_loss: 0.3828 | train_acc: 0.8510 | test_loss: 0.5486 | test_acc: 1.0000\n",
      "Epoch: 987 | train_loss: 0.3658 | train_acc: 0.8422 | test_loss: 0.5772 | test_acc: 1.0000\n",
      "Epoch: 988 | train_loss: 0.3660 | train_acc: 0.8639 | test_loss: 0.6152 | test_acc: 1.0000\n",
      "Epoch: 989 | train_loss: 0.3453 | train_acc: 0.8711 | test_loss: 0.6148 | test_acc: 1.0000\n",
      "Epoch: 990 | train_loss: 0.3309 | train_acc: 0.8727 | test_loss: 0.5663 | test_acc: 1.0000\n",
      "Epoch: 991 | train_loss: 0.3443 | train_acc: 0.8622 | test_loss: 0.6264 | test_acc: 1.0000\n",
      "Epoch: 992 | train_loss: 0.3606 | train_acc: 0.8542 | test_loss: 0.5667 | test_acc: 1.0000\n",
      "Epoch: 993 | train_loss: 0.3480 | train_acc: 0.8689 | test_loss: 0.5367 | test_acc: 1.0000\n",
      "Epoch: 994 | train_loss: 0.3656 | train_acc: 0.8673 | test_loss: 0.6703 | test_acc: 1.0000\n",
      "Epoch: 995 | train_loss: 0.3659 | train_acc: 0.8486 | test_loss: 0.5106 | test_acc: 1.0000\n",
      "Epoch: 996 | train_loss: 0.3479 | train_acc: 0.8654 | test_loss: 0.6085 | test_acc: 1.0000\n",
      "Epoch: 997 | train_loss: 0.3559 | train_acc: 0.8542 | test_loss: 0.6102 | test_acc: 1.0000\n",
      "Epoch: 998 | train_loss: 0.3733 | train_acc: 0.8534 | test_loss: 0.5570 | test_acc: 1.0000\n",
      "Epoch: 999 | train_loss: 0.3530 | train_acc: 0.8623 | test_loss: 0.6319 | test_acc: 1.0000\n",
      "Epoch: 1000 | train_loss: 0.3729 | train_acc: 0.8711 | test_loss: 0.5263 | test_acc: 1.0000\n",
      "Total training time: 84.743 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 1000\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0\n",
    "model_0_results = train(model=model_pattern,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "45318010-3ce2-4150-bc3e-7d3a285ff860",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_pattern, \"model_pattern_mlp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "75e7f444-ff2d-437b-83a5-86fe48806d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_trend, \"model_trend_mlp.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "35e05adb-fe03-4fd6-b4b2-40f26cb73c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"gaf_pattern_3dcnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1496c58a-902c-46b5-b26a-6ce0f6e7e764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"gaf_pattern_2dcnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2209196-fa37-4bd2-8c46-c8a8ee2ff7fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7af0641c-33e5-44c7-8805-fa2a9a8ce8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryAccuracy:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply sigmoid to logits to get probabilities\n",
    "        probabilities = torch.sigmoid(logits).squeeze(dim=1)\n",
    "        # Convert probabilities to binary predictions\n",
    "        predictions = (probabilities >= self.threshold).float()\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()\n",
    "        return accuracy.item()\n",
    "\n",
    "class MultiClassAccuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply softmax to logits to get class probabilities (optional, for insight)\n",
    "        # probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the predicted class indices by applying argmax to logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()  # Total number of samples\n",
    "        return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c53d927-7644-4030-9fdc-fdb36faef75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 1. Forward pass\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # print(X.shape)\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # print(y_pred)\n",
    "        # print()\n",
    "        # print(y)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred.squeeze(1), y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        train_acc += accuracy_fn(y_pred, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e8791b8-9bb8-45df-9035-8f2b977760fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits.squeeze(1), y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_acc += accuracy_fn(test_pred_logits, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7791ac48-71a1-44e4-8d43-1b231144dfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int = 5):\n",
    "\n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "\n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4d8817e4-a73a-4d23-8f70-89b5e922ca29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Engulfing Bearish', 'Engulfing Bullish'], dtype='<U17')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e93bee1-7422-4b39-a468-a7b8a2194409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Engulfing Bearish', 'Engulfing Bullish'], dtype='<U17')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.inverse_transform([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f9c87e-14bc-4dff-b1c5-4043e3d6c9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace629d3-a9f2-4219-becc-c2181a8bc9cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59bc16c-e200-4103-adb2-293268429ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
