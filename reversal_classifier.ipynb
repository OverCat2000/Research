{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "ca3ea6b3-40f6-478b-a04e-7df824ce0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "3fe0cf69-c072-45b6-b1eb-9b54805d8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgDataLoader():\n",
    "    DATABASE_URL = \"postgresql://overcat:overmind@localhost:5432/stocks\"\n",
    "    query = \"\"\"\n",
    "    SELECT * from data.reversals;\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(DATABASE_URL)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            results = cur.fetchall()  # Fetch all rows from the query result\n",
    "            \n",
    "            for row in results:\n",
    "                matrix1 = np.array([\n",
    "                    row[1][\"Open\"],\n",
    "                    row[1][\"High\"],\n",
    "                    row[1][\"Low\"],\n",
    "                    row[1][\"Close\"],\n",
    "                    row[1][\"Volume\"]\n",
    "                    # row[1][\"Time\"]\n",
    "                ])\n",
    "    \n",
    "                matrix2 = np.array([\n",
    "                    row[2][\"Open\"],\n",
    "                    row[2][\"High\"],\n",
    "                    row[2][\"Low\"],\n",
    "                    row[2][\"Close\"],\n",
    "                    row[2][\"Volume\"]\n",
    "                    # row[2][\"Time\"]\n",
    "                ])\n",
    "    \n",
    "                # print(row[3], row[4])\n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.concatenate((matrix1, matrix2), axis=1), ax=axes, t0=row[4])\n",
    "    \n",
    "                matrix1 = np.moveaxis(matrix1, 1, 0)\n",
    "                matrix2 = np.moveaxis(matrix2, 1, 0)\n",
    "    \n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.moveaxis(np.concatenate((matrix1, matrix2)), 1, 0), ax=axes, t0=temp[0])\n",
    "                dataset.append(matrix1)\n",
    "                labels.append(row[5])\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "ac331467-8b7a-46e6-8edf-1177db13659f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1355, 1355)"
      ]
     },
     "execution_count": 561,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = pgDataLoader()\n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "0915eebd-a272-41b4-8b96-34f5d4262a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_data = []\n",
    "for i, sample in enumerate(data):\n",
    "    open_prices = sample[:, 0]\n",
    "    high_prices = sample[:, 1]\n",
    "    low_prices = sample[:, 2]\n",
    "    close_prices = sample[:, 3]\n",
    "    # volume = sample[:, 4]\n",
    "\n",
    "    body_length = np.abs(close_prices - open_prices)\n",
    "    upper_shadow_length = high_prices - np.maximum(open_prices, close_prices)\n",
    "    lower_shadow_length = np.minimum(open_prices, close_prices) - low_prices\n",
    "\n",
    "    alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices, volume)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length)), 1, 0)\n",
    "\n",
    "    alt_data.append(alt_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "b4ddaedc-41b5-40a3-909e-6d4c64d6ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.sequences = data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "ff95c1a5-fec5-46b0-bb13-d9d1b5f7f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.8*len(data))\n",
    "# test_size = len(data) - train_size\n",
    "# dataset = CustomDataset(data, labels)\n",
    "\n",
    "train_size = int(0.999*len(alt_data))\n",
    "test_size = len(alt_data) - train_size\n",
    "dataset = CustomDataset(alt_data, labels)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "1e9551c2-c8fc-46ba-9034-aad143552bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(hidden_size*14*2, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc(out)\n",
    "        # out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "ed90c19e-1ac4-4fab-ad5f-dcd6b27a1c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(5, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=1792, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 5\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 2\n",
    "\n",
    "model = LSTMClassifier(input_size, hidden_size, num_layers, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "ff599b89-3109-4c5a-bef5-ab82e6ca3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryAccuracy:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply sigmoid to logits to get probabilities\n",
    "        probabilities = torch.sigmoid(logits).squeeze(dim=1)\n",
    "        # Convert probabilities to binary predictions\n",
    "        predictions = (probabilities >= self.threshold).float()\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()\n",
    "        return accuracy.item()\n",
    "\n",
    "class MultiClassAccuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply softmax to logits to get class probabilities (optional, for insight)\n",
    "        # probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the predicted class indices by applying argmax to logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()  # Total number of samples\n",
    "        return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "5f5202a9-7bbe-415e-8f7b-d0649e6b5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n",
    "accuracy_fn = MultiClassAccuracy()\n",
    "# accuracy_fn = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "a54e50c2-5f8e-4264-aeb8-dd5119db3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 1. Forward pass\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # print(y_pred)\n",
    "        # print()\n",
    "        # print(y)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred.squeeze(1), y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        train_acc += accuracy_fn(y_pred, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "4692b420-0c1c-4a6d-9790-d7c63aada1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits.squeeze(1), y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_acc += accuracy_fn(test_pred_logits, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa45f7-eab5-4e7c-8706-676c196b0983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "11798971-37ff-4b95-9203-9bea26197884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int = 5):\n",
    "\n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "\n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "f2dae77a-3ca0-48f9-a86f-3f28d060386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54bd60f65494a5eabfa883bedf79e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 0.6956 | train_acc: 0.5374 | test_loss: 0.7630 | test_acc: 0.0000\n",
      "Epoch: 2 | train_loss: 0.6923 | train_acc: 0.5251 | test_loss: 0.7356 | test_acc: 0.0000\n",
      "Epoch: 3 | train_loss: 0.6881 | train_acc: 0.5477 | test_loss: 0.7355 | test_acc: 0.5000\n",
      "Epoch: 4 | train_loss: 0.6885 | train_acc: 0.5486 | test_loss: 0.8194 | test_acc: 0.0000\n",
      "Epoch: 5 | train_loss: 0.6895 | train_acc: 0.5431 | test_loss: 0.7413 | test_acc: 0.0000\n",
      "Epoch: 6 | train_loss: 0.6884 | train_acc: 0.5536 | test_loss: 0.7155 | test_acc: 0.5000\n",
      "Epoch: 7 | train_loss: 0.6861 | train_acc: 0.5520 | test_loss: 0.7511 | test_acc: 0.0000\n",
      "Epoch: 8 | train_loss: 0.6846 | train_acc: 0.5393 | test_loss: 0.7855 | test_acc: 0.0000\n",
      "Epoch: 9 | train_loss: 0.6845 | train_acc: 0.5699 | test_loss: 0.7747 | test_acc: 0.0000\n",
      "Epoch: 10 | train_loss: 0.6799 | train_acc: 0.5685 | test_loss: 0.6663 | test_acc: 1.0000\n",
      "Epoch: 11 | train_loss: 0.6840 | train_acc: 0.5751 | test_loss: 0.7917 | test_acc: 0.0000\n",
      "Epoch: 12 | train_loss: 0.6753 | train_acc: 0.5819 | test_loss: 0.8887 | test_acc: 0.0000\n",
      "Epoch: 13 | train_loss: 0.6743 | train_acc: 0.5889 | test_loss: 0.7239 | test_acc: 0.0000\n",
      "Epoch: 14 | train_loss: 0.6695 | train_acc: 0.6076 | test_loss: 0.7108 | test_acc: 0.5000\n",
      "Epoch: 15 | train_loss: 0.6644 | train_acc: 0.6113 | test_loss: 0.9472 | test_acc: 0.0000\n",
      "Epoch: 16 | train_loss: 0.6607 | train_acc: 0.6262 | test_loss: 1.0411 | test_acc: 0.0000\n",
      "Epoch: 17 | train_loss: 0.6675 | train_acc: 0.6034 | test_loss: 0.8954 | test_acc: 0.0000\n",
      "Epoch: 18 | train_loss: 0.6692 | train_acc: 0.6031 | test_loss: 0.9428 | test_acc: 0.0000\n",
      "Epoch: 19 | train_loss: 0.6507 | train_acc: 0.6292 | test_loss: 0.8105 | test_acc: 0.5000\n",
      "Epoch: 20 | train_loss: 0.6566 | train_acc: 0.6202 | test_loss: 0.8331 | test_acc: 0.0000\n",
      "Epoch: 21 | train_loss: 0.6505 | train_acc: 0.6176 | test_loss: 0.7665 | test_acc: 0.5000\n",
      "Epoch: 22 | train_loss: 0.6471 | train_acc: 0.6132 | test_loss: 1.1137 | test_acc: 0.0000\n",
      "Epoch: 23 | train_loss: 0.6572 | train_acc: 0.6248 | test_loss: 0.7353 | test_acc: 0.5000\n",
      "Epoch: 24 | train_loss: 0.6473 | train_acc: 0.6333 | test_loss: 0.7895 | test_acc: 0.5000\n",
      "Epoch: 25 | train_loss: 0.6458 | train_acc: 0.6350 | test_loss: 1.0534 | test_acc: 0.0000\n",
      "Epoch: 26 | train_loss: 0.6447 | train_acc: 0.6336 | test_loss: 0.7457 | test_acc: 0.5000\n",
      "Epoch: 27 | train_loss: 0.6442 | train_acc: 0.6350 | test_loss: 0.7570 | test_acc: 0.5000\n",
      "Epoch: 28 | train_loss: 0.6313 | train_acc: 0.6621 | test_loss: 0.9140 | test_acc: 0.0000\n",
      "Epoch: 29 | train_loss: 0.6316 | train_acc: 0.6507 | test_loss: 0.7779 | test_acc: 0.0000\n",
      "Epoch: 30 | train_loss: 0.6380 | train_acc: 0.6483 | test_loss: 0.8820 | test_acc: 0.0000\n",
      "Epoch: 31 | train_loss: 0.6359 | train_acc: 0.6511 | test_loss: 0.7912 | test_acc: 0.0000\n",
      "Epoch: 32 | train_loss: 0.6230 | train_acc: 0.6630 | test_loss: 0.6499 | test_acc: 0.5000\n",
      "Epoch: 33 | train_loss: 0.6414 | train_acc: 0.6306 | test_loss: 0.8438 | test_acc: 0.5000\n",
      "Epoch: 34 | train_loss: 0.6320 | train_acc: 0.6462 | test_loss: 0.7219 | test_acc: 0.5000\n",
      "Epoch: 35 | train_loss: 0.6283 | train_acc: 0.6694 | test_loss: 0.8972 | test_acc: 0.0000\n",
      "Epoch: 36 | train_loss: 0.6167 | train_acc: 0.6968 | test_loss: 0.9356 | test_acc: 0.5000\n",
      "Epoch: 37 | train_loss: 0.6202 | train_acc: 0.6780 | test_loss: 0.6868 | test_acc: 0.5000\n",
      "Epoch: 38 | train_loss: 0.6152 | train_acc: 0.6771 | test_loss: 0.8199 | test_acc: 0.5000\n",
      "Epoch: 39 | train_loss: 0.6267 | train_acc: 0.6613 | test_loss: 1.1195 | test_acc: 0.0000\n",
      "Epoch: 40 | train_loss: 0.6213 | train_acc: 0.6590 | test_loss: 0.7948 | test_acc: 0.5000\n",
      "Epoch: 41 | train_loss: 0.6123 | train_acc: 0.6843 | test_loss: 0.5566 | test_acc: 1.0000\n",
      "Epoch: 42 | train_loss: 0.6205 | train_acc: 0.6599 | test_loss: 0.6971 | test_acc: 0.5000\n",
      "Epoch: 43 | train_loss: 0.6145 | train_acc: 0.6624 | test_loss: 1.0829 | test_acc: 0.5000\n",
      "Epoch: 44 | train_loss: 0.6017 | train_acc: 0.6832 | test_loss: 0.6315 | test_acc: 0.5000\n",
      "Epoch: 45 | train_loss: 0.6057 | train_acc: 0.6980 | test_loss: 1.0995 | test_acc: 0.0000\n",
      "Epoch: 46 | train_loss: 0.6220 | train_acc: 0.6659 | test_loss: 0.7316 | test_acc: 0.5000\n",
      "Epoch: 47 | train_loss: 0.6023 | train_acc: 0.6983 | test_loss: 0.5609 | test_acc: 0.5000\n",
      "Epoch: 48 | train_loss: 0.6122 | train_acc: 0.6692 | test_loss: 0.5354 | test_acc: 0.5000\n",
      "Epoch: 49 | train_loss: 0.5993 | train_acc: 0.6939 | test_loss: 0.7358 | test_acc: 0.5000\n",
      "Epoch: 50 | train_loss: 0.6019 | train_acc: 0.6823 | test_loss: 0.6840 | test_acc: 0.5000\n",
      "Epoch: 51 | train_loss: 0.6215 | train_acc: 0.6549 | test_loss: 0.7031 | test_acc: 0.5000\n",
      "Epoch: 52 | train_loss: 0.6023 | train_acc: 0.6896 | test_loss: 0.8870 | test_acc: 0.0000\n",
      "Epoch: 53 | train_loss: 0.6029 | train_acc: 0.6913 | test_loss: 0.8380 | test_acc: 0.0000\n",
      "Epoch: 54 | train_loss: 0.5885 | train_acc: 0.7167 | test_loss: 0.8468 | test_acc: 0.5000\n",
      "Epoch: 55 | train_loss: 0.5877 | train_acc: 0.6992 | test_loss: 1.0217 | test_acc: 0.0000\n",
      "Epoch: 56 | train_loss: 0.5918 | train_acc: 0.7067 | test_loss: 0.4101 | test_acc: 1.0000\n",
      "Epoch: 57 | train_loss: 0.5902 | train_acc: 0.6980 | test_loss: 0.9023 | test_acc: 0.0000\n",
      "Epoch: 58 | train_loss: 0.5842 | train_acc: 0.7127 | test_loss: 0.8716 | test_acc: 0.5000\n",
      "Epoch: 59 | train_loss: 0.5850 | train_acc: 0.7143 | test_loss: 1.6421 | test_acc: 0.0000\n",
      "Epoch: 60 | train_loss: 0.6125 | train_acc: 0.6855 | test_loss: 0.8611 | test_acc: 0.0000\n",
      "Epoch: 61 | train_loss: 0.5902 | train_acc: 0.6977 | test_loss: 1.0236 | test_acc: 0.0000\n",
      "Epoch: 62 | train_loss: 0.5931 | train_acc: 0.7178 | test_loss: 1.2095 | test_acc: 0.0000\n",
      "Epoch: 63 | train_loss: 0.5953 | train_acc: 0.6993 | test_loss: 0.7507 | test_acc: 0.5000\n",
      "Epoch: 64 | train_loss: 0.5922 | train_acc: 0.6887 | test_loss: 0.9175 | test_acc: 0.5000\n",
      "Epoch: 65 | train_loss: 0.5812 | train_acc: 0.7143 | test_loss: 0.7072 | test_acc: 0.5000\n",
      "Epoch: 66 | train_loss: 0.5800 | train_acc: 0.7093 | test_loss: 1.0790 | test_acc: 0.0000\n",
      "Epoch: 67 | train_loss: 0.6051 | train_acc: 0.6839 | test_loss: 0.7808 | test_acc: 0.5000\n",
      "Epoch: 68 | train_loss: 0.5845 | train_acc: 0.7003 | test_loss: 0.6730 | test_acc: 0.5000\n",
      "Epoch: 69 | train_loss: 0.5836 | train_acc: 0.7067 | test_loss: 0.6701 | test_acc: 1.0000\n",
      "Epoch: 70 | train_loss: 0.5833 | train_acc: 0.7068 | test_loss: 0.6146 | test_acc: 0.5000\n",
      "Epoch: 71 | train_loss: 0.5866 | train_acc: 0.6877 | test_loss: 0.8990 | test_acc: 0.5000\n",
      "Epoch: 72 | train_loss: 0.5833 | train_acc: 0.7174 | test_loss: 0.7821 | test_acc: 0.5000\n",
      "Epoch: 73 | train_loss: 0.5889 | train_acc: 0.7202 | test_loss: 0.6992 | test_acc: 0.5000\n",
      "Epoch: 74 | train_loss: 0.5674 | train_acc: 0.7218 | test_loss: 0.8628 | test_acc: 0.5000\n",
      "Epoch: 75 | train_loss: 0.5927 | train_acc: 0.6926 | test_loss: 0.8734 | test_acc: 0.0000\n",
      "Epoch: 76 | train_loss: 0.5750 | train_acc: 0.7240 | test_loss: 0.8102 | test_acc: 0.5000\n",
      "Epoch: 77 | train_loss: 0.5734 | train_acc: 0.7290 | test_loss: 0.8398 | test_acc: 0.5000\n",
      "Epoch: 78 | train_loss: 0.5616 | train_acc: 0.7274 | test_loss: 0.6693 | test_acc: 0.5000\n",
      "Epoch: 79 | train_loss: 0.5745 | train_acc: 0.7141 | test_loss: 0.8940 | test_acc: 0.0000\n",
      "Epoch: 80 | train_loss: 0.5663 | train_acc: 0.7418 | test_loss: 0.6735 | test_acc: 0.5000\n",
      "Epoch: 81 | train_loss: 0.5662 | train_acc: 0.7233 | test_loss: 0.6334 | test_acc: 0.5000\n",
      "Epoch: 82 | train_loss: 0.5606 | train_acc: 0.7227 | test_loss: 0.8459 | test_acc: 0.0000\n",
      "Epoch: 83 | train_loss: 0.5780 | train_acc: 0.7109 | test_loss: 0.8045 | test_acc: 0.0000\n",
      "Epoch: 84 | train_loss: 0.5601 | train_acc: 0.7222 | test_loss: 0.8024 | test_acc: 0.5000\n",
      "Epoch: 85 | train_loss: 0.5730 | train_acc: 0.7083 | test_loss: 0.9711 | test_acc: 0.0000\n",
      "Epoch: 86 | train_loss: 0.5600 | train_acc: 0.7283 | test_loss: 0.6778 | test_acc: 0.5000\n",
      "Epoch: 87 | train_loss: 0.5613 | train_acc: 0.7327 | test_loss: 0.8421 | test_acc: 0.5000\n",
      "Epoch: 88 | train_loss: 0.5632 | train_acc: 0.7217 | test_loss: 1.1768 | test_acc: 0.0000\n",
      "Epoch: 89 | train_loss: 0.5624 | train_acc: 0.7350 | test_loss: 0.9065 | test_acc: 0.0000\n",
      "Epoch: 90 | train_loss: 0.5623 | train_acc: 0.7236 | test_loss: 1.1096 | test_acc: 0.5000\n",
      "Epoch: 91 | train_loss: 0.5488 | train_acc: 0.7323 | test_loss: 0.9791 | test_acc: 0.0000\n",
      "Epoch: 92 | train_loss: 0.5604 | train_acc: 0.7262 | test_loss: 0.5902 | test_acc: 0.5000\n",
      "Epoch: 93 | train_loss: 0.5592 | train_acc: 0.7112 | test_loss: 1.4776 | test_acc: 0.0000\n",
      "Epoch: 94 | train_loss: 0.5612 | train_acc: 0.7312 | test_loss: 0.6881 | test_acc: 0.5000\n",
      "Epoch: 95 | train_loss: 0.5492 | train_acc: 0.7306 | test_loss: 0.6554 | test_acc: 1.0000\n",
      "Epoch: 96 | train_loss: 0.5688 | train_acc: 0.7034 | test_loss: 0.6092 | test_acc: 1.0000\n",
      "Epoch: 97 | train_loss: 0.5557 | train_acc: 0.7189 | test_loss: 0.6035 | test_acc: 1.0000\n",
      "Epoch: 98 | train_loss: 0.5514 | train_acc: 0.7340 | test_loss: 0.6939 | test_acc: 0.5000\n",
      "Epoch: 99 | train_loss: 0.5475 | train_acc: 0.7312 | test_loss: 0.6651 | test_acc: 0.5000\n",
      "Epoch: 100 | train_loss: 0.5434 | train_acc: 0.7395 | test_loss: 0.9521 | test_acc: 0.0000\n",
      "Epoch: 101 | train_loss: 0.5448 | train_acc: 0.7393 | test_loss: 0.6777 | test_acc: 0.5000\n",
      "Epoch: 102 | train_loss: 0.5395 | train_acc: 0.7374 | test_loss: 0.8227 | test_acc: 0.5000\n",
      "Epoch: 103 | train_loss: 0.5454 | train_acc: 0.7364 | test_loss: 0.7739 | test_acc: 0.5000\n",
      "Epoch: 104 | train_loss: 0.5494 | train_acc: 0.7406 | test_loss: 1.0028 | test_acc: 0.0000\n",
      "Epoch: 105 | train_loss: 0.5527 | train_acc: 0.7299 | test_loss: 0.5283 | test_acc: 1.0000\n",
      "Epoch: 106 | train_loss: 0.5578 | train_acc: 0.7231 | test_loss: 0.9300 | test_acc: 0.5000\n",
      "Epoch: 107 | train_loss: 0.5349 | train_acc: 0.7408 | test_loss: 0.6029 | test_acc: 1.0000\n",
      "Epoch: 108 | train_loss: 0.5355 | train_acc: 0.7378 | test_loss: 1.1615 | test_acc: 0.0000\n",
      "Epoch: 109 | train_loss: 0.5506 | train_acc: 0.7323 | test_loss: 0.8498 | test_acc: 0.5000\n",
      "Epoch: 110 | train_loss: 0.5242 | train_acc: 0.7448 | test_loss: 1.2077 | test_acc: 0.5000\n",
      "Epoch: 111 | train_loss: 0.5351 | train_acc: 0.7345 | test_loss: 0.8663 | test_acc: 0.5000\n",
      "Epoch: 112 | train_loss: 0.5519 | train_acc: 0.7232 | test_loss: 0.8133 | test_acc: 0.5000\n",
      "Epoch: 113 | train_loss: 0.5431 | train_acc: 0.7461 | test_loss: 0.6805 | test_acc: 0.5000\n",
      "Epoch: 114 | train_loss: 0.5248 | train_acc: 0.7496 | test_loss: 0.8201 | test_acc: 0.5000\n",
      "Epoch: 115 | train_loss: 0.5565 | train_acc: 0.7378 | test_loss: 0.7919 | test_acc: 0.5000\n",
      "Epoch: 116 | train_loss: 0.5387 | train_acc: 0.7342 | test_loss: 0.5323 | test_acc: 0.5000\n",
      "Epoch: 117 | train_loss: 0.5284 | train_acc: 0.7500 | test_loss: 0.7470 | test_acc: 0.5000\n",
      "Epoch: 118 | train_loss: 0.5351 | train_acc: 0.7452 | test_loss: 0.6302 | test_acc: 0.5000\n",
      "Epoch: 119 | train_loss: 0.5337 | train_acc: 0.7489 | test_loss: 0.6698 | test_acc: 0.5000\n",
      "Epoch: 120 | train_loss: 0.5147 | train_acc: 0.7624 | test_loss: 0.8622 | test_acc: 0.0000\n",
      "Epoch: 121 | train_loss: 0.5273 | train_acc: 0.7481 | test_loss: 0.7274 | test_acc: 0.5000\n",
      "Epoch: 122 | train_loss: 0.5305 | train_acc: 0.7208 | test_loss: 0.7919 | test_acc: 0.5000\n",
      "Epoch: 123 | train_loss: 0.5180 | train_acc: 0.7494 | test_loss: 0.5775 | test_acc: 1.0000\n",
      "Epoch: 124 | train_loss: 0.5094 | train_acc: 0.7715 | test_loss: 0.7045 | test_acc: 0.5000\n",
      "Epoch: 125 | train_loss: 0.5204 | train_acc: 0.7516 | test_loss: 0.8475 | test_acc: 0.0000\n",
      "Epoch: 126 | train_loss: 0.5301 | train_acc: 0.7343 | test_loss: 0.6899 | test_acc: 0.5000\n",
      "Epoch: 127 | train_loss: 0.5313 | train_acc: 0.7523 | test_loss: 0.6890 | test_acc: 0.5000\n",
      "Epoch: 128 | train_loss: 0.5186 | train_acc: 0.7669 | test_loss: 0.9406 | test_acc: 0.0000\n",
      "Epoch: 129 | train_loss: 0.5230 | train_acc: 0.7452 | test_loss: 0.6875 | test_acc: 0.5000\n",
      "Epoch: 130 | train_loss: 0.5228 | train_acc: 0.7424 | test_loss: 0.6672 | test_acc: 0.5000\n",
      "Epoch: 131 | train_loss: 0.5248 | train_acc: 0.7440 | test_loss: 0.4510 | test_acc: 1.0000\n",
      "Epoch: 132 | train_loss: 0.5232 | train_acc: 0.7516 | test_loss: 0.4808 | test_acc: 1.0000\n",
      "Epoch: 133 | train_loss: 0.5262 | train_acc: 0.7365 | test_loss: 1.2047 | test_acc: 0.0000\n",
      "Epoch: 134 | train_loss: 0.5296 | train_acc: 0.7592 | test_loss: 0.9509 | test_acc: 0.0000\n",
      "Epoch: 135 | train_loss: 0.5302 | train_acc: 0.7400 | test_loss: 0.9011 | test_acc: 0.5000\n",
      "Epoch: 136 | train_loss: 0.5115 | train_acc: 0.7649 | test_loss: 0.8598 | test_acc: 0.5000\n",
      "Epoch: 137 | train_loss: 0.5296 | train_acc: 0.7424 | test_loss: 0.9458 | test_acc: 0.0000\n",
      "Epoch: 138 | train_loss: 0.5162 | train_acc: 0.7439 | test_loss: 0.5079 | test_acc: 0.5000\n",
      "Epoch: 139 | train_loss: 0.5218 | train_acc: 0.7487 | test_loss: 0.5445 | test_acc: 0.5000\n",
      "Epoch: 140 | train_loss: 0.5167 | train_acc: 0.7562 | test_loss: 0.9232 | test_acc: 0.5000\n",
      "Epoch: 141 | train_loss: 0.5177 | train_acc: 0.7487 | test_loss: 0.9720 | test_acc: 0.0000\n",
      "Epoch: 142 | train_loss: 0.5063 | train_acc: 0.7600 | test_loss: 1.1123 | test_acc: 0.0000\n",
      "Epoch: 143 | train_loss: 0.5267 | train_acc: 0.7502 | test_loss: 0.6850 | test_acc: 0.5000\n",
      "Epoch: 144 | train_loss: 0.5077 | train_acc: 0.7615 | test_loss: 0.8623 | test_acc: 0.0000\n",
      "Epoch: 145 | train_loss: 0.4997 | train_acc: 0.7422 | test_loss: 1.0751 | test_acc: 0.0000\n",
      "Epoch: 146 | train_loss: 0.5019 | train_acc: 0.7687 | test_loss: 0.8038 | test_acc: 0.0000\n",
      "Epoch: 147 | train_loss: 0.5042 | train_acc: 0.7612 | test_loss: 0.5844 | test_acc: 0.5000\n",
      "Epoch: 148 | train_loss: 0.4946 | train_acc: 0.7691 | test_loss: 0.6905 | test_acc: 0.5000\n",
      "Epoch: 149 | train_loss: 0.5395 | train_acc: 0.7409 | test_loss: 0.8098 | test_acc: 0.5000\n",
      "Epoch: 150 | train_loss: 0.4934 | train_acc: 0.7605 | test_loss: 0.6601 | test_acc: 0.5000\n",
      "Epoch: 151 | train_loss: 0.5120 | train_acc: 0.7524 | test_loss: 1.3360 | test_acc: 0.0000\n",
      "Epoch: 152 | train_loss: 0.4988 | train_acc: 0.7614 | test_loss: 0.8759 | test_acc: 0.0000\n",
      "Epoch: 153 | train_loss: 0.5048 | train_acc: 0.7472 | test_loss: 1.1026 | test_acc: 0.0000\n",
      "Epoch: 154 | train_loss: 0.4997 | train_acc: 0.7610 | test_loss: 0.8347 | test_acc: 0.0000\n",
      "Epoch: 155 | train_loss: 0.4999 | train_acc: 0.7583 | test_loss: 0.5299 | test_acc: 1.0000\n",
      "Epoch: 156 | train_loss: 0.4857 | train_acc: 0.7643 | test_loss: 0.5311 | test_acc: 0.5000\n",
      "Epoch: 157 | train_loss: 0.4973 | train_acc: 0.7812 | test_loss: 1.1070 | test_acc: 0.5000\n",
      "Epoch: 158 | train_loss: 0.5195 | train_acc: 0.7458 | test_loss: 0.7373 | test_acc: 0.5000\n",
      "Epoch: 159 | train_loss: 0.5142 | train_acc: 0.7553 | test_loss: 1.2206 | test_acc: 0.0000\n",
      "Epoch: 160 | train_loss: 0.5077 | train_acc: 0.7590 | test_loss: 0.6054 | test_acc: 1.0000\n",
      "Epoch: 161 | train_loss: 0.4940 | train_acc: 0.7665 | test_loss: 0.8931 | test_acc: 0.5000\n",
      "Epoch: 162 | train_loss: 0.4910 | train_acc: 0.7654 | test_loss: 0.5982 | test_acc: 0.5000\n",
      "Epoch: 163 | train_loss: 0.4898 | train_acc: 0.7705 | test_loss: 0.5936 | test_acc: 1.0000\n",
      "Epoch: 164 | train_loss: 0.4789 | train_acc: 0.7750 | test_loss: 0.7768 | test_acc: 0.5000\n",
      "Epoch: 165 | train_loss: 0.4667 | train_acc: 0.7882 | test_loss: 0.9559 | test_acc: 0.0000\n",
      "Epoch: 166 | train_loss: 0.4898 | train_acc: 0.7656 | test_loss: 0.7702 | test_acc: 0.5000\n",
      "Epoch: 167 | train_loss: 0.4816 | train_acc: 0.7641 | test_loss: 0.7503 | test_acc: 0.5000\n",
      "Epoch: 168 | train_loss: 0.4932 | train_acc: 0.7645 | test_loss: 0.7866 | test_acc: 0.0000\n",
      "Epoch: 169 | train_loss: 0.4750 | train_acc: 0.7836 | test_loss: 0.8004 | test_acc: 0.5000\n",
      "Epoch: 170 | train_loss: 0.4638 | train_acc: 0.7958 | test_loss: 0.8630 | test_acc: 0.5000\n",
      "Epoch: 171 | train_loss: 0.4802 | train_acc: 0.7796 | test_loss: 0.6972 | test_acc: 0.5000\n",
      "Epoch: 172 | train_loss: 0.4643 | train_acc: 0.7812 | test_loss: 1.0410 | test_acc: 0.0000\n",
      "Epoch: 173 | train_loss: 0.4821 | train_acc: 0.7623 | test_loss: 0.8681 | test_acc: 0.0000\n",
      "Epoch: 174 | train_loss: 0.4818 | train_acc: 0.7805 | test_loss: 0.9522 | test_acc: 0.0000\n",
      "Epoch: 175 | train_loss: 0.4755 | train_acc: 0.7790 | test_loss: 0.6780 | test_acc: 0.5000\n",
      "Epoch: 176 | train_loss: 0.4792 | train_acc: 0.7685 | test_loss: 1.0215 | test_acc: 0.0000\n",
      "Epoch: 177 | train_loss: 0.4723 | train_acc: 0.7599 | test_loss: 1.0702 | test_acc: 0.0000\n",
      "Epoch: 178 | train_loss: 0.4800 | train_acc: 0.7705 | test_loss: 1.0628 | test_acc: 0.5000\n",
      "Epoch: 179 | train_loss: 0.4590 | train_acc: 0.7915 | test_loss: 0.6853 | test_acc: 0.5000\n",
      "Epoch: 180 | train_loss: 0.4639 | train_acc: 0.7893 | test_loss: 1.0263 | test_acc: 0.0000\n",
      "Epoch: 181 | train_loss: 0.4631 | train_acc: 0.7827 | test_loss: 0.6188 | test_acc: 1.0000\n",
      "Epoch: 182 | train_loss: 0.4567 | train_acc: 0.7952 | test_loss: 0.5092 | test_acc: 1.0000\n",
      "Epoch: 183 | train_loss: 0.4908 | train_acc: 0.7790 | test_loss: 0.5600 | test_acc: 0.5000\n",
      "Epoch: 184 | train_loss: 0.4674 | train_acc: 0.7902 | test_loss: 0.8864 | test_acc: 0.5000\n",
      "Epoch: 185 | train_loss: 0.4623 | train_acc: 0.7950 | test_loss: 0.7594 | test_acc: 0.5000\n",
      "Epoch: 186 | train_loss: 0.4694 | train_acc: 0.7783 | test_loss: 0.5093 | test_acc: 1.0000\n",
      "Epoch: 187 | train_loss: 0.4626 | train_acc: 0.7884 | test_loss: 0.6373 | test_acc: 0.5000\n",
      "Epoch: 188 | train_loss: 0.4591 | train_acc: 0.7935 | test_loss: 1.1177 | test_acc: 0.0000\n",
      "Epoch: 189 | train_loss: 0.5004 | train_acc: 0.7602 | test_loss: 0.8551 | test_acc: 0.0000\n",
      "Epoch: 190 | train_loss: 0.4771 | train_acc: 0.7762 | test_loss: 0.6340 | test_acc: 0.5000\n",
      "Epoch: 191 | train_loss: 0.4497 | train_acc: 0.8083 | test_loss: 1.2240 | test_acc: 0.5000\n",
      "Epoch: 192 | train_loss: 0.4508 | train_acc: 0.7971 | test_loss: 0.7901 | test_acc: 0.5000\n",
      "Epoch: 193 | train_loss: 0.4375 | train_acc: 0.7871 | test_loss: 1.1107 | test_acc: 0.0000\n",
      "Epoch: 194 | train_loss: 0.4472 | train_acc: 0.7935 | test_loss: 0.8960 | test_acc: 0.5000\n",
      "Epoch: 195 | train_loss: 0.4439 | train_acc: 0.7975 | test_loss: 1.0003 | test_acc: 0.5000\n",
      "Epoch: 196 | train_loss: 0.4533 | train_acc: 0.7871 | test_loss: 1.0058 | test_acc: 0.0000\n",
      "Epoch: 197 | train_loss: 0.4475 | train_acc: 0.7904 | test_loss: 0.4983 | test_acc: 1.0000\n",
      "Epoch: 198 | train_loss: 0.4424 | train_acc: 0.7996 | test_loss: 0.6362 | test_acc: 0.5000\n",
      "Epoch: 199 | train_loss: 0.4589 | train_acc: 0.7788 | test_loss: 1.0667 | test_acc: 0.0000\n",
      "Epoch: 200 | train_loss: 0.4456 | train_acc: 0.7899 | test_loss: 0.8936 | test_acc: 0.0000\n",
      "Total training time: 208.513 seconds\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 200\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0\n",
    "model_0_results = train(model=model,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "c430feaa-6213-410b-9757-064b4e9e9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"lstm_model.pth\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f34bb-bb7f-44a6-ac3b-e31bc58d9a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10266b73-86d1-42a5-aed8-7ceb994cfed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169d9ef-ceef-4efb-b121-8d1f8968f92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
