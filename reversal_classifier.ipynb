{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3ea6b3-40f6-478b-a04e-7df824ce0d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from pyts.image import GramianAngularField\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe0cf69-c072-45b6-b1eb-9b54805d8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgDataLoader():\n",
    "    DATABASE_URL = \"postgresql://overcat:overmind@localhost:5432/stocks\"\n",
    "    query = \"\"\"\n",
    "    SELECT * from data.reversals;\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = []\n",
    "    labels = []\n",
    "    \n",
    "    try:\n",
    "        conn = psycopg2.connect(DATABASE_URL)\n",
    "        with conn.cursor() as cur:\n",
    "            cur.execute(query)\n",
    "            results = cur.fetchall()  # Fetch all rows from the query result\n",
    "            \n",
    "            for row in results:\n",
    "                matrix1 = np.array([\n",
    "                    row[1][\"Open\"],\n",
    "                    row[1][\"High\"],\n",
    "                    row[1][\"Low\"],\n",
    "                    row[1][\"Close\"],\n",
    "                    row[1][\"Volume\"]\n",
    "                    # row[1][\"Time\"]\n",
    "                ])\n",
    "    \n",
    "                matrix2 = np.array([\n",
    "                    row[2][\"Open\"],\n",
    "                    row[2][\"High\"],\n",
    "                    row[2][\"Low\"],\n",
    "                    row[2][\"Close\"],\n",
    "                    row[2][\"Volume\"]\n",
    "                    # row[2][\"Time\"]\n",
    "                ])\n",
    "    \n",
    "                # print(row[3], row[4])\n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.concatenate((matrix1, matrix2), axis=1), ax=axes, t0=row[4])\n",
    "    \n",
    "                matrix1 = np.moveaxis(matrix1, 1, 0)\n",
    "                matrix2 = np.moveaxis(matrix2, 1, 0)\n",
    "    \n",
    "                # fig, axes = plt.subplots(1, 1)\n",
    "                # candle(np.moveaxis(np.concatenate((matrix1, matrix2)), 1, 0), ax=axes, t0=temp[0])\n",
    "                dataset.append(matrix1)\n",
    "                labels.append(row[5])\n",
    "    finally:\n",
    "        conn.close()\n",
    "        \n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73c7ee45-024f-496a-9d54-2dcdc49f5a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gasf = GramianAngularField(method=\"summation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac331467-8b7a-46e6-8edf-1177db13659f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2093, 2093)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = pgDataLoader()\n",
    "len(data), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0915eebd-a272-41b4-8b96-34f5d4262a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_data = []\n",
    "for i, sample in enumerate(data):\n",
    "    open_prices = sample[:, 0]\n",
    "    high_prices = sample[:, 1]\n",
    "    low_prices = sample[:, 2]\n",
    "    close_prices = sample[:, 3]\n",
    "    volume = sample[:, 4]\n",
    "\n",
    "    body_length = np.abs(close_prices - open_prices)\n",
    "    upper_shadow_length = high_prices - np.maximum(open_prices, close_prices)\n",
    "    lower_shadow_length = np.minimum(open_prices, close_prices) - low_prices\n",
    "\n",
    "    alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices, volume)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length, close_prices)), 1, 0)\n",
    "    # alt_sample = np.moveaxis(np.vstack((body_length, upper_shadow_length, lower_shadow_length)), 1, 0)\n",
    "\n",
    "    alt_data.append(alt_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4ddaedc-41b5-40a3-909e-6d4c64d6ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, gasf_transform=None):\n",
    "        self.sequences = data\n",
    "        self.labels = labels\n",
    "        self.gasf_transform = gasf_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        if self.gasf_transform:\n",
    "            gasf_images = np.array(self.gasf_transform.transform(np.moveaxis(self.sequences[idx], 1, 0)))\n",
    "            # return torch.unsqueeze(torch.tensor(gasf_images, dtype=torch.float32), dim=0), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return torch.tensor(gasf_images, dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        else:\n",
    "            return torch.unsqueeze(torch.tensor(self.sequences[idx], dtype=torch.float32), dim=0), torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        # return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff95c1a5-fec5-46b0-bb13-d9d1b5f7f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_size = int(0.8*len(data))\n",
    "# test_size = len(data) - train_size\n",
    "# dataset = CustomDataset(data, labels)\n",
    "\n",
    "train_size = int(0.999*len(alt_data))\n",
    "test_size = len(alt_data) - train_size\n",
    "dataset = CustomDataset(alt_data, labels, gasf)\n",
    "\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48c20ecb-3696-4143-b5cb-2f28dab4ac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(5, 5))\n",
    "axes = axes.flatten()\n",
    "# for idx, img in enumerate(next(iter(train_dataloader))[0][0][0]):\n",
    "for idx, img in enumerate(next(iter(train_dataloader))[0][0]):\n",
    "    axes[idx].matshow(img)\n",
    "    axes[idx].set_xticks([])  \n",
    "    axes[idx].set_yticks([])  \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f0db7e-203f-4de1-b20f-cbbfe7419261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 8, 16, 16]), torch.Size([32, 5, 16, 16]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = next(iter(train_dataloader))[0]\n",
    "# c1 = nn.Conv1d(in_channels=4, out_channels=1, kernel_size=3, stride=1)\n",
    "# c1(temp.permute(0, 2, 1))\n",
    "c1 = nn.Conv2d(in_channels=5, out_channels=8, kernel_size=3, padding=1)\n",
    "c1(temp).shape, temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e9551c2-c8fc-46ba-9034-aad143552bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LSTMClassifier(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes, filters):\n",
    "#         super(LSTMClassifier, self).__init__()\n",
    "#         self.c1 = nn.Conv1d(in_channels=input_size, out_channels=filters, kernel_size=2, stride=1)\n",
    "#         self.lstm = nn.LSTM(filters, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "#         self.hidden_size = hidden_size\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.c1(x.permute(0, 2, 1))\n",
    "#         x = torch.tanh(x)\n",
    "\n",
    "#         h0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "#         c0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "#         out, _ = self.lstm(x.permute(0, 2, 1), (h0, c0))\n",
    "#         # out = self.flatten(out)\n",
    "#         out = self.fc(out[:, -1, :])\n",
    "#         # out = self.fc(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.c1 = nn.Conv2d(in_channels=4, out_channels=20, kernel_size=3, padding=1)\n",
    "        self.c2 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "        self.dropout1 = nn.Dropout(p=0.4)\n",
    "        self.dropout2 = nn.Dropout(p=0.2)\n",
    "        self.bn1 = nn.BatchNorm2d(10)\n",
    "        self.bn2 = nn.BatchNorm2d(10)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=40, hidden_size=hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=0.4, )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(hidden_size*2, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = x.reshape(-1, 256, 40)\n",
    "\n",
    "        h0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(num_layers*2, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # out, _ = self.lstm(x.permute(0, 2, 1), (h0, c0))\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # out = self.flatten(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed90c19e-1ac4-4fab-ad5f-dcd6b27a1c65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (c1): Conv2d(4, 20, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (c2): Conv2d(20, 40, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.01)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm): LSTM(40, 32, num_layers=4, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "num_layers = 4\n",
    "num_classes = 3\n",
    "\n",
    "model = LSTMClassifier(hidden_size, num_layers, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a6a13f-2e71-4c5f-937d-6ac151053595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(next(iter(train_dataloader))[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff599b89-3109-4c5a-bef5-ab82e6ca3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryAccuracy:\n",
    "    def __init__(self, threshold=0.5):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply sigmoid to logits to get probabilities\n",
    "        probabilities = torch.sigmoid(logits).squeeze(dim=1)\n",
    "        # Convert probabilities to binary predictions\n",
    "        predictions = (probabilities >= self.threshold).float()\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()\n",
    "        return accuracy.item()\n",
    "\n",
    "class MultiClassAccuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, logits, targets):\n",
    "        # Apply softmax to logits to get class probabilities (optional, for insight)\n",
    "        # probabilities = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the predicted class indices by applying argmax to logits\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        # Compare predictions with targets and calculate accuracy\n",
    "        correct = (predictions == targets).float().sum()\n",
    "        accuracy = correct / targets.numel()  # Total number of samples\n",
    "        return accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f5202a9-7bbe-415e-8f7b-d0649e6b5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "accuracy_fn = MultiClassAccuracy()\n",
    "# accuracy_fn = BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a54e50c2-5f8e-4264-aeb8-dd5119db3258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 1. Forward pass\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # print(X.shape)\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # print(y_pred)\n",
    "        # print()\n",
    "        # print(y)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = loss_fn(y_pred.squeeze(1), y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        train_acc += accuracy_fn(y_pred, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4692b420-0c1c-4a6d-9790-d7c63aada1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits.squeeze(1), y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_acc += accuracy_fn(test_pred_logits, y)\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa45f7-eab5-4e7c-8706-676c196b0983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11798971-37ff-4b95-9203-9bea26197884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int = 5):\n",
    "\n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "\n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2dae77a-3ca0-48f9-a86f-3f28d060386c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca945bacd17408093d3dfb55ec94403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [20, 4, 3, 3], expected input[32, 5, 16, 16] to have 4 channels, but got 5 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m start_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train model_0\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model_0_results \u001b[38;5;241m=\u001b[39m train(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     13\u001b[0m                         train_dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     14\u001b[0m                         test_dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m     15\u001b[0m                         optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[1;32m     16\u001b[0m                         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     17\u001b[0m                         epochs\u001b[38;5;241m=\u001b[39mNUM_EPOCHS)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# End the timer and print out how long it took\u001b[39;00m\n\u001b[1;32m     20\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 3. Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m---> 17\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     18\u001b[0m                                        dataloader\u001b[38;5;241m=\u001b[39mtrain_dataloader,\n\u001b[1;32m     19\u001b[0m                                        loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m     20\u001b[0m                                        optimizer\u001b[38;5;241m=\u001b[39moptimizer)\n\u001b[1;32m     21\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     22\u001b[0m         dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m     23\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# 4. Print out what's happening\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 18\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(X.shape)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(X)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# print(y_pred)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print()\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# print(y)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m), y)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[10], line 42\u001b[0m, in \u001b[0;36mLSTMClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc1(x)\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleaky_relu(x)\n\u001b[1;32m     44\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc2(x)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [20, 4, 3, 3], expected input[32, 5, 16, 16] to have 4 channels, but got 5 channels instead"
     ]
    }
   ],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0\n",
    "model_0_results = train(model=model,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c430feaa-6213-410b-9757-064b4e9e9974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"lstm_model.pth\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083f34bb-bb7f-44a6-ac3b-e31bc58d9a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10266b73-86d1-42a5-aed8-7ceb994cfed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7169d9ef-ceef-4efb-b121-8d1f8968f92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
